{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "torch.set_printoptions(precision=5, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dataset(dataset, percentage):\n",
    "    data_size = len(dataset)\n",
    "    return dataset[:int(data_size*percentage/100)]\n",
    "\n",
    "def make_dataset(dataset, n_bus):\n",
    "    x_raw_1, y_raw_1 = [], []\n",
    "    x_raw, y_raw = [], []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        for n in range(n_bus):\n",
    "            x_raw_1.append(list([dataset[i, 4*n+1], dataset[i, 4*n+2]]))\n",
    "            y_raw_1.extend(dataset[i, 4*n+3:4*n+5])\n",
    "\n",
    "        x_raw.append(list(x_raw_1))\n",
    "        y_raw.append(y_raw_1)\n",
    "        x_raw_1, y_raw_1 = [], []\n",
    "\n",
    "    x_raw = torch.tensor(x_raw, dtype=torch.float)\n",
    "    y_raw = torch.tensor(y_raw, dtype=torch.float)\n",
    "    return x_raw, y_raw\n",
    "\n",
    "def normalize_dataset(x, y):\n",
    "    x_mean = torch.mean(x,0)\n",
    "    y_mean = torch.mean(y,0)\n",
    "    x_std = torch.std(x,0)\n",
    "    y_std = torch.std(y,0)\n",
    "    x_norm = (x-x_mean)/x_std\n",
    "    y_norm = (y-y_mean)/y_std\n",
    "    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)\n",
    "    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)\n",
    "    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)\n",
    "    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)\n",
    "    return x_norm, y_norm, x_mean, y_mean, x_std, y_std\n",
    "\n",
    "def denormalize_output(y_norm, y_mean, y_std):\n",
    "    y = y_norm*y_std+y_mean\n",
    "    return y\n",
    "\n",
    "def NRMSE(yhat,y):\n",
    "    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))\n",
    "\n",
    "def MSE(yhat,y):\n",
    "    return torch.mean((yhat-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_excel('dataset\\Grid_14 bus_1.xlsx').values\n",
    "dataset2 = pd.read_excel('dataset\\Grid_14 bus_2.xlsx').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 100\n",
    "val_percentage = 100\n",
    "\n",
    "train_dataset = slice_dataset(dataset1, train_percentage)\n",
    "val_dataset = slice_dataset(dataset2, val_percentage)\n",
    "\n",
    "n_bus = 14\n",
    "\n",
    "#actual data\n",
    "x_raw_train, y_raw_train = make_dataset(train_dataset, n_bus)\n",
    "x_raw_val, y_raw_val = make_dataset(val_dataset, n_bus)\n",
    "\n",
    "#normalized data\n",
    "x_norm_train, y_norm_train, _, _, _, _ = normalize_dataset(x_raw_train, y_raw_train)\n",
    "x_norm_val, y_norm_val, x_val_mean, y_val_mean, x_val_std, y_val_std = normalize_dataset(x_raw_val, y_raw_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x_norm_train, y_norm_train\n",
    "x_val, y_val = x_norm_val, y_norm_val\n",
    "edge_index = torch.tensor([[0, 1, 1, 2, 1, 3, 2, 4, 3, 5, 4, 6, 4, 7, 5, 8, 5, 9, 1, 10, 10, 11, 11, 12, 11, 13],\n",
    "                           [1, 0, 2, 1, 3, 1, 4, 2, 5, 3, 6, 4, 7, 4, 8, 5, 9, 5, 10, 1, 11, 10, 12, 11, 13, 11]], dtype=torch.long)\n",
    "\n",
    "data_train_list, data_val_list = [], []\n",
    "for i,_ in enumerate(x_train):\n",
    "    data_train_list.append(Data(x=x_train[i], y=y_train[i], edge_index=edge_index))\n",
    "for i,_ in enumerate(x_val):\n",
    "    data_val_list.append(Data(x=x_val[i], y=y_val[i], edge_index=edge_index))\n",
    "\n",
    "train_loader = DataLoader(data_train_list, batch_size=1)\n",
    "val_loader = DataLoader(data_val_list, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_GNN_GNN_NN(torch.nn.Module):\n",
    "    def __init__(self, node_size=None, feat_in=None, feat_size1=None, feat_size2=None, hidden_size1=None, output_size=None):\n",
    "        super(My_GNN_GNN_NN, self).__init__()\n",
    "        self.feat_in = feat_in if feat_in is not None else 2\n",
    "        self.feat_size1 = feat_in if feat_in is not None else 5\n",
    "        self.feat_size2 = feat_in if feat_in is not None else 4\n",
    "        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38\n",
    "        self.output_size = output_size if output_size is not None else 18\n",
    "        \n",
    "        self.conv1 = GCNConv(feat_in, feat_size1)\n",
    "        self.conv2 = GCNConv(feat_size1, feat_size2)\n",
    "        self.lin1 = Linear(node_size*feat_size2, hidden_size1)\n",
    "        self.lin2 = Linear(hidden_size1, output_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        x = x.flatten(start_dim = 0)\n",
    "        x = self.lin1(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def save_weights(self, model, name):\n",
    "        torch.save(model, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "torch.Size([2, 8])\n",
      "conv1.bias\n",
      "torch.Size([8])\n",
      "conv2.weight\n",
      "torch.Size([8, 4])\n",
      "conv2.bias\n",
      "torch.Size([4])\n",
      "lin1.weight\n",
      "torch.Size([30, 56])\n",
      "lin1.bias\n",
      "torch.Size([30])\n",
      "lin2.weight\n",
      "torch.Size([28, 30])\n",
      "lin2.bias\n",
      "torch.Size([28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2638"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "feat_in = 2\n",
    "feat_size1 = 8\n",
    "feat_size2 = 4\n",
    "hidden_size1 = 30\n",
    "output_size = n_bus*2\n",
    "lr = 0.0001\n",
    "\n",
    "model = My_GNN_GNN_NN(n_bus, feat_in, feat_size1, feat_size2, hidden_size1, output_size)\n",
    "for name, param in model.named_parameters():\n",
    "  print(name)\n",
    "  print(param.size())\n",
    "\n",
    "param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0    train loss: 0.1537334    val loss: 0.0221647\n",
      "epoch: 10    train loss: 0.0041269    val loss: 0.0040684\n",
      "epoch: 20    train loss: 0.0029323    val loss: 0.0030754\n",
      "epoch: 30    train loss: 0.0023691    val loss: 0.0025800\n",
      "epoch: 40    train loss: 0.0019231    val loss: 0.0021512\n",
      "epoch: 50    train loss: 0.0014367    val loss: 0.0016353\n",
      "epoch: 60    train loss: 0.0011500    val loss: 0.0013308\n",
      "epoch: 70    train loss: 0.0010074    val loss: 0.0011741\n",
      "epoch: 80    train loss: 0.0009034    val loss: 0.0010557\n",
      "epoch: 90    train loss: 0.0008166    val loss: 0.0009562\n",
      "epoch: 100    train loss: 0.0007411    val loss: 0.0008697\n",
      "epoch: 110    train loss: 0.0006742    val loss: 0.0007940\n",
      "epoch: 120    train loss: 0.0006149    val loss: 0.0007281\n",
      "epoch: 130    train loss: 0.0005620    val loss: 0.0006714\n",
      "epoch: 140    train loss: 0.0005155    val loss: 0.0006239\n",
      "epoch: 150    train loss: 0.0004745    val loss: 0.0005849\n",
      "epoch: 160    train loss: 0.0004381    val loss: 0.0005531\n",
      "epoch: 170    train loss: 0.0004055    val loss: 0.0005271\n",
      "epoch: 180    train loss: 0.0003764    val loss: 0.0005059\n",
      "epoch: 190    train loss: 0.0003502    val loss: 0.0004883\n",
      "epoch: 200    train loss: 0.0003267    val loss: 0.0004737\n",
      "epoch: 210    train loss: 0.0003055    val loss: 0.0004615\n",
      "epoch: 220    train loss: 0.0002863    val loss: 0.0004515\n",
      "epoch: 230    train loss: 0.0002689    val loss: 0.0004433\n",
      "epoch: 240    train loss: 0.0002531    val loss: 0.0004369\n",
      "epoch: 250    train loss: 0.0002386    val loss: 0.0004319\n",
      "epoch: 260    train loss: 0.0002254    val loss: 0.0004284\n",
      "epoch: 270    train loss: 0.0002132    val loss: 0.0004260\n",
      "epoch: 280    train loss: 0.0002021    val loss: 0.0004246\n",
      "epoch: 290    train loss: 0.0001918    val loss: 0.0004240\n",
      "epoch: 300    train loss: 0.0001823    val loss: 0.0004240\n",
      "epoch: 310    train loss: 0.0001735    val loss: 0.0004245\n",
      "epoch: 320    train loss: 0.0001654    val loss: 0.0004253\n",
      "epoch: 330    train loss: 0.0001579    val loss: 0.0004261\n",
      "epoch: 340    train loss: 0.0001509    val loss: 0.0004268\n",
      "epoch: 350    train loss: 0.0001443    val loss: 0.0004273\n",
      "epoch: 360    train loss: 0.0001382    val loss: 0.0004273\n",
      "epoch: 370    train loss: 0.0001325    val loss: 0.0004268\n",
      "epoch: 380    train loss: 0.0001272    val loss: 0.0004258\n",
      "epoch: 390    train loss: 0.0001222    val loss: 0.0004241\n",
      "epoch: 400    train loss: 0.0001176    val loss: 0.0004217\n",
      "epoch: 410    train loss: 0.0001132    val loss: 0.0004187\n",
      "epoch: 420    train loss: 0.0001091    val loss: 0.0004152\n",
      "epoch: 430    train loss: 0.0001053    val loss: 0.0004111\n",
      "epoch: 440    train loss: 0.0001017    val loss: 0.0004067\n",
      "epoch: 450    train loss: 0.0000984    val loss: 0.0004019\n",
      "epoch: 460    train loss: 0.0000953    val loss: 0.0003970\n",
      "epoch: 470    train loss: 0.0000923    val loss: 0.0003920\n",
      "epoch: 480    train loss: 0.0000896    val loss: 0.0003870\n",
      "epoch: 490    train loss: 0.0000870    val loss: 0.0003822\n",
      "epoch: 500    train loss: 0.0000846    val loss: 0.0003774\n",
      "epoch: 510    train loss: 0.0000823    val loss: 0.0003728\n",
      "epoch: 520    train loss: 0.0000802    val loss: 0.0003685\n",
      "epoch: 530    train loss: 0.0000782    val loss: 0.0003643\n",
      "epoch: 540    train loss: 0.0000763    val loss: 0.0003604\n",
      "epoch: 550    train loss: 0.0000745    val loss: 0.0003566\n",
      "epoch: 560    train loss: 0.0000728    val loss: 0.0003530\n",
      "epoch: 570    train loss: 0.0000713    val loss: 0.0003497\n",
      "epoch: 580    train loss: 0.0000698    val loss: 0.0003467\n",
      "epoch: 590    train loss: 0.0000685    val loss: 0.0003439\n",
      "epoch: 600    train loss: 0.0000672    val loss: 0.0003414\n",
      "epoch: 610    train loss: 0.0000660    val loss: 0.0003394\n",
      "epoch: 620    train loss: 0.0000649    val loss: 0.0003376\n",
      "epoch: 630    train loss: 0.0000638    val loss: 0.0003362\n",
      "epoch: 640    train loss: 0.0000628    val loss: 0.0003347\n",
      "epoch: 650    train loss: 0.0000619    val loss: 0.0003334\n",
      "epoch: 660    train loss: 0.0000610    val loss: 0.0003321\n",
      "epoch: 670    train loss: 0.0000602    val loss: 0.0003309\n",
      "epoch: 680    train loss: 0.0000594    val loss: 0.0003298\n",
      "epoch: 690    train loss: 0.0000586    val loss: 0.0003288\n",
      "epoch: 700    train loss: 0.0000579    val loss: 0.0003277\n",
      "epoch: 710    train loss: 0.0000572    val loss: 0.0003268\n",
      "epoch: 720    train loss: 0.0000565    val loss: 0.0003260\n",
      "epoch: 730    train loss: 0.0000559    val loss: 0.0003253\n",
      "epoch: 740    train loss: 0.0000552    val loss: 0.0003246\n",
      "epoch: 750    train loss: 0.0000546    val loss: 0.0003239\n",
      "epoch: 760    train loss: 0.0000541    val loss: 0.0003233\n",
      "epoch: 770    train loss: 0.0000535    val loss: 0.0003227\n",
      "epoch: 780    train loss: 0.0000530    val loss: 0.0003222\n",
      "epoch: 790    train loss: 0.0000525    val loss: 0.0003218\n",
      "epoch: 800    train loss: 0.0000520    val loss: 0.0003213\n",
      "epoch: 810    train loss: 0.0000515    val loss: 0.0003209\n",
      "epoch: 820    train loss: 0.0000510    val loss: 0.0003205\n",
      "epoch: 830    train loss: 0.0000505    val loss: 0.0003202\n",
      "epoch: 840    train loss: 0.0000501    val loss: 0.0003199\n",
      "epoch: 850    train loss: 0.0000497    val loss: 0.0003195\n",
      "epoch: 860    train loss: 0.0000492    val loss: 0.0003192\n",
      "epoch: 870    train loss: 0.0000488    val loss: 0.0003189\n",
      "epoch: 880    train loss: 0.0000484    val loss: 0.0003186\n",
      "epoch: 890    train loss: 0.0000480    val loss: 0.0003183\n",
      "epoch: 900    train loss: 0.0000477    val loss: 0.0003181\n",
      "epoch: 910    train loss: 0.0000473    val loss: 0.0003178\n",
      "epoch: 920    train loss: 0.0000469    val loss: 0.0003175\n",
      "epoch: 930    train loss: 0.0000466    val loss: 0.0003173\n",
      "epoch: 940    train loss: 0.0000462    val loss: 0.0003169\n",
      "epoch: 950    train loss: 0.0000459    val loss: 0.0003166\n",
      "epoch: 960    train loss: 0.0000456    val loss: 0.0003164\n",
      "epoch: 970    train loss: 0.0000452    val loss: 0.0003160\n",
      "epoch: 980    train loss: 0.0000449    val loss: 0.0003157\n",
      "epoch: 990    train loss: 0.0000446    val loss: 0.0003154\n",
      "epoch: 1000    train loss: 0.0000443    val loss: 0.0003150\n",
      "epoch: 1010    train loss: 0.0000440    val loss: 0.0003146\n",
      "epoch: 1020    train loss: 0.0000437    val loss: 0.0003143\n",
      "epoch: 1030    train loss: 0.0000434    val loss: 0.0003139\n",
      "epoch: 1040    train loss: 0.0000431    val loss: 0.0003134\n",
      "epoch: 1050    train loss: 0.0000429    val loss: 0.0003131\n",
      "epoch: 1060    train loss: 0.0000426    val loss: 0.0003126\n",
      "epoch: 1070    train loss: 0.0000423    val loss: 0.0003122\n",
      "epoch: 1080    train loss: 0.0000421    val loss: 0.0003117\n",
      "epoch: 1090    train loss: 0.0000418    val loss: 0.0003113\n",
      "epoch: 1100    train loss: 0.0000415    val loss: 0.0003107\n",
      "epoch: 1110    train loss: 0.0000413    val loss: 0.0003103\n",
      "epoch: 1120    train loss: 0.0000410    val loss: 0.0003098\n",
      "epoch: 1130    train loss: 0.0000408    val loss: 0.0003093\n",
      "epoch: 1140    train loss: 0.0000406    val loss: 0.0003088\n",
      "epoch: 1150    train loss: 0.0000403    val loss: 0.0003083\n",
      "epoch: 1160    train loss: 0.0000401    val loss: 0.0003079\n",
      "epoch: 1170    train loss: 0.0000399    val loss: 0.0003075\n",
      "epoch: 1180    train loss: 0.0000397    val loss: 0.0003071\n",
      "epoch: 1190    train loss: 0.0000394    val loss: 0.0003068\n",
      "epoch: 1200    train loss: 0.0000392    val loss: 0.0003065\n",
      "epoch: 1210    train loss: 0.0000390    val loss: 0.0003062\n",
      "epoch: 1220    train loss: 0.0000388    val loss: 0.0003061\n",
      "epoch: 1230    train loss: 0.0000386    val loss: 0.0003059\n",
      "epoch: 1240    train loss: 0.0000384    val loss: 0.0003059\n",
      "epoch: 1250    train loss: 0.0000381    val loss: 0.0003059\n",
      "epoch: 1260    train loss: 0.0000379    val loss: 0.0003059\n",
      "epoch: 1270    train loss: 0.0000377    val loss: 0.0003059\n",
      "epoch: 1280    train loss: 0.0000375    val loss: 0.0003061\n",
      "epoch: 1290    train loss: 0.0000373    val loss: 0.0003062\n",
      "epoch: 1300    train loss: 0.0000371    val loss: 0.0003064\n",
      "epoch: 1310    train loss: 0.0000369    val loss: 0.0003066\n",
      "epoch: 1320    train loss: 0.0000367    val loss: 0.0003069\n",
      "epoch: 1330    train loss: 0.0000365    val loss: 0.0003072\n",
      "epoch: 1340    train loss: 0.0000363    val loss: 0.0003075\n",
      "epoch: 1350    train loss: 0.0000362    val loss: 0.0003079\n",
      "epoch: 1360    train loss: 0.0000360    val loss: 0.0003082\n",
      "epoch: 1370    train loss: 0.0000358    val loss: 0.0003086\n",
      "epoch: 1380    train loss: 0.0000356    val loss: 0.0003091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1390    train loss: 0.0000354    val loss: 0.0003095\n",
      "epoch: 1400    train loss: 0.0000352    val loss: 0.0003100\n",
      "epoch: 1410    train loss: 0.0000350    val loss: 0.0003105\n",
      "epoch: 1420    train loss: 0.0000348    val loss: 0.0003109\n",
      "epoch: 1430    train loss: 0.0000346    val loss: 0.0003115\n",
      "epoch: 1440    train loss: 0.0000344    val loss: 0.0003120\n",
      "epoch: 1450    train loss: 0.0000343    val loss: 0.0003125\n",
      "epoch: 1460    train loss: 0.0000341    val loss: 0.0003131\n",
      "epoch: 1470    train loss: 0.0000339    val loss: 0.0003137\n",
      "epoch: 1480    train loss: 0.0000337    val loss: 0.0003143\n",
      "epoch: 1490    train loss: 0.0000335    val loss: 0.0003149\n",
      "epoch: 1500    train loss: 0.0000333    val loss: 0.0003155\n",
      "epoch: 1510    train loss: 0.0000331    val loss: 0.0003161\n",
      "epoch: 1520    train loss: 0.0000329    val loss: 0.0003167\n",
      "epoch: 1530    train loss: 0.0000327    val loss: 0.0003173\n",
      "epoch: 1540    train loss: 0.0000325    val loss: 0.0003179\n",
      "epoch: 1550    train loss: 0.0000323    val loss: 0.0003185\n",
      "epoch: 1560    train loss: 0.0000321    val loss: 0.0003191\n",
      "epoch: 1570    train loss: 0.0000319    val loss: 0.0003197\n",
      "epoch: 1580    train loss: 0.0000317    val loss: 0.0003204\n",
      "epoch: 1590    train loss: 0.0000315    val loss: 0.0003209\n",
      "epoch: 1600    train loss: 0.0000313    val loss: 0.0003215\n",
      "epoch: 1610    train loss: 0.0000311    val loss: 0.0003220\n",
      "epoch: 1620    train loss: 0.0000309    val loss: 0.0003225\n",
      "epoch: 1630    train loss: 0.0000306    val loss: 0.0003230\n",
      "epoch: 1640    train loss: 0.0000304    val loss: 0.0003235\n",
      "epoch: 1650    train loss: 0.0000302    val loss: 0.0003239\n",
      "epoch: 1660    train loss: 0.0000300    val loss: 0.0003241\n",
      "epoch: 1670    train loss: 0.0000297    val loss: 0.0003244\n",
      "epoch: 1680    train loss: 0.0000295    val loss: 0.0003246\n",
      "epoch: 1690    train loss: 0.0000293    val loss: 0.0003248\n",
      "epoch: 1700    train loss: 0.0000291    val loss: 0.0003249\n",
      "epoch: 1710    train loss: 0.0000289    val loss: 0.0003248\n",
      "epoch: 1720    train loss: 0.0000286    val loss: 0.0003249\n",
      "epoch: 1730    train loss: 0.0000284    val loss: 0.0003250\n",
      "epoch: 1740    train loss: 0.0000282    val loss: 0.0003251\n",
      "epoch: 1750    train loss: 0.0000280    val loss: 0.0003250\n",
      "epoch: 1760    train loss: 0.0000278    val loss: 0.0003248\n",
      "epoch: 1770    train loss: 0.0000276    val loss: 0.0003242\n",
      "epoch: 1780    train loss: 0.0000274    val loss: 0.0003236\n",
      "epoch: 1790    train loss: 0.0000272    val loss: 0.0003228\n",
      "epoch: 1800    train loss: 0.0000270    val loss: 0.0003220\n",
      "epoch: 1810    train loss: 0.0000268    val loss: 0.0003212\n",
      "epoch: 1820    train loss: 0.0000266    val loss: 0.0003203\n",
      "epoch: 1830    train loss: 0.0000265    val loss: 0.0003193\n",
      "epoch: 1840    train loss: 0.0000263    val loss: 0.0003183\n",
      "epoch: 1850    train loss: 0.0000261    val loss: 0.0003173\n",
      "epoch: 1860    train loss: 0.0000260    val loss: 0.0003163\n",
      "epoch: 1870    train loss: 0.0000258    val loss: 0.0003151\n",
      "epoch: 1880    train loss: 0.0000257    val loss: 0.0003141\n",
      "epoch: 1890    train loss: 0.0000255    val loss: 0.0003131\n",
      "epoch: 1900    train loss: 0.0000254    val loss: 0.0003119\n",
      "epoch: 1910    train loss: 0.0000252    val loss: 0.0003108\n",
      "epoch: 1920    train loss: 0.0000251    val loss: 0.0003097\n",
      "epoch: 1930    train loss: 0.0000250    val loss: 0.0003086\n",
      "epoch: 1940    train loss: 0.0000248    val loss: 0.0003076\n",
      "epoch: 1950    train loss: 0.0000247    val loss: 0.0003065\n",
      "epoch: 1960    train loss: 0.0000246    val loss: 0.0003054\n",
      "epoch: 1970    train loss: 0.0000245    val loss: 0.0003044\n",
      "epoch: 1980    train loss: 0.0000244    val loss: 0.0003033\n",
      "epoch: 1990    train loss: 0.0000242    val loss: 0.0003023\n",
      "epoch: 2000    train loss: 0.0000241    val loss: 0.0003012\n",
      "Wall time: 10h 43min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "feat_in = 2\n",
    "feat_size1 = 8\n",
    "feat_size2 = 4\n",
    "hidden_size1 = 30\n",
    "output_size = n_bus*2\n",
    "lr = 0.0001\n",
    "\n",
    "model = My_GNN_GNN_NN(n_bus, feat_in, feat_size1, feat_size2, hidden_size1, output_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train_loss_list, val_loss_list = [], []\n",
    "\n",
    "count=0\n",
    "patience=2000\n",
    "lossMin = 1e10\n",
    "\n",
    "for epoch in range(2001):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_train_prediction = model(batch)\n",
    "        loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch.num_graphs\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss=0\n",
    "    for batch in val_loader:\n",
    "        y_val_prediction = model(batch)\n",
    "        loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "        val_loss += loss.item() * batch.num_graphs\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_loss_list.append(val_loss)\n",
    "\n",
    "    #early stopping\n",
    "    if (val_loss < lossMin):\n",
    "        lossMin = val_loss\n",
    "        count = 0\n",
    "        best_epoch = epoch\n",
    "        best_train_loss = train_loss\n",
    "        best_val_loss = val_loss\n",
    "        model.save_weights(model, \"[PyG] [14 bus] Best_GNN_GNN_NN_model.pt\")\n",
    "    else:\n",
    "        count+=1\n",
    "        if(count>patience):\n",
    "            print(\"early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(epoch, train_loss, val_loss))\n",
    "            print(\"best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(best_epoch, best_train_loss, best_val_loss))\n",
    "            break\n",
    "    \n",
    "    if (train_loss <= 0):\n",
    "        print(\"min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(epoch, train_loss, val_loss))\n",
    "        break\n",
    "\n",
    "    if (epoch % 10) == 0:\n",
    "        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last epoch: 2000, train loss: 0.0000241, val loss: 0.0003012\n",
      "best epoch: 2000, train loss: 0.0000241, val loss: 0.0003012\n"
     ]
    }
   ],
   "source": [
    "plt.title('GNN GNN NN on power flow dataset')\n",
    "plt.plot(train_loss_list, label=\"train loss\")\n",
    "plt.plot(val_loss_list, label=\"val loss\")\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"# Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print('last epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(epoch, train_loss, val_loss))\n",
    "print('best epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(best_epoch, best_train_loss, best_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 datapoint] Train output ground-truth: \n",
      "[  10.324 -154.216    9.882 -156.117    9.753 -156.712    9.724 -156.839\n",
      "    9.659 -157.161    9.607 -157.399    9.617 -157.37     9.627 -157.311\n",
      "    9.581 -157.512    9.56  -157.633    9.773 -156.613    9.682 -157.039\n",
      "    9.658 -157.152    9.664 -157.123]\n",
      "[1 datapoint] Train output prediction: \n",
      "[  10.32485 -154.21043    9.88219 -156.11086    9.75412 -156.70464\n",
      "    9.72574 -156.8286     9.65994 -157.15352    9.60771 -157.38568\n",
      "    9.61815 -157.3611     9.62844 -157.30487    9.58253 -157.49971\n",
      "    9.56058 -157.61708    9.77272 -156.61021    9.68278 -157.03413\n",
      "    9.65834 -157.14716    9.66356 -157.12132]\n",
      "[1 datapoint] Train loss (MSE): 0.0000250\n",
      "Train loss (MSE): 0.0000313\n",
      "=========================================================================\n",
      "[1 datapoint] Val output ground-truth: \n",
      "[  10.302 -154.788    9.844 -156.745    9.686 -157.465    9.685 -157.484\n",
      "    9.574 -157.989    9.568 -158.053    9.543 -158.141    9.532 -158.187\n",
      "    9.518 -158.294    9.535 -158.209    9.727 -157.275    9.629 -157.731\n",
      "    9.599 -157.869    9.594 -157.896]\n",
      "[1 datapoint] Val output prediction: \n",
      "[  10.30267 -154.7807     9.84434 -156.73418    9.68798 -157.44936\n",
      "    9.68608 -157.47235    9.57643 -157.9707     9.56699 -158.0462\n",
      "    9.54471 -158.12393    9.53471 -158.16666    9.51851 -158.28639\n",
      "    9.53493 -158.20297    9.72717 -157.26643    9.63    -157.72171\n",
      "    9.60018 -157.8621     9.59502 -157.88791]\n",
      "[1 datapoint] Val loss (MSE): 0.0000725\n",
      "Val loss (MSE): 0.0003012\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_train_prediction_1 = model(train_loader.dataset[0])\n",
    "train_loss_1 = MSE(denormalize_output(y_train_prediction_1, y_val_mean, y_val_std), denormalize_output(y_norm_train[0], y_val_mean, y_val_std))\n",
    "print(\"[1 datapoint] Train output ground-truth: \\n\" + str(y_raw_train[0].detach().numpy()))\n",
    "print(\"[1 datapoint] Train output prediction: \\n\" + str(denormalize_output(y_train_prediction_1, y_val_mean, y_val_std).detach().numpy()))\n",
    "print('[1 datapoint] Train loss (MSE): {:.7f}'.format(train_loss_1))\n",
    "\n",
    "train_loss = 0\n",
    "for batch in train_loader:\n",
    "    pred = model(batch)\n",
    "    loss = MSE(denormalize_output(pred, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "    train_loss += loss.item() * batch.num_graphs\n",
    "train_loss /= len(train_loader.dataset)\n",
    "print('Train loss (MSE): {:.7f}'.format(train_loss))\n",
    "\n",
    "print(\"=========================================================================\")\n",
    "\n",
    "y_val_prediction_1 = model(val_loader.dataset[0])\n",
    "val_loss_1 = MSE(denormalize_output(y_val_prediction_1, y_val_mean, y_val_std), denormalize_output(y_norm_val[0], y_val_mean, y_val_std))\n",
    "print(\"[1 datapoint] Val output ground-truth: \\n\" + str(y_raw_val[0].detach().numpy()))\n",
    "print(\"[1 datapoint] Val output prediction: \\n\" + str(denormalize_output(y_val_prediction_1, y_val_mean, y_val_std).detach().numpy()))\n",
    "print('[1 datapoint] Val loss (MSE): {:.7f}'.format(val_loss_1))\n",
    "\n",
    "val_loss=0\n",
    "for batch in val_loader:\n",
    "    pred = model(batch)\n",
    "    loss = MSE(denormalize_output(pred, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "    val_loss += loss.item() * batch.num_graphs\n",
    "val_loss /= len(val_loader.dataset)\n",
    "print('Val loss (MSE): {:.7f}'.format(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 datapoint] Train output ground-truth: \n",
      "[  10.324 -154.216    9.882 -156.117    9.753 -156.712    9.724 -156.839\n",
      "    9.659 -157.161    9.607 -157.399    9.617 -157.37     9.627 -157.311\n",
      "    9.581 -157.512    9.56  -157.633    9.773 -156.613    9.682 -157.039\n",
      "    9.658 -157.152    9.664 -157.123]\n",
      "[1 datapoint] Train output prediction: \n",
      "[  10.32485 -154.21043    9.88219 -156.11086    9.75412 -156.70464\n",
      "    9.72574 -156.8286     9.65994 -157.15352    9.60771 -157.38568\n",
      "    9.61815 -157.3611     9.62844 -157.30487    9.58253 -157.49971\n",
      "    9.56058 -157.61708    9.77272 -156.61021    9.68278 -157.03413\n",
      "    9.65834 -157.14716    9.66356 -157.12132]\n",
      "[1 datapoint] Train loss (MSE): 0.0000250\n",
      "Train loss (MSE): 0.0000313\n",
      "=========================================================================\n",
      "[1 datapoint] Val output ground-truth: \n",
      "[  10.302 -154.788    9.844 -156.745    9.686 -157.465    9.685 -157.484\n",
      "    9.574 -157.989    9.568 -158.053    9.543 -158.141    9.532 -158.187\n",
      "    9.518 -158.294    9.535 -158.209    9.727 -157.275    9.629 -157.731\n",
      "    9.599 -157.869    9.594 -157.896]\n",
      "[1 datapoint] Val output prediction: \n",
      "[  10.30267 -154.7807     9.84434 -156.73418    9.68798 -157.44936\n",
      "    9.68608 -157.47235    9.57643 -157.9707     9.56699 -158.0462\n",
      "    9.54471 -158.12393    9.53471 -158.16666    9.51851 -158.28639\n",
      "    9.53493 -158.20297    9.72717 -157.26643    9.63    -157.72171\n",
      "    9.60018 -157.8621     9.59502 -157.88791]\n",
      "[1 datapoint] Val loss (MSE): 0.0000725\n",
      "Val loss (MSE): 0.0003012\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load(\"[PyG] [14 bus] Best_GNN_GNN_NN_model.pt\")\n",
    "best_model.eval()\n",
    "\n",
    "y_train_prediction_1 = best_model(train_loader.dataset[0])\n",
    "train_loss_1 = MSE(denormalize_output(y_train_prediction_1, y_val_mean, y_val_std), denormalize_output(y_norm_train[0], y_val_mean, y_val_std))\n",
    "print(\"[1 datapoint] Train output ground-truth: \\n\" + str(y_raw_train[0].detach().numpy()))\n",
    "print(\"[1 datapoint] Train output prediction: \\n\" + str(denormalize_output(y_train_prediction_1, y_val_mean, y_val_std).detach().numpy()))\n",
    "print('[1 datapoint] Train loss (MSE): {:.7f}'.format(train_loss_1))\n",
    "\n",
    "train_loss = 0\n",
    "for batch in train_loader:\n",
    "    pred = best_model(batch)\n",
    "    loss = MSE(denormalize_output(pred, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "    train_loss += loss.item() * batch.num_graphs\n",
    "train_loss /= len(train_loader.dataset)\n",
    "print('Train loss (MSE): {:.7f}'.format(train_loss))\n",
    "\n",
    "print(\"=========================================================================\")\n",
    "\n",
    "y_val_prediction_1 = best_model(val_loader.dataset[0])\n",
    "val_loss_1 = MSE(denormalize_output(y_val_prediction_1, y_val_mean, y_val_std), denormalize_output(y_norm_val[0], y_val_mean, y_val_std))\n",
    "print(\"[1 datapoint] Val output ground-truth: \\n\" + str(y_raw_val[0].detach().numpy()))\n",
    "print(\"[1 datapoint] Val output prediction: \\n\" + str(denormalize_output(y_val_prediction_1, y_val_mean, y_val_std).detach().numpy()))\n",
    "print('[1 datapoint] Val loss (MSE): {:.7f}'.format(val_loss_1))\n",
    "\n",
    "val_loss=0\n",
    "for batch in val_loader:\n",
    "    pred = best_model(batch)\n",
    "    loss = MSE(denormalize_output(pred, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "    val_loss += loss.item() * batch.num_graphs\n",
    "val_loss /= len(val_loader.dataset)\n",
    "print('Val loss (MSE): {:.7f}'.format(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 1\n",
      "Train loss (MSE): 0.0000313\n",
      "===========================\n",
      "dataset 2\n",
      "Val loss (MSE): 0.0003012\n",
      "===========================\n",
      "dataset 3\n",
      "Test loss (MSE): 0.0001376\n",
      "===========================\n",
      "dataset 4\n",
      "Test loss (MSE): 0.0002257\n",
      "===========================\n",
      "dataset 5\n",
      "Test loss (MSE): 0.0000845\n",
      "===========================\n",
      "dataset 6\n",
      "Test loss (MSE): 0.0000494\n",
      "===========================\n",
      "dataset 7\n",
      "Test loss (MSE): 0.0001754\n",
      "===========================\n",
      "dataset 8\n",
      "Test loss (MSE): 0.0000878\n",
      "===========================\n",
      "dataset 9\n",
      "Test loss (MSE): 0.0000876\n",
      "===========================\n",
      "dataset 10\n",
      "Test loss (MSE): 0.0000969\n",
      "===========================\n",
      "dataset 11\n",
      "Test loss (MSE): 0.0001296\n",
      "===========================\n",
      "dataset 12\n",
      "Test loss (MSE): 0.0001576\n",
      "===========================\n",
      "dataset 13\n",
      "Test loss (MSE): 0.0000549\n",
      "===========================\n",
      "dataset 14\n",
      "Test loss (MSE): 0.0001378\n",
      "===========================\n",
      "dataset 15\n",
      "Test loss (MSE): 0.0001066\n",
      "===========================\n",
      "dataset 16\n",
      "Test loss (MSE): 0.0000850\n",
      "===========================\n",
      "dataset 17\n",
      "Test loss (MSE): 0.0000806\n",
      "===========================\n",
      "dataset 18\n",
      "Test loss (MSE): 0.0001702\n",
      "===========================\n",
      "dataset 19\n",
      "Test loss (MSE): 0.0001532\n",
      "===========================\n",
      "dataset 20\n",
      "Test loss (MSE): 0.0001554\n",
      "===========================\n",
      "dataset 21\n",
      "Test loss (MSE): 0.0000886\n",
      "===========================\n",
      "dataset 22\n",
      "Test loss (MSE): 0.0000702\n",
      "===========================\n",
      "dataset 23\n",
      "Test loss (MSE): 0.0001485\n",
      "===========================\n",
      "dataset 24\n",
      "Test loss (MSE): 0.0001794\n",
      "===========================\n",
      "dataset 25\n",
      "Test loss (MSE): 0.0001070\n",
      "===========================\n",
      "dataset 26\n",
      "Test loss (MSE): 0.0001093\n",
      "===========================\n",
      "dataset 27\n",
      "Test loss (MSE): 0.0001202\n",
      "===========================\n",
      "dataset 28\n",
      "Test loss (MSE): 0.0000931\n",
      "===========================\n",
      "dataset 29\n",
      "Test loss (MSE): 0.0001168\n",
      "===========================\n",
      "dataset 30\n",
      "Test loss (MSE): 0.0002493\n",
      "===========================\n",
      "dataset 31\n",
      "Test loss (MSE): 0.0002062\n",
      "===========================\n",
      "dataset 32\n",
      "Test loss (MSE): 0.0001016\n",
      "===========================\n",
      "dataset 33\n",
      "Test loss (MSE): 0.0000526\n",
      "===========================\n",
      "dataset 34\n",
      "Test loss (MSE): 0.0001027\n",
      "===========================\n",
      "dataset 35\n",
      "Test loss (MSE): 0.0000679\n",
      "===========================\n",
      "dataset 36\n",
      "Test loss (MSE): 0.0001546\n",
      "===========================\n",
      "dataset 37\n",
      "Test loss (MSE): 0.0000802\n",
      "===========================\n",
      "dataset 38\n",
      "Test loss (MSE): 0.0001744\n",
      "===========================\n",
      "dataset 39\n",
      "Test loss (MSE): 0.0000721\n",
      "===========================\n",
      "dataset 40\n",
      "Test loss (MSE): 0.0001220\n",
      "===========================\n",
      "dataset 41\n",
      "Test loss (MSE): 0.0000269\n",
      "===========================\n",
      "dataset 42\n",
      "Test loss (MSE): 0.0001044\n",
      "===========================\n",
      "dataset 43\n",
      "Test loss (MSE): 0.0001377\n",
      "===========================\n",
      "dataset 44\n",
      "Test loss (MSE): 0.0002065\n",
      "===========================\n",
      "dataset 45\n",
      "Test loss (MSE): 0.0001811\n",
      "===========================\n",
      "dataset 46\n",
      "Test loss (MSE): 0.0001945\n",
      "===========================\n",
      "dataset 47\n",
      "Test loss (MSE): 0.0001644\n",
      "===========================\n",
      "dataset 48\n",
      "Test loss (MSE): 0.0001831\n",
      "===========================\n",
      "dataset 49\n",
      "Test loss (MSE): 0.0000600\n",
      "===========================\n",
      "dataset 50\n",
      "Test loss (MSE): 0.0000480\n",
      "===========================\n",
      "dataset 51\n",
      "Test loss (MSE): 0.0001545\n",
      "===========================\n",
      "dataset 52\n",
      "Test loss (MSE): 0.0003007\n",
      "===========================\n",
      "dataset 53\n",
      "Test loss (MSE): 0.0000564\n",
      "===========================\n",
      "dataset 54\n",
      "Test loss (MSE): 0.0001825\n",
      "===========================\n",
      "dataset 55\n",
      "Test loss (MSE): 0.0000996\n",
      "===========================\n",
      "dataset 56\n",
      "Test loss (MSE): 0.0000807\n",
      "===========================\n",
      "dataset 57\n",
      "Test loss (MSE): 0.0001216\n",
      "===========================\n",
      "dataset 58\n",
      "Test loss (MSE): 0.0000952\n",
      "===========================\n",
      "dataset 59\n",
      "Test loss (MSE): 0.0002365\n",
      "===========================\n",
      "dataset 60\n",
      "Test loss (MSE): 0.0002028\n",
      "===========================\n",
      "dataset 61\n",
      "Test loss (MSE): 0.0000995\n",
      "===========================\n",
      "dataset 62\n",
      "Test loss (MSE): 0.0002581\n",
      "===========================\n",
      "dataset 63\n",
      "Test loss (MSE): 0.0001091\n",
      "===========================\n",
      "dataset 64\n",
      "Test loss (MSE): 0.0002207\n",
      "===========================\n",
      "dataset 65\n",
      "Test loss (MSE): 0.0000700\n",
      "===========================\n",
      "dataset 66\n",
      "Test loss (MSE): 0.0003156\n",
      "===========================\n",
      "dataset 67\n",
      "Test loss (MSE): 0.0001387\n",
      "===========================\n",
      "dataset 68\n",
      "Test loss (MSE): 0.0002372\n",
      "===========================\n",
      "dataset 69\n",
      "Test loss (MSE): 0.0000921\n",
      "===========================\n",
      "dataset 70\n",
      "Test loss (MSE): 0.0002872\n",
      "===========================\n",
      "dataset 71\n",
      "Test loss (MSE): 0.0002545\n",
      "===========================\n",
      "dataset 72\n",
      "Test loss (MSE): 0.0000822\n",
      "===========================\n",
      "dataset 73\n",
      "Test loss (MSE): 0.0001719\n",
      "===========================\n",
      "dataset 74\n",
      "Test loss (MSE): 0.0000759\n",
      "===========================\n",
      "dataset 75\n",
      "Test loss (MSE): 0.0001121\n",
      "===========================\n",
      "dataset 76\n",
      "Test loss (MSE): 0.0000483\n",
      "===========================\n",
      "dataset 77\n",
      "Test loss (MSE): 0.0001296\n",
      "===========================\n",
      "dataset 78\n",
      "Test loss (MSE): 0.0004407\n",
      "===========================\n",
      "dataset 79\n",
      "Test loss (MSE): 0.0000703\n",
      "===========================\n",
      "dataset 80\n",
      "Test loss (MSE): 0.0000477\n",
      "===========================\n",
      "dataset 81\n",
      "Test loss (MSE): 0.0002680\n",
      "===========================\n",
      "dataset 82\n",
      "Test loss (MSE): 0.0000879\n",
      "===========================\n",
      "dataset 83\n",
      "Test loss (MSE): 0.0001962\n",
      "===========================\n",
      "dataset 84\n",
      "Test loss (MSE): 0.0000950\n",
      "===========================\n",
      "dataset 85\n",
      "Test loss (MSE): 0.0002136\n",
      "===========================\n",
      "dataset 86\n",
      "Test loss (MSE): 0.0000748\n",
      "===========================\n",
      "dataset 87\n",
      "Test loss (MSE): 0.0001014\n",
      "===========================\n",
      "dataset 88\n",
      "Test loss (MSE): 0.0002121\n",
      "===========================\n",
      "dataset 89\n",
      "Test loss (MSE): 0.0005154\n",
      "===========================\n",
      "dataset 90\n",
      "Test loss (MSE): 0.0000826\n",
      "===========================\n",
      "dataset 91\n",
      "Test loss (MSE): 0.0001864\n",
      "===========================\n",
      "dataset 92\n",
      "Test loss (MSE): 0.0002359\n",
      "===========================\n",
      "dataset 93\n",
      "Test loss (MSE): 0.0002737\n",
      "===========================\n",
      "dataset 94\n",
      "Test loss (MSE): 0.0000533\n",
      "===========================\n",
      "dataset 95\n",
      "Test loss (MSE): 0.0002721\n",
      "===========================\n",
      "dataset 96\n",
      "Test loss (MSE): 0.0001864\n",
      "===========================\n",
      "dataset 97\n",
      "Test loss (MSE): 0.0000701\n",
      "===========================\n",
      "dataset 98\n",
      "Test loss (MSE): 0.0000926\n",
      "===========================\n",
      "dataset 99\n",
      "Test loss (MSE): 0.0001313\n",
      "===========================\n",
      "dataset 100\n",
      "Test loss (MSE): 0.0000878\n",
      "===========================\n",
      "dataset 101\n",
      "Test loss (MSE): 0.0001041\n",
      "===========================\n",
      "dataset 102\n",
      "Test loss (MSE): 0.0000548\n",
      "===========================\n",
      "\n",
      "test loss file saved!\n",
      "\n",
      "Wall time: 25min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model = torch.load(\"[PyG] [14 bus] Best_GNN_GNN_NN_model.pt\")\n",
    "best_model.eval()\n",
    "\n",
    "test_loss_list = []\n",
    "\n",
    "for i in range(102):\n",
    "    \n",
    "    dataset = pd.read_excel('dataset\\Grid_14 bus_%d.xlsx' % (i+1)).values\n",
    "    test_percentage = 100\n",
    "    test_dataset = slice_dataset(dataset, test_percentage)\n",
    "    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)\n",
    "    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)\n",
    "    \n",
    "    x_test, y_test = x_norm_test, y_norm_test\n",
    "    \n",
    "    data_test_list = []\n",
    "    for j,_ in enumerate(x_test):\n",
    "        data_test_list.append(Data(x=x_test[j], y=y_test[j], edge_index=edge_index))\n",
    "\n",
    "    test_loader = DataLoader(data_test_list, batch_size=1)\n",
    "    \n",
    "    print('dataset {:d}'.format(i+1))\n",
    "    \n",
    "    test_loss = 0\n",
    "    for batch in test_loader:\n",
    "        y_test_prediction = best_model(batch)\n",
    "        loss = MSE(denormalize_output(y_test_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "        test_loss += loss.item() * batch.num_graphs\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    if i == 0:\n",
    "        print('Train loss (MSE): {:.7f}'.format(test_loss))\n",
    "    elif i == 1:\n",
    "        print('Val loss (MSE): {:.7f}'.format(test_loss))\n",
    "    else:\n",
    "        print('Test loss (MSE): {:.7f}'.format(test_loss))\n",
    "        test_loss_list.append(test_loss)\n",
    "    \n",
    "    print(\"===========================\")\n",
    "\n",
    "column = []\n",
    "for i in range(100):\n",
    "    column.append('test loss %d' % (i+1))\n",
    "    \n",
    "test_loss_file = pd.DataFrame([test_loss_list], columns=column)\n",
    "test_loss_file.to_excel(\"[PyG] [14 bus] [MSE] GNN GNN NN test loss.xlsx\")\n",
    "print(\"\\ntest loss file saved!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 1\n",
      "Train loss (NRMSE): 0.0320580\n",
      "===========================\n",
      "dataset 2\n",
      "Val loss (NRMSE): 0.0303914\n",
      "===========================\n",
      "dataset 3\n",
      "Test loss (NRMSE): 0.0314084\n",
      "===========================\n",
      "dataset 4\n",
      "Test loss (NRMSE): 0.0348826\n",
      "===========================\n",
      "dataset 5\n",
      "Test loss (NRMSE): 0.0371720\n",
      "===========================\n",
      "dataset 6\n",
      "Test loss (NRMSE): 0.0290156\n",
      "===========================\n",
      "dataset 7\n",
      "Test loss (NRMSE): 0.0509769\n",
      "===========================\n",
      "dataset 8\n",
      "Test loss (NRMSE): 0.0682174\n",
      "===========================\n",
      "dataset 9\n",
      "Test loss (NRMSE): 0.0327468\n",
      "===========================\n",
      "dataset 10\n",
      "Test loss (NRMSE): 0.0561564\n",
      "===========================\n",
      "dataset 11\n",
      "Test loss (NRMSE): 0.0462868\n",
      "===========================\n",
      "dataset 12\n",
      "Test loss (NRMSE): 0.0393915\n",
      "===========================\n",
      "dataset 13\n",
      "Test loss (NRMSE): 0.0307153\n",
      "===========================\n",
      "dataset 14\n",
      "Test loss (NRMSE): 0.0315468\n",
      "===========================\n",
      "dataset 15\n",
      "Test loss (NRMSE): 0.0319377\n",
      "===========================\n",
      "dataset 16\n",
      "Test loss (NRMSE): 0.0335499\n",
      "===========================\n",
      "dataset 17\n",
      "Test loss (NRMSE): 0.0359092\n",
      "===========================\n",
      "dataset 18\n",
      "Test loss (NRMSE): 0.0411022\n",
      "===========================\n",
      "dataset 19\n",
      "Test loss (NRMSE): 0.0307250\n",
      "===========================\n",
      "dataset 20\n",
      "Test loss (NRMSE): 0.0383114\n",
      "===========================\n",
      "dataset 21\n",
      "Test loss (NRMSE): 0.0445544\n",
      "===========================\n",
      "dataset 22\n",
      "Test loss (NRMSE): 0.0373744\n",
      "===========================\n",
      "dataset 23\n",
      "Test loss (NRMSE): 0.0351655\n",
      "===========================\n",
      "dataset 24\n",
      "Test loss (NRMSE): 0.0569242\n",
      "===========================\n",
      "dataset 25\n",
      "Test loss (NRMSE): 0.0478730\n",
      "===========================\n",
      "dataset 26\n",
      "Test loss (NRMSE): 0.0335474\n",
      "===========================\n",
      "dataset 27\n",
      "Test loss (NRMSE): 0.0455690\n",
      "===========================\n",
      "dataset 28\n",
      "Test loss (NRMSE): 0.0342481\n",
      "===========================\n",
      "dataset 29\n",
      "Test loss (NRMSE): 0.0311042\n",
      "===========================\n",
      "dataset 30\n",
      "Test loss (NRMSE): 0.0552231\n",
      "===========================\n",
      "dataset 31\n",
      "Test loss (NRMSE): 0.0556760\n",
      "===========================\n",
      "dataset 32\n",
      "Test loss (NRMSE): 0.0274523\n",
      "===========================\n",
      "dataset 33\n",
      "Test loss (NRMSE): 0.0463691\n",
      "===========================\n",
      "dataset 34\n",
      "Test loss (NRMSE): 0.0339447\n",
      "===========================\n",
      "dataset 35\n",
      "Test loss (NRMSE): 0.0602693\n",
      "===========================\n",
      "dataset 36\n",
      "Test loss (NRMSE): 0.0348882\n",
      "===========================\n",
      "dataset 37\n",
      "Test loss (NRMSE): 0.0296305\n",
      "===========================\n",
      "dataset 38\n",
      "Test loss (NRMSE): 0.0450902\n",
      "===========================\n",
      "dataset 39\n",
      "Test loss (NRMSE): 0.0413447\n",
      "===========================\n",
      "dataset 40\n",
      "Test loss (NRMSE): 0.0524645\n",
      "===========================\n",
      "dataset 41\n",
      "Test loss (NRMSE): 0.0319821\n",
      "===========================\n",
      "dataset 42\n",
      "Test loss (NRMSE): 0.0358408\n",
      "===========================\n",
      "dataset 43\n",
      "Test loss (NRMSE): 0.0374950\n",
      "===========================\n",
      "dataset 44\n",
      "Test loss (NRMSE): 0.0383397\n",
      "===========================\n",
      "dataset 45\n",
      "Test loss (NRMSE): 0.0316870\n",
      "===========================\n",
      "dataset 46\n",
      "Test loss (NRMSE): 0.0338216\n",
      "===========================\n",
      "dataset 47\n",
      "Test loss (NRMSE): 0.0366458\n",
      "===========================\n",
      "dataset 48\n",
      "Test loss (NRMSE): 0.0400011\n",
      "===========================\n",
      "dataset 49\n",
      "Test loss (NRMSE): 0.0419725\n",
      "===========================\n",
      "dataset 50\n",
      "Test loss (NRMSE): 0.0331411\n",
      "===========================\n",
      "dataset 51\n",
      "Test loss (NRMSE): 0.0369666\n",
      "===========================\n",
      "dataset 52\n",
      "Test loss (NRMSE): 0.0360211\n",
      "===========================\n",
      "dataset 53\n",
      "Test loss (NRMSE): 0.0427384\n",
      "===========================\n",
      "dataset 54\n",
      "Test loss (NRMSE): 0.0330543\n",
      "===========================\n",
      "dataset 55\n",
      "Test loss (NRMSE): 0.0350412\n",
      "===========================\n",
      "dataset 56\n",
      "Test loss (NRMSE): 0.0464874\n",
      "===========================\n",
      "dataset 57\n",
      "Test loss (NRMSE): 0.0359100\n",
      "===========================\n",
      "dataset 58\n",
      "Test loss (NRMSE): 0.0428309\n",
      "===========================\n",
      "dataset 59\n",
      "Test loss (NRMSE): 0.0347247\n",
      "===========================\n",
      "dataset 60\n",
      "Test loss (NRMSE): 0.0341005\n",
      "===========================\n",
      "dataset 61\n",
      "Test loss (NRMSE): 0.0374249\n",
      "===========================\n",
      "dataset 62\n",
      "Test loss (NRMSE): 0.0350793\n",
      "===========================\n",
      "dataset 63\n",
      "Test loss (NRMSE): 0.0408173\n",
      "===========================\n",
      "dataset 64\n",
      "Test loss (NRMSE): 0.0356918\n",
      "===========================\n",
      "dataset 65\n",
      "Test loss (NRMSE): 0.0317583\n",
      "===========================\n",
      "dataset 66\n",
      "Test loss (NRMSE): 0.0342126\n",
      "===========================\n",
      "dataset 67\n",
      "Test loss (NRMSE): 0.0325821\n",
      "===========================\n",
      "dataset 68\n",
      "Test loss (NRMSE): 0.0368711\n",
      "===========================\n",
      "dataset 69\n",
      "Test loss (NRMSE): 0.0377766\n",
      "===========================\n",
      "dataset 70\n",
      "Test loss (NRMSE): 0.0558283\n",
      "===========================\n",
      "dataset 71\n",
      "Test loss (NRMSE): 0.0332992\n",
      "===========================\n",
      "dataset 72\n",
      "Test loss (NRMSE): 0.0378781\n",
      "===========================\n",
      "dataset 73\n",
      "Test loss (NRMSE): 0.0611175\n",
      "===========================\n",
      "dataset 74\n",
      "Test loss (NRMSE): 0.0315408\n",
      "===========================\n",
      "dataset 75\n",
      "Test loss (NRMSE): 0.0425844\n",
      "===========================\n",
      "dataset 76\n",
      "Test loss (NRMSE): 0.0318909\n",
      "===========================\n",
      "dataset 77\n",
      "Test loss (NRMSE): 0.0358546\n",
      "===========================\n",
      "dataset 78\n",
      "Test loss (NRMSE): 0.0463812\n",
      "===========================\n",
      "dataset 79\n",
      "Test loss (NRMSE): 0.0317302\n",
      "===========================\n",
      "dataset 80\n",
      "Test loss (NRMSE): 0.0420540\n",
      "===========================\n",
      "dataset 81\n",
      "Test loss (NRMSE): 0.0331810\n",
      "===========================\n",
      "dataset 82\n",
      "Test loss (NRMSE): 0.0490645\n",
      "===========================\n",
      "dataset 83\n",
      "Test loss (NRMSE): 0.0342902\n",
      "===========================\n",
      "dataset 84\n",
      "Test loss (NRMSE): 0.0515997\n",
      "===========================\n",
      "dataset 85\n",
      "Test loss (NRMSE): 0.0349316\n",
      "===========================\n",
      "dataset 86\n",
      "Test loss (NRMSE): 0.0639957\n",
      "===========================\n",
      "dataset 87\n",
      "Test loss (NRMSE): 0.0390491\n",
      "===========================\n",
      "dataset 88\n",
      "Test loss (NRMSE): 0.0360901\n",
      "===========================\n",
      "dataset 89\n",
      "Test loss (NRMSE): 0.0344074\n",
      "===========================\n",
      "dataset 90\n",
      "Test loss (NRMSE): 0.0445109\n",
      "===========================\n",
      "dataset 91\n",
      "Test loss (NRMSE): 0.0333824\n",
      "===========================\n",
      "dataset 92\n",
      "Test loss (NRMSE): 0.0457255\n",
      "===========================\n",
      "dataset 93\n",
      "Test loss (NRMSE): 0.0797244\n",
      "===========================\n",
      "dataset 94\n",
      "Test loss (NRMSE): 0.0374237\n",
      "===========================\n",
      "dataset 95\n",
      "Test loss (NRMSE): 0.0321360\n",
      "===========================\n",
      "dataset 96\n",
      "Test loss (NRMSE): 0.0492999\n",
      "===========================\n",
      "dataset 97\n",
      "Test loss (NRMSE): 0.0256720\n",
      "===========================\n",
      "dataset 98\n",
      "Test loss (NRMSE): 0.0533655\n",
      "===========================\n",
      "dataset 99\n",
      "Test loss (NRMSE): 0.0358957\n",
      "===========================\n",
      "dataset 100\n",
      "Test loss (NRMSE): 0.0326261\n",
      "===========================\n",
      "dataset 101\n",
      "Test loss (NRMSE): 0.0281140\n",
      "===========================\n",
      "dataset 102\n",
      "Test loss (NRMSE): 0.0455353\n",
      "===========================\n",
      "\n",
      "test loss file saved!\n",
      "\n",
      "Wall time: 38min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model = torch.load(\"[PyG] [14 bus] Best_GNN_GNN_NN_model.pt\")\n",
    "best_model.eval()\n",
    "\n",
    "test_loss_list = []\n",
    "\n",
    "for i in range(102):\n",
    "    \n",
    "    dataset = pd.read_excel('dataset\\Grid_14 bus_%d.xlsx' % (i+1)).values\n",
    "    test_percentage = 100\n",
    "    test_dataset = slice_dataset(dataset, test_percentage)\n",
    "    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)\n",
    "    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)\n",
    "    \n",
    "    x_test, y_test = x_norm_test, y_norm_test\n",
    "    \n",
    "    data_test_list = []\n",
    "    for j,_ in enumerate(x_test):\n",
    "        data_test_list.append(Data(x=x_test[j], y=y_test[j], edge_index=edge_index))\n",
    "\n",
    "    test_loader = DataLoader(data_test_list, batch_size=1)\n",
    "    \n",
    "    print('dataset {:d}'.format(i+1))\n",
    "    \n",
    "    test_loss = 0\n",
    "    yhat = torch.empty(0, n_bus*2)\n",
    "    for batch in test_loader:\n",
    "        y_test_prediction = best_model(batch)\n",
    "        yhat = torch.cat((yhat, y_test_prediction.reshape(1, n_bus*2)))\n",
    "    \n",
    "    yhat = denormalize_output(yhat, y_val_mean, y_val_std)\n",
    "    y = y_raw_test\n",
    "    test_loss_NRMSE = NRMSE(yhat, y)\n",
    "    \n",
    "    if i == 0:\n",
    "        print('Train loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))\n",
    "    elif i == 1:\n",
    "        print('Val loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))\n",
    "    else:\n",
    "        print('Test loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))\n",
    "        test_loss_list.append(test_loss_NRMSE.detach().numpy())\n",
    "    \n",
    "    print(\"===========================\")\n",
    "\n",
    "column = []\n",
    "for i in range(100):\n",
    "    column.append('test loss %d' % (i+1))\n",
    "    \n",
    "test_loss_file = pd.DataFrame([test_loss_list], columns=column)\n",
    "test_loss_file.to_excel(\"[PyG] [14 bus] [NRMSE] GNN GNN NN test loss.xlsx\")\n",
    "print(\"\\ntest loss file saved!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
