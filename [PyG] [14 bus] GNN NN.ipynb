{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "torch.set_printoptions(precision=5, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dataset(dataset, percentage):\n",
    "    data_size = len(dataset)\n",
    "    return dataset[:int(data_size*percentage/100)]\n",
    "\n",
    "def make_dataset(dataset, n_bus):\n",
    "    x_raw_1, y_raw_1 = [], []\n",
    "    x_raw, y_raw = [], []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        for n in range(n_bus):\n",
    "            x_raw_1.append(list([dataset[i, 4*n+1], dataset[i, 4*n+2]]))\n",
    "            y_raw_1.extend(dataset[i, 4*n+3:4*n+5])\n",
    "\n",
    "        x_raw.append(list(x_raw_1))\n",
    "        y_raw.append(y_raw_1)\n",
    "        x_raw_1, y_raw_1 = [], []\n",
    "\n",
    "    x_raw = torch.tensor(x_raw, dtype=torch.float)\n",
    "    y_raw = torch.tensor(y_raw, dtype=torch.float)\n",
    "    return x_raw, y_raw\n",
    "\n",
    "def normalize_dataset(x, y):\n",
    "    x_mean = torch.mean(x,0)\n",
    "    y_mean = torch.mean(y,0)\n",
    "    x_std = torch.std(x,0)\n",
    "    y_std = torch.std(y,0)\n",
    "    x_norm = (x-x_mean)/x_std\n",
    "    y_norm = (y-y_mean)/y_std\n",
    "    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)\n",
    "    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)\n",
    "    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)\n",
    "    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)\n",
    "    return x_norm, y_norm, x_mean, y_mean, x_std, y_std\n",
    "\n",
    "def denormalize_output(y_norm, y_mean, y_std):\n",
    "    y = y_norm*y_std+y_mean\n",
    "    return y\n",
    "\n",
    "def NRMSE(yhat,y):\n",
    "    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))\n",
    "\n",
    "def MSE(yhat,y):\n",
    "    return torch.mean((yhat-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_excel('dataset\\Grid_14 bus_1.xlsx').values\n",
    "dataset2 = pd.read_excel('dataset\\Grid_14 bus_2.xlsx').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 100\n",
    "val_percentage = 100\n",
    "\n",
    "train_dataset = slice_dataset(dataset1, train_percentage)\n",
    "val_dataset = slice_dataset(dataset2, val_percentage)\n",
    "\n",
    "n_bus = 14\n",
    "\n",
    "#actual data\n",
    "x_raw_train, y_raw_train = make_dataset(train_dataset, n_bus)\n",
    "x_raw_val, y_raw_val = make_dataset(val_dataset, n_bus)\n",
    "\n",
    "#normalized data\n",
    "x_norm_train, y_norm_train, _, _, _, _ = normalize_dataset(x_raw_train, y_raw_train)\n",
    "x_norm_val, y_norm_val, x_val_mean, y_val_mean, x_val_std, y_val_std = normalize_dataset(x_raw_val, y_raw_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x_norm_train, y_norm_train\n",
    "x_val, y_val = x_norm_val, y_norm_val\n",
    "edge_index = torch.tensor([[0, 1, 1, 2, 1, 3, 2, 4, 3, 5, 4, 6, 4, 7, 5, 8, 5, 9, 1, 10, 10, 11, 11, 12, 11, 13],\n",
    "                           [1, 0, 2, 1, 3, 1, 4, 2, 5, 3, 6, 4, 7, 4, 8, 5, 9, 5, 10, 1, 11, 10, 12, 11, 13, 11]], dtype=torch.long)\n",
    "\n",
    "data_train_list, data_val_list = [], []\n",
    "for i,_ in enumerate(x_train):\n",
    "    data_train_list.append(Data(x=x_train[i], y=y_train[i], edge_index=edge_index))\n",
    "for i,_ in enumerate(x_val):\n",
    "    data_val_list.append(Data(x=x_val[i], y=y_val[i], edge_index=edge_index))\n",
    "\n",
    "train_loader = DataLoader(data_train_list, batch_size=1)\n",
    "val_loader = DataLoader(data_val_list, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_GNN_NN(torch.nn.Module):\n",
    "    def __init__(self, node_size=None, feat_in=None, feat_size1=None, hidden_size1=None, output_size=None):\n",
    "        super(My_GNN_NN, self).__init__()\n",
    "        self.feat_in = feat_in if feat_in is not None else 2\n",
    "        self.feat_size1 = feat_in if feat_in is not None else 4\n",
    "        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 20\n",
    "        self.output_size = output_size if output_size is not None else 12\n",
    "        \n",
    "        self.conv1 = GCNConv(feat_in, feat_size1)\n",
    "        self.lin1 = Linear(node_size*feat_size1, hidden_size1)\n",
    "        self.lin2 = Linear(hidden_size1, output_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        x = x.flatten(start_dim = 0)\n",
    "        x = self.lin1(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def save_weights(self, model, name):\n",
    "        torch.save(model, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "torch.Size([2, 4])\n",
      "conv1.bias\n",
      "torch.Size([4])\n",
      "lin1.weight\n",
      "torch.Size([30, 56])\n",
      "lin1.bias\n",
      "torch.Size([30])\n",
      "lin2.weight\n",
      "torch.Size([28, 30])\n",
      "lin2.bias\n",
      "torch.Size([28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2590"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_in = 2\n",
    "feat_size1 = 4\n",
    "hidden_size1 = 30\n",
    "output_size = n_bus*2\n",
    "lr = 0.0001\n",
    "\n",
    "model = My_GNN_NN(n_bus, feat_in, feat_size1, hidden_size1, output_size)\n",
    "for name, param in model.named_parameters():\n",
    "  print(name)\n",
    "  print(param.size())\n",
    "\n",
    "param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0    train loss: 0.1735121    val loss: 0.0261103\n",
      "epoch: 10    train loss: 0.0026192    val loss: 0.0024028\n",
      "epoch: 20    train loss: 0.0010459    val loss: 0.0010568\n",
      "epoch: 30    train loss: 0.0005664    val loss: 0.0006689\n",
      "epoch: 40    train loss: 0.0003642    val loss: 0.0005213\n",
      "epoch: 50    train loss: 0.0002622    val loss: 0.0004505\n",
      "epoch: 60    train loss: 0.0002028    val loss: 0.0004064\n",
      "epoch: 70    train loss: 0.0001664    val loss: 0.0003768\n",
      "epoch: 80    train loss: 0.0001424    val loss: 0.0003573\n",
      "epoch: 90    train loss: 0.0001254    val loss: 0.0003447\n",
      "epoch: 100    train loss: 0.0001127    val loss: 0.0003369\n",
      "epoch: 110    train loss: 0.0001026    val loss: 0.0003322\n",
      "epoch: 120    train loss: 0.0000944    val loss: 0.0003301\n",
      "epoch: 130    train loss: 0.0000876    val loss: 0.0003298\n",
      "epoch: 140    train loss: 0.0000818    val loss: 0.0003311\n",
      "epoch: 150    train loss: 0.0000769    val loss: 0.0003336\n",
      "epoch: 160    train loss: 0.0000726    val loss: 0.0003368\n",
      "epoch: 170    train loss: 0.0000690    val loss: 0.0003405\n",
      "epoch: 180    train loss: 0.0000657    val loss: 0.0003442\n",
      "epoch: 190    train loss: 0.0000629    val loss: 0.0003477\n",
      "epoch: 200    train loss: 0.0000604    val loss: 0.0003509\n",
      "epoch: 210    train loss: 0.0000582    val loss: 0.0003534\n",
      "epoch: 220    train loss: 0.0000561    val loss: 0.0003555\n",
      "epoch: 230    train loss: 0.0000543    val loss: 0.0003571\n",
      "epoch: 240    train loss: 0.0000526    val loss: 0.0003583\n",
      "epoch: 250    train loss: 0.0000510    val loss: 0.0003591\n",
      "epoch: 260    train loss: 0.0000496    val loss: 0.0003597\n",
      "epoch: 270    train loss: 0.0000482    val loss: 0.0003601\n",
      "epoch: 280    train loss: 0.0000470    val loss: 0.0003603\n",
      "epoch: 290    train loss: 0.0000458    val loss: 0.0003605\n",
      "epoch: 300    train loss: 0.0000447    val loss: 0.0003605\n",
      "epoch: 310    train loss: 0.0000437    val loss: 0.0003605\n",
      "epoch: 320    train loss: 0.0000427    val loss: 0.0003605\n",
      "epoch: 330    train loss: 0.0000417    val loss: 0.0003606\n",
      "epoch: 340    train loss: 0.0000409    val loss: 0.0003606\n",
      "epoch: 350    train loss: 0.0000400    val loss: 0.0003605\n",
      "epoch: 360    train loss: 0.0000392    val loss: 0.0003605\n",
      "epoch: 370    train loss: 0.0000384    val loss: 0.0003604\n",
      "epoch: 380    train loss: 0.0000377    val loss: 0.0003603\n",
      "epoch: 390    train loss: 0.0000370    val loss: 0.0003602\n",
      "epoch: 400    train loss: 0.0000363    val loss: 0.0003601\n",
      "epoch: 410    train loss: 0.0000357    val loss: 0.0003599\n",
      "epoch: 420    train loss: 0.0000350    val loss: 0.0003597\n",
      "epoch: 430    train loss: 0.0000344    val loss: 0.0003595\n",
      "epoch: 440    train loss: 0.0000339    val loss: 0.0003593\n",
      "epoch: 450    train loss: 0.0000333    val loss: 0.0003591\n",
      "epoch: 460    train loss: 0.0000328    val loss: 0.0003587\n",
      "epoch: 470    train loss: 0.0000323    val loss: 0.0003586\n",
      "epoch: 480    train loss: 0.0000317    val loss: 0.0003583\n",
      "epoch: 490    train loss: 0.0000313    val loss: 0.0003580\n",
      "epoch: 500    train loss: 0.0000308    val loss: 0.0003577\n",
      "epoch: 510    train loss: 0.0000303    val loss: 0.0003575\n",
      "epoch: 520    train loss: 0.0000299    val loss: 0.0003573\n",
      "epoch: 530    train loss: 0.0000294    val loss: 0.0003570\n",
      "epoch: 540    train loss: 0.0000290    val loss: 0.0003568\n",
      "epoch: 550    train loss: 0.0000286    val loss: 0.0003565\n",
      "epoch: 560    train loss: 0.0000282    val loss: 0.0003563\n",
      "epoch: 570    train loss: 0.0000278    val loss: 0.0003561\n",
      "epoch: 580    train loss: 0.0000274    val loss: 0.0003558\n",
      "epoch: 590    train loss: 0.0000270    val loss: 0.0003555\n",
      "epoch: 600    train loss: 0.0000267    val loss: 0.0003553\n",
      "epoch: 610    train loss: 0.0000263    val loss: 0.0003551\n",
      "epoch: 620    train loss: 0.0000260    val loss: 0.0003548\n",
      "epoch: 630    train loss: 0.0000256    val loss: 0.0003546\n",
      "epoch: 640    train loss: 0.0000253    val loss: 0.0003542\n",
      "epoch: 650    train loss: 0.0000249    val loss: 0.0003539\n",
      "epoch: 660    train loss: 0.0000246    val loss: 0.0003536\n",
      "epoch: 670    train loss: 0.0000243    val loss: 0.0003534\n",
      "epoch: 680    train loss: 0.0000240    val loss: 0.0003530\n",
      "epoch: 690    train loss: 0.0000237    val loss: 0.0003526\n",
      "epoch: 700    train loss: 0.0000234    val loss: 0.0003522\n",
      "epoch: 710    train loss: 0.0000231    val loss: 0.0003518\n",
      "epoch: 720    train loss: 0.0000229    val loss: 0.0003513\n",
      "epoch: 730    train loss: 0.0000226    val loss: 0.0003507\n",
      "epoch: 740    train loss: 0.0000223    val loss: 0.0003502\n",
      "epoch: 750    train loss: 0.0000221    val loss: 0.0003496\n",
      "epoch: 760    train loss: 0.0000218    val loss: 0.0003489\n",
      "epoch: 770    train loss: 0.0000216    val loss: 0.0003482\n",
      "epoch: 780    train loss: 0.0000213    val loss: 0.0003474\n",
      "epoch: 790    train loss: 0.0000211    val loss: 0.0003466\n",
      "epoch: 800    train loss: 0.0000208    val loss: 0.0003458\n",
      "epoch: 810    train loss: 0.0000206    val loss: 0.0003448\n",
      "epoch: 820    train loss: 0.0000204    val loss: 0.0003438\n",
      "epoch: 830    train loss: 0.0000202    val loss: 0.0003427\n",
      "epoch: 840    train loss: 0.0000199    val loss: 0.0003416\n",
      "epoch: 850    train loss: 0.0000197    val loss: 0.0003404\n",
      "epoch: 860    train loss: 0.0000195    val loss: 0.0003391\n",
      "epoch: 870    train loss: 0.0000193    val loss: 0.0003377\n",
      "epoch: 880    train loss: 0.0000191    val loss: 0.0003363\n",
      "epoch: 890    train loss: 0.0000189    val loss: 0.0003348\n",
      "epoch: 900    train loss: 0.0000187    val loss: 0.0003332\n",
      "epoch: 910    train loss: 0.0000185    val loss: 0.0003318\n",
      "epoch: 920    train loss: 0.0000184    val loss: 0.0003303\n",
      "epoch: 930    train loss: 0.0000182    val loss: 0.0003289\n",
      "epoch: 940    train loss: 0.0000180    val loss: 0.0003276\n",
      "epoch: 950    train loss: 0.0000179    val loss: 0.0003264\n",
      "epoch: 960    train loss: 0.0000177    val loss: 0.0003249\n",
      "epoch: 970    train loss: 0.0000176    val loss: 0.0003234\n",
      "epoch: 980    train loss: 0.0000175    val loss: 0.0003220\n",
      "epoch: 990    train loss: 0.0000173    val loss: 0.0003205\n",
      "epoch: 1000    train loss: 0.0000172    val loss: 0.0003190\n",
      "epoch: 1010    train loss: 0.0000171    val loss: 0.0003174\n",
      "epoch: 1020    train loss: 0.0000170    val loss: 0.0003159\n",
      "epoch: 1030    train loss: 0.0000168    val loss: 0.0003144\n",
      "epoch: 1040    train loss: 0.0000167    val loss: 0.0003128\n",
      "epoch: 1050    train loss: 0.0000166    val loss: 0.0003113\n",
      "epoch: 1060    train loss: 0.0000165    val loss: 0.0003097\n",
      "epoch: 1070    train loss: 0.0000164    val loss: 0.0003082\n",
      "epoch: 1080    train loss: 0.0000163    val loss: 0.0003065\n",
      "epoch: 1090    train loss: 0.0000162    val loss: 0.0003048\n",
      "epoch: 1100    train loss: 0.0000161    val loss: 0.0003031\n",
      "epoch: 1110    train loss: 0.0000160    val loss: 0.0003014\n",
      "epoch: 1120    train loss: 0.0000159    val loss: 0.0002997\n",
      "epoch: 1130    train loss: 0.0000158    val loss: 0.0002978\n",
      "epoch: 1140    train loss: 0.0000157    val loss: 0.0002960\n",
      "epoch: 1150    train loss: 0.0000157    val loss: 0.0002943\n",
      "epoch: 1160    train loss: 0.0000156    val loss: 0.0002922\n",
      "epoch: 1170    train loss: 0.0000155    val loss: 0.0002903\n",
      "epoch: 1180    train loss: 0.0000154    val loss: 0.0002882\n",
      "epoch: 1190    train loss: 0.0000153    val loss: 0.0002861\n",
      "epoch: 1200    train loss: 0.0000153    val loss: 0.0002839\n",
      "epoch: 1210    train loss: 0.0000152    val loss: 0.0002819\n",
      "epoch: 1220    train loss: 0.0000151    val loss: 0.0002795\n",
      "epoch: 1230    train loss: 0.0000151    val loss: 0.0002772\n",
      "epoch: 1240    train loss: 0.0000150    val loss: 0.0002750\n",
      "epoch: 1250    train loss: 0.0000150    val loss: 0.0002724\n",
      "epoch: 1260    train loss: 0.0000149    val loss: 0.0002699\n",
      "epoch: 1270    train loss: 0.0000149    val loss: 0.0002675\n",
      "epoch: 1280    train loss: 0.0000148    val loss: 0.0002649\n",
      "epoch: 1290    train loss: 0.0000148    val loss: 0.0002624\n",
      "epoch: 1300    train loss: 0.0000147    val loss: 0.0002598\n",
      "epoch: 1310    train loss: 0.0000147    val loss: 0.0002573\n",
      "epoch: 1320    train loss: 0.0000147    val loss: 0.0002549\n",
      "epoch: 1330    train loss: 0.0000146    val loss: 0.0002526\n",
      "epoch: 1340    train loss: 0.0000146    val loss: 0.0002501\n",
      "epoch: 1350    train loss: 0.0000146    val loss: 0.0002480\n",
      "epoch: 1360    train loss: 0.0000146    val loss: 0.0002459\n",
      "epoch: 1370    train loss: 0.0000146    val loss: 0.0002439\n",
      "epoch: 1380    train loss: 0.0000146    val loss: 0.0002419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1390    train loss: 0.0000145    val loss: 0.0002402\n",
      "epoch: 1400    train loss: 0.0000145    val loss: 0.0002384\n",
      "epoch: 1410    train loss: 0.0000145    val loss: 0.0002369\n",
      "epoch: 1420    train loss: 0.0000145    val loss: 0.0002354\n",
      "epoch: 1430    train loss: 0.0000145    val loss: 0.0002340\n",
      "epoch: 1440    train loss: 0.0000145    val loss: 0.0002327\n",
      "epoch: 1450    train loss: 0.0000145    val loss: 0.0002314\n",
      "epoch: 1460    train loss: 0.0000145    val loss: 0.0002303\n",
      "epoch: 1470    train loss: 0.0000145    val loss: 0.0002291\n",
      "epoch: 1480    train loss: 0.0000145    val loss: 0.0002281\n",
      "epoch: 1490    train loss: 0.0000145    val loss: 0.0002272\n",
      "epoch: 1500    train loss: 0.0000145    val loss: 0.0002263\n",
      "epoch: 1510    train loss: 0.0000145    val loss: 0.0002255\n",
      "epoch: 1520    train loss: 0.0000145    val loss: 0.0002246\n",
      "epoch: 1530    train loss: 0.0000145    val loss: 0.0002240\n",
      "epoch: 1540    train loss: 0.0000144    val loss: 0.0002233\n",
      "epoch: 1550    train loss: 0.0000144    val loss: 0.0002227\n",
      "epoch: 1560    train loss: 0.0000144    val loss: 0.0002222\n",
      "epoch: 1570    train loss: 0.0000144    val loss: 0.0002217\n",
      "epoch: 1580    train loss: 0.0000144    val loss: 0.0002212\n",
      "epoch: 1590    train loss: 0.0000144    val loss: 0.0002208\n",
      "epoch: 1600    train loss: 0.0000144    val loss: 0.0002205\n",
      "epoch: 1610    train loss: 0.0000143    val loss: 0.0002201\n",
      "epoch: 1620    train loss: 0.0000143    val loss: 0.0002199\n",
      "epoch: 1630    train loss: 0.0000143    val loss: 0.0002197\n",
      "epoch: 1640    train loss: 0.0000143    val loss: 0.0002194\n",
      "epoch: 1650    train loss: 0.0000143    val loss: 0.0002192\n",
      "epoch: 1660    train loss: 0.0000143    val loss: 0.0002191\n",
      "epoch: 1670    train loss: 0.0000142    val loss: 0.0002189\n",
      "epoch: 1680    train loss: 0.0000142    val loss: 0.0002188\n",
      "epoch: 1690    train loss: 0.0000142    val loss: 0.0002188\n",
      "epoch: 1700    train loss: 0.0000142    val loss: 0.0002187\n",
      "epoch: 1710    train loss: 0.0000142    val loss: 0.0002187\n",
      "epoch: 1720    train loss: 0.0000141    val loss: 0.0002187\n",
      "epoch: 1730    train loss: 0.0000141    val loss: 0.0002187\n",
      "epoch: 1740    train loss: 0.0000141    val loss: 0.0002187\n",
      "epoch: 1750    train loss: 0.0000141    val loss: 0.0002188\n",
      "epoch: 1760    train loss: 0.0000140    val loss: 0.0002188\n",
      "epoch: 1770    train loss: 0.0000140    val loss: 0.0002188\n",
      "epoch: 1780    train loss: 0.0000140    val loss: 0.0002190\n",
      "epoch: 1790    train loss: 0.0000140    val loss: 0.0002190\n",
      "epoch: 1800    train loss: 0.0000140    val loss: 0.0002192\n",
      "epoch: 1810    train loss: 0.0000139    val loss: 0.0002193\n",
      "epoch: 1820    train loss: 0.0000139    val loss: 0.0002194\n",
      "epoch: 1830    train loss: 0.0000139    val loss: 0.0002195\n",
      "epoch: 1840    train loss: 0.0000139    val loss: 0.0002197\n",
      "epoch: 1850    train loss: 0.0000138    val loss: 0.0002198\n",
      "epoch: 1860    train loss: 0.0000138    val loss: 0.0002199\n",
      "epoch: 1870    train loss: 0.0000138    val loss: 0.0002201\n",
      "epoch: 1880    train loss: 0.0000138    val loss: 0.0002203\n",
      "epoch: 1890    train loss: 0.0000138    val loss: 0.0002204\n",
      "epoch: 1900    train loss: 0.0000137    val loss: 0.0002206\n",
      "epoch: 1910    train loss: 0.0000137    val loss: 0.0002208\n",
      "epoch: 1920    train loss: 0.0000137    val loss: 0.0002210\n",
      "epoch: 1930    train loss: 0.0000137    val loss: 0.0002212\n",
      "epoch: 1940    train loss: 0.0000137    val loss: 0.0002213\n",
      "epoch: 1950    train loss: 0.0000136    val loss: 0.0002215\n",
      "epoch: 1960    train loss: 0.0000136    val loss: 0.0002218\n",
      "epoch: 1970    train loss: 0.0000136    val loss: 0.0002220\n",
      "epoch: 1980    train loss: 0.0000136    val loss: 0.0002222\n",
      "epoch: 1990    train loss: 0.0000136    val loss: 0.0002224\n",
      "epoch: 2000    train loss: 0.0000135    val loss: 0.0002227\n",
      "Wall time: 6h 43min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "feat_in = 2\n",
    "feat_size1 = 4\n",
    "hidden_size1 = 30\n",
    "output_size = n_bus*2\n",
    "lr = 0.0001\n",
    "\n",
    "model = My_GNN_NN(n_bus, feat_in, feat_size1, hidden_size1, output_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train_loss_list, val_loss_list = [], []\n",
    "\n",
    "count=0\n",
    "patience=2000\n",
    "lossMin = 1e10\n",
    "\n",
    "for epoch in range(2001):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_train_prediction = model(batch)\n",
    "        loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch.num_graphs\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss=0\n",
    "    for batch in val_loader:\n",
    "        y_val_prediction = model(batch)\n",
    "        loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "        val_loss += loss.item() * batch.num_graphs\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_loss_list.append(val_loss)\n",
    "\n",
    "    #early stopping\n",
    "    if (val_loss < lossMin):\n",
    "        lossMin = val_loss\n",
    "        count = 0\n",
    "        best_epoch = epoch\n",
    "        best_train_loss = train_loss\n",
    "        best_val_loss = val_loss\n",
    "        model.save_weights(model, \"[PyG] [14 bus] Best_GNN_NN_model.pt\")\n",
    "    else:\n",
    "        count+=1\n",
    "        if(count>patience):\n",
    "            print(\"early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(epoch, train_loss, val_loss))\n",
    "            print(\"best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(best_epoch, best_train_loss, best_val_loss))\n",
    "            break\n",
    "    \n",
    "    if (train_loss <= 0):\n",
    "        print(\"min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(epoch, train_loss, val_loss))\n",
    "        break\n",
    "\n",
    "    if (epoch % 10) == 0:\n",
    "        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last epoch: 2000, train loss: 0.0000135, val loss: 0.0002227\n",
      "best epoch: 1709, train loss: 0.0000142, val loss: 0.0002186\n"
     ]
    }
   ],
   "source": [
    "plt.title('GNN NN on power flow dataset')\n",
    "plt.plot(train_loss_list, label=\"train loss\")\n",
    "plt.plot(val_loss_list, label=\"val loss\")\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"# Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print('last epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(epoch, train_loss, val_loss))\n",
    "print('best epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(best_epoch, best_train_loss, best_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 datapoint] Train output ground-truth: \n",
      "[  10.324 -154.216    9.882 -156.117    9.753 -156.712    9.724 -156.839\n",
      "    9.659 -157.161    9.607 -157.399    9.617 -157.37     9.627 -157.311\n",
      "    9.581 -157.512    9.56  -157.633    9.773 -156.613    9.682 -157.039\n",
      "    9.658 -157.152    9.664 -157.123]\n",
      "[1 datapoint] Train output prediction: \n",
      "[  10.32426 -154.21445    9.8819  -156.1134     9.75351 -156.70868\n",
      "    9.72507 -156.8308     9.65913 -157.15704    9.60815 -157.38731\n",
      "    9.61732 -157.36414    9.62751 -157.3056     9.58268 -157.502\n",
      "    9.56094 -157.61946    9.77273 -156.61284    9.68262 -157.0407\n",
      "    9.65863 -157.15247    9.66405 -157.12569]\n",
      "[1 datapoint] Train loss (MSE): 0.0000072\n",
      "Train loss (MSE): 0.0000093\n",
      "=========================================================================\n",
      "[1 datapoint] Val output ground-truth: \n",
      "[  10.302 -154.788    9.844 -156.745    9.686 -157.465    9.685 -157.484\n",
      "    9.574 -157.989    9.568 -158.053    9.543 -158.141    9.532 -158.187\n",
      "    9.518 -158.294    9.535 -158.209    9.727 -157.275    9.629 -157.731\n",
      "    9.599 -157.869    9.594 -157.896]\n",
      "[1 datapoint] Val output prediction: \n",
      "[  10.3022  -154.78319    9.84332 -156.73907    9.68686 -157.4529\n",
      "    9.68416 -157.4793     9.57577 -157.97375    9.56562 -158.05151\n",
      "    9.54354 -158.12675    9.53403 -158.17114    9.5162  -158.29393\n",
      "    9.53354 -158.21033    9.72643 -157.27193    9.62864 -157.72896\n",
      "    9.59874 -157.86827    9.59361 -157.89337]\n",
      "[1 datapoint] Val loss (MSE): 0.0000343\n",
      "Val loss (MSE): 0.0002227\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_train_prediction_1 = model(train_loader.dataset[0])\n",
    "train_loss_1 = MSE(denormalize_output(y_train_prediction_1, y_val_mean, y_val_std), denormalize_output(y_norm_train[0], y_val_mean, y_val_std))\n",
    "print(\"[1 datapoint] Train output ground-truth: \\n\" + str(y_raw_train[0].detach().numpy()))\n",
    "print(\"[1 datapoint] Train output prediction: \\n\" + str(denormalize_output(y_train_prediction_1, y_val_mean, y_val_std).detach().numpy()))\n",
    "print('[1 datapoint] Train loss (MSE): {:.7f}'.format(train_loss_1))\n",
    "\n",
    "train_loss = 0\n",
    "for batch in train_loader:\n",
    "    pred = model(batch)\n",
    "    loss = MSE(denormalize_output(pred, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "    train_loss += loss.item() * batch.num_graphs\n",
    "train_loss /= len(train_loader.dataset)\n",
    "print('Train loss (MSE): {:.7f}'.format(train_loss))\n",
    "\n",
    "print(\"=========================================================================\")\n",
    "\n",
    "y_val_prediction_1 = model(val_loader.dataset[0])\n",
    "val_loss_1 = MSE(denormalize_output(y_val_prediction_1, y_val_mean, y_val_std), denormalize_output(y_norm_val[0], y_val_mean, y_val_std))\n",
    "print(\"[1 datapoint] Val output ground-truth: \\n\" + str(y_raw_val[0].detach().numpy()))\n",
    "print(\"[1 datapoint] Val output prediction: \\n\" + str(denormalize_output(y_val_prediction_1, y_val_mean, y_val_std).detach().numpy()))\n",
    "print('[1 datapoint] Val loss (MSE): {:.7f}'.format(val_loss_1))\n",
    "\n",
    "val_loss=0\n",
    "for batch in val_loader:\n",
    "    pred = model(batch)\n",
    "    loss = MSE(denormalize_output(pred, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "    val_loss += loss.item() * batch.num_graphs\n",
    "val_loss /= len(val_loader.dataset)\n",
    "print('Val loss (MSE): {:.7f}'.format(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 datapoint] Train output ground-truth: \n",
      "[  10.324 -154.216    9.882 -156.117    9.753 -156.712    9.724 -156.839\n",
      "    9.659 -157.161    9.607 -157.399    9.617 -157.37     9.627 -157.311\n",
      "    9.581 -157.512    9.56  -157.633    9.773 -156.613    9.682 -157.039\n",
      "    9.658 -157.152    9.664 -157.123]\n",
      "[1 datapoint] Train output prediction: \n",
      "[  10.32422 -154.21547    9.8818  -156.1143     9.75341 -156.70953\n",
      "    9.72493 -156.83214    9.65899 -157.1584     9.60793 -157.38869\n",
      "    9.61718 -157.36559    9.6274  -157.30704    9.58245 -157.50336\n",
      "    9.56066 -157.62135    9.77269 -156.61354    9.68267 -157.04115\n",
      "    9.65849 -157.15309    9.66419 -157.12685]\n",
      "[1 datapoint] Train loss (MSE): 0.0000036\n",
      "Train loss (MSE): 0.0000085\n",
      "=========================================================================\n",
      "[1 datapoint] Val output ground-truth: \n",
      "[  10.302 -154.788    9.844 -156.745    9.686 -157.465    9.685 -157.484\n",
      "    9.574 -157.989    9.568 -158.053    9.543 -158.141    9.532 -158.187\n",
      "    9.518 -158.294    9.535 -158.209    9.727 -157.275    9.629 -157.731\n",
      "    9.599 -157.869    9.594 -157.896]\n",
      "[1 datapoint] Val output prediction: \n",
      "[  10.30222 -154.78305    9.84334 -156.73929    9.68684 -157.45308\n",
      "    9.68415 -157.47981    9.57582 -157.97388    9.56569 -158.05196\n",
      "    9.54353 -158.12715    9.53407 -158.17162    9.51628 -158.2948\n",
      "    9.53352 -158.21109    9.72653 -157.27176    9.62882 -157.72849\n",
      "    9.59902 -157.86754    9.5938  -157.89267]\n",
      "[1 datapoint] Val loss (MSE): 0.0000332\n",
      "Val loss (MSE): 0.0002186\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load(\"[PyG] [14 bus] Best_GNN_NN_model.pt\")\n",
    "best_model.eval()\n",
    "\n",
    "y_train_prediction_1 = best_model(train_loader.dataset[0])\n",
    "train_loss_1 = MSE(denormalize_output(y_train_prediction_1, y_val_mean, y_val_std), denormalize_output(y_norm_train[0], y_val_mean, y_val_std))\n",
    "print(\"[1 datapoint] Train output ground-truth: \\n\" + str(y_raw_train[0].detach().numpy()))\n",
    "print(\"[1 datapoint] Train output prediction: \\n\" + str(denormalize_output(y_train_prediction_1, y_val_mean, y_val_std).detach().numpy()))\n",
    "print('[1 datapoint] Train loss (MSE): {:.7f}'.format(train_loss_1))\n",
    "\n",
    "train_loss = 0\n",
    "for batch in train_loader:\n",
    "    pred = best_model(batch)\n",
    "    loss = MSE(denormalize_output(pred, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "    train_loss += loss.item() * batch.num_graphs\n",
    "train_loss /= len(train_loader.dataset)\n",
    "print('Train loss (MSE): {:.7f}'.format(train_loss))\n",
    "\n",
    "print(\"=========================================================================\")\n",
    "\n",
    "y_val_prediction_1 = best_model(val_loader.dataset[0])\n",
    "val_loss_1 = MSE(denormalize_output(y_val_prediction_1, y_val_mean, y_val_std), denormalize_output(y_norm_val[0], y_val_mean, y_val_std))\n",
    "print(\"[1 datapoint] Val output ground-truth: \\n\" + str(y_raw_val[0].detach().numpy()))\n",
    "print(\"[1 datapoint] Val output prediction: \\n\" + str(denormalize_output(y_val_prediction_1, y_val_mean, y_val_std).detach().numpy()))\n",
    "print('[1 datapoint] Val loss (MSE): {:.7f}'.format(val_loss_1))\n",
    "\n",
    "val_loss=0\n",
    "for batch in val_loader:\n",
    "    pred = best_model(batch)\n",
    "    loss = MSE(denormalize_output(pred, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "    val_loss += loss.item() * batch.num_graphs\n",
    "val_loss /= len(val_loader.dataset)\n",
    "print('Val loss (MSE): {:.7f}'.format(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 1\n",
      "Train loss (MSE): 0.0000085\n",
      "===========================\n",
      "dataset 2\n",
      "Val loss (MSE): 0.0002186\n",
      "===========================\n",
      "dataset 3\n",
      "Test loss (MSE): 0.0000748\n",
      "===========================\n",
      "dataset 4\n",
      "Test loss (MSE): 0.0001593\n",
      "===========================\n",
      "dataset 5\n",
      "Test loss (MSE): 0.0001090\n",
      "===========================\n",
      "dataset 6\n",
      "Test loss (MSE): 0.0000295\n",
      "===========================\n",
      "dataset 7\n",
      "Test loss (MSE): 0.0001177\n",
      "===========================\n",
      "dataset 8\n",
      "Test loss (MSE): 0.0001252\n",
      "===========================\n",
      "dataset 9\n",
      "Test loss (MSE): 0.0000823\n",
      "===========================\n",
      "dataset 10\n",
      "Test loss (MSE): 0.0001097\n",
      "===========================\n",
      "dataset 11\n",
      "Test loss (MSE): 0.0000991\n",
      "===========================\n",
      "dataset 12\n",
      "Test loss (MSE): 0.0002357\n",
      "===========================\n",
      "dataset 13\n",
      "Test loss (MSE): 0.0000383\n",
      "===========================\n",
      "dataset 14\n",
      "Test loss (MSE): 0.0000884\n",
      "===========================\n",
      "dataset 15\n",
      "Test loss (MSE): 0.0000705\n",
      "===========================\n",
      "dataset 16\n",
      "Test loss (MSE): 0.0000512\n",
      "===========================\n",
      "dataset 17\n",
      "Test loss (MSE): 0.0000496\n",
      "===========================\n",
      "dataset 18\n",
      "Test loss (MSE): 0.0001267\n",
      "===========================\n",
      "dataset 19\n",
      "Test loss (MSE): 0.0001037\n",
      "===========================\n",
      "dataset 20\n",
      "Test loss (MSE): 0.0001962\n",
      "===========================\n",
      "dataset 21\n",
      "Test loss (MSE): 0.0001072\n",
      "===========================\n",
      "dataset 22\n",
      "Test loss (MSE): 0.0000795\n",
      "===========================\n",
      "dataset 23\n",
      "Test loss (MSE): 0.0000910\n",
      "===========================\n",
      "dataset 24\n",
      "Test loss (MSE): 0.0001169\n",
      "===========================\n",
      "dataset 25\n",
      "Test loss (MSE): 0.0000881\n",
      "===========================\n",
      "dataset 26\n",
      "Test loss (MSE): 0.0000709\n",
      "===========================\n",
      "dataset 27\n",
      "Test loss (MSE): 0.0001380\n",
      "===========================\n",
      "dataset 28\n",
      "Test loss (MSE): 0.0001573\n",
      "===========================\n",
      "dataset 29\n",
      "Test loss (MSE): 0.0000848\n",
      "===========================\n",
      "dataset 30\n",
      "Test loss (MSE): 0.0001792\n",
      "===========================\n",
      "dataset 31\n",
      "Test loss (MSE): 0.0001414\n",
      "===========================\n",
      "dataset 32\n",
      "Test loss (MSE): 0.0000614\n",
      "===========================\n",
      "dataset 33\n",
      "Test loss (MSE): 0.0000517\n",
      "===========================\n",
      "dataset 34\n",
      "Test loss (MSE): 0.0000673\n",
      "===========================\n",
      "dataset 35\n",
      "Test loss (MSE): 0.0000445\n",
      "===========================\n",
      "dataset 36\n",
      "Test loss (MSE): 0.0001007\n",
      "===========================\n",
      "dataset 37\n",
      "Test loss (MSE): 0.0000626\n",
      "===========================\n",
      "dataset 38\n",
      "Test loss (MSE): 0.0001100\n",
      "===========================\n",
      "dataset 39\n",
      "Test loss (MSE): 0.0000896\n",
      "===========================\n",
      "dataset 40\n",
      "Test loss (MSE): 0.0001481\n",
      "===========================\n",
      "dataset 41\n",
      "Test loss (MSE): 0.0000396\n",
      "===========================\n",
      "dataset 42\n",
      "Test loss (MSE): 0.0001398\n",
      "===========================\n",
      "dataset 43\n",
      "Test loss (MSE): 0.0000849\n",
      "===========================\n",
      "dataset 44\n",
      "Test loss (MSE): 0.0001415\n",
      "===========================\n",
      "dataset 45\n",
      "Test loss (MSE): 0.0001181\n",
      "===========================\n",
      "dataset 46\n",
      "Test loss (MSE): 0.0001184\n",
      "===========================\n",
      "dataset 47\n",
      "Test loss (MSE): 0.0001199\n",
      "===========================\n",
      "dataset 48\n",
      "Test loss (MSE): 0.0001077\n",
      "===========================\n",
      "dataset 49\n",
      "Test loss (MSE): 0.0000627\n",
      "===========================\n",
      "dataset 50\n",
      "Test loss (MSE): 0.0000646\n",
      "===========================\n",
      "dataset 51\n",
      "Test loss (MSE): 0.0000981\n",
      "===========================\n",
      "dataset 52\n",
      "Test loss (MSE): 0.0002178\n",
      "===========================\n",
      "dataset 53\n",
      "Test loss (MSE): 0.0000488\n",
      "===========================\n",
      "dataset 54\n",
      "Test loss (MSE): 0.0001130\n",
      "===========================\n",
      "dataset 55\n",
      "Test loss (MSE): 0.0001442\n",
      "===========================\n",
      "dataset 56\n",
      "Test loss (MSE): 0.0001205\n",
      "===========================\n",
      "dataset 57\n",
      "Test loss (MSE): 0.0000925\n",
      "===========================\n",
      "dataset 58\n",
      "Test loss (MSE): 0.0000702\n",
      "===========================\n",
      "dataset 59\n",
      "Test loss (MSE): 0.0001590\n",
      "===========================\n",
      "dataset 60\n",
      "Test loss (MSE): 0.0002916\n",
      "===========================\n",
      "dataset 61\n",
      "Test loss (MSE): 0.0000794\n",
      "===========================\n",
      "dataset 62\n",
      "Test loss (MSE): 0.0001704\n",
      "===========================\n",
      "dataset 63\n",
      "Test loss (MSE): 0.0000604\n",
      "===========================\n",
      "dataset 64\n",
      "Test loss (MSE): 0.0001507\n",
      "===========================\n",
      "dataset 65\n",
      "Test loss (MSE): 0.0000731\n",
      "===========================\n",
      "dataset 66\n",
      "Test loss (MSE): 0.0002235\n",
      "===========================\n",
      "dataset 67\n",
      "Test loss (MSE): 0.0000837\n",
      "===========================\n",
      "dataset 68\n",
      "Test loss (MSE): 0.0001625\n",
      "===========================\n",
      "dataset 69\n",
      "Test loss (MSE): 0.0000660\n",
      "===========================\n",
      "dataset 70\n",
      "Test loss (MSE): 0.0001870\n",
      "===========================\n",
      "dataset 71\n",
      "Test loss (MSE): 0.0001601\n",
      "===========================\n",
      "dataset 72\n",
      "Test loss (MSE): 0.0000820\n",
      "===========================\n",
      "dataset 73\n",
      "Test loss (MSE): 0.0002345\n",
      "===========================\n",
      "dataset 74\n",
      "Test loss (MSE): 0.0000875\n",
      "===========================\n",
      "dataset 75\n",
      "Test loss (MSE): 0.0001322\n",
      "===========================\n",
      "dataset 76\n",
      "Test loss (MSE): 0.0000748\n",
      "===========================\n",
      "dataset 77\n",
      "Test loss (MSE): 0.0000889\n",
      "===========================\n",
      "dataset 78\n",
      "Test loss (MSE): 0.0003347\n",
      "===========================\n",
      "dataset 79\n",
      "Test loss (MSE): 0.0000687\n",
      "===========================\n",
      "dataset 80\n",
      "Test loss (MSE): 0.0000376\n",
      "===========================\n",
      "dataset 81\n",
      "Test loss (MSE): 0.0001846\n",
      "===========================\n",
      "dataset 82\n",
      "Test loss (MSE): 0.0001347\n",
      "===========================\n",
      "dataset 83\n",
      "Test loss (MSE): 0.0001373\n",
      "===========================\n",
      "dataset 84\n",
      "Test loss (MSE): 0.0000560\n",
      "===========================\n",
      "dataset 85\n",
      "Test loss (MSE): 0.0001375\n",
      "===========================\n",
      "dataset 86\n",
      "Test loss (MSE): 0.0000538\n",
      "===========================\n",
      "dataset 87\n",
      "Test loss (MSE): 0.0000870\n",
      "===========================\n",
      "dataset 88\n",
      "Test loss (MSE): 0.0001326\n",
      "===========================\n",
      "dataset 89\n",
      "Test loss (MSE): 0.0003985\n",
      "===========================\n",
      "dataset 90\n",
      "Test loss (MSE): 0.0000783\n",
      "===========================\n",
      "dataset 91\n",
      "Test loss (MSE): 0.0002681\n",
      "===========================\n",
      "dataset 92\n",
      "Test loss (MSE): 0.0001718\n",
      "===========================\n",
      "dataset 93\n",
      "Test loss (MSE): 0.0001963\n",
      "===========================\n",
      "dataset 94\n",
      "Test loss (MSE): 0.0000304\n",
      "===========================\n",
      "dataset 95\n",
      "Test loss (MSE): 0.0001934\n",
      "===========================\n",
      "dataset 96\n",
      "Test loss (MSE): 0.0001181\n",
      "===========================\n",
      "dataset 97\n",
      "Test loss (MSE): 0.0001084\n",
      "===========================\n",
      "dataset 98\n",
      "Test loss (MSE): 0.0000562\n",
      "===========================\n",
      "dataset 99\n",
      "Test loss (MSE): 0.0001121\n",
      "===========================\n",
      "dataset 100\n",
      "Test loss (MSE): 0.0000662\n",
      "===========================\n",
      "dataset 101\n",
      "Test loss (MSE): 0.0001319\n",
      "===========================\n",
      "dataset 102\n",
      "Test loss (MSE): 0.0000687\n",
      "===========================\n",
      "\n",
      "test loss file saved!\n",
      "\n",
      "Wall time: 9min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model = torch.load(\"[PyG] [14 bus] Best_GNN_NN_model.pt\")\n",
    "best_model.eval()\n",
    "\n",
    "test_loss_list = []\n",
    "\n",
    "for i in range(102):\n",
    "    \n",
    "    dataset = pd.read_excel('dataset\\Grid_14 bus_%d.xlsx' % (i+1)).values\n",
    "    test_percentage = 100\n",
    "    test_dataset = slice_dataset(dataset, test_percentage)\n",
    "    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)\n",
    "    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)\n",
    "    \n",
    "    x_test, y_test = x_norm_test, y_norm_test\n",
    "    \n",
    "    data_test_list = []\n",
    "    for j,_ in enumerate(x_test):\n",
    "        data_test_list.append(Data(x=x_test[j], y=y_test[j], edge_index=edge_index))\n",
    "\n",
    "    test_loader = DataLoader(data_test_list, batch_size=1)\n",
    "    \n",
    "    print('dataset {:d}'.format(i+1))\n",
    "    \n",
    "    test_loss = 0\n",
    "    for batch in test_loader:\n",
    "        y_test_prediction = best_model(batch)\n",
    "        loss = MSE(denormalize_output(y_test_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))\n",
    "        test_loss += loss.item() * batch.num_graphs\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    if i == 0:\n",
    "        print('Train loss (MSE): {:.7f}'.format(test_loss))\n",
    "    elif i == 1:\n",
    "        print('Val loss (MSE): {:.7f}'.format(test_loss))\n",
    "    else:\n",
    "        print('Test loss (MSE): {:.7f}'.format(test_loss))\n",
    "        test_loss_list.append(test_loss)\n",
    "    \n",
    "    print(\"===========================\")\n",
    "\n",
    "column = []\n",
    "for i in range(100):\n",
    "    column.append('test loss %d' % (i+1))\n",
    "    \n",
    "test_loss_file = pd.DataFrame([test_loss_list], columns=column)\n",
    "test_loss_file.to_excel(\"[PyG] [14 bus] [MSE] GNN NN test loss.xlsx\")\n",
    "print(\"\\ntest loss file saved!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 1\n",
      "Train loss (NRMSE): 0.0250480\n",
      "===========================\n",
      "dataset 2\n",
      "Val loss (NRMSE): 0.0256365\n",
      "===========================\n",
      "dataset 3\n",
      "Test loss (NRMSE): 0.0249336\n",
      "===========================\n",
      "dataset 4\n",
      "Test loss (NRMSE): 0.0296441\n",
      "===========================\n",
      "dataset 5\n",
      "Test loss (NRMSE): 0.0302202\n",
      "===========================\n",
      "dataset 6\n",
      "Test loss (NRMSE): 0.0246259\n",
      "===========================\n",
      "dataset 7\n",
      "Test loss (NRMSE): 0.0514972\n",
      "===========================\n",
      "dataset 8\n",
      "Test loss (NRMSE): 0.0695113\n",
      "===========================\n",
      "dataset 9\n",
      "Test loss (NRMSE): 0.0302408\n",
      "===========================\n",
      "dataset 10\n",
      "Test loss (NRMSE): 0.0574157\n",
      "===========================\n",
      "dataset 11\n",
      "Test loss (NRMSE): 0.0457214\n",
      "===========================\n",
      "dataset 12\n",
      "Test loss (NRMSE): 0.0379261\n",
      "===========================\n",
      "dataset 13\n",
      "Test loss (NRMSE): 0.0250264\n",
      "===========================\n",
      "dataset 14\n",
      "Test loss (NRMSE): 0.0277047\n",
      "===========================\n",
      "dataset 15\n",
      "Test loss (NRMSE): 0.0276273\n",
      "===========================\n",
      "dataset 16\n",
      "Test loss (NRMSE): 0.0319469\n",
      "===========================\n",
      "dataset 17\n",
      "Test loss (NRMSE): 0.0331290\n",
      "===========================\n",
      "dataset 18\n",
      "Test loss (NRMSE): 0.0399479\n",
      "===========================\n",
      "dataset 19\n",
      "Test loss (NRMSE): 0.0287043\n",
      "===========================\n",
      "dataset 20\n",
      "Test loss (NRMSE): 0.0317926\n",
      "===========================\n",
      "dataset 21\n",
      "Test loss (NRMSE): 0.0416937\n",
      "===========================\n",
      "dataset 22\n",
      "Test loss (NRMSE): 0.0358849\n",
      "===========================\n",
      "dataset 23\n",
      "Test loss (NRMSE): 0.0340770\n",
      "===========================\n",
      "dataset 24\n",
      "Test loss (NRMSE): 0.0583493\n",
      "===========================\n",
      "dataset 25\n",
      "Test loss (NRMSE): 0.0418610\n",
      "===========================\n",
      "dataset 26\n",
      "Test loss (NRMSE): 0.0294159\n",
      "===========================\n",
      "dataset 27\n",
      "Test loss (NRMSE): 0.0395584\n",
      "===========================\n",
      "dataset 28\n",
      "Test loss (NRMSE): 0.0309717\n",
      "===========================\n",
      "dataset 29\n",
      "Test loss (NRMSE): 0.0271518\n",
      "===========================\n",
      "dataset 30\n",
      "Test loss (NRMSE): 0.0557151\n",
      "===========================\n",
      "dataset 31\n",
      "Test loss (NRMSE): 0.0492371\n",
      "===========================\n",
      "dataset 32\n",
      "Test loss (NRMSE): 0.0237049\n",
      "===========================\n",
      "dataset 33\n",
      "Test loss (NRMSE): 0.0396953\n",
      "===========================\n",
      "dataset 34\n",
      "Test loss (NRMSE): 0.0278041\n",
      "===========================\n",
      "dataset 35\n",
      "Test loss (NRMSE): 0.0617477\n",
      "===========================\n",
      "dataset 36\n",
      "Test loss (NRMSE): 0.0327501\n",
      "===========================\n",
      "dataset 37\n",
      "Test loss (NRMSE): 0.0250761\n",
      "===========================\n",
      "dataset 38\n",
      "Test loss (NRMSE): 0.0453079\n",
      "===========================\n",
      "dataset 39\n",
      "Test loss (NRMSE): 0.0409870\n",
      "===========================\n",
      "dataset 40\n",
      "Test loss (NRMSE): 0.0529536\n",
      "===========================\n",
      "dataset 41\n",
      "Test loss (NRMSE): 0.0294512\n",
      "===========================\n",
      "dataset 42\n",
      "Test loss (NRMSE): 0.0345243\n",
      "===========================\n",
      "dataset 43\n",
      "Test loss (NRMSE): 0.0307512\n",
      "===========================\n",
      "dataset 44\n",
      "Test loss (NRMSE): 0.0322385\n",
      "===========================\n",
      "dataset 45\n",
      "Test loss (NRMSE): 0.0293174\n",
      "===========================\n",
      "dataset 46\n",
      "Test loss (NRMSE): 0.0310732\n",
      "===========================\n",
      "dataset 47\n",
      "Test loss (NRMSE): 0.0305259\n",
      "===========================\n",
      "dataset 48\n",
      "Test loss (NRMSE): 0.0325948\n",
      "===========================\n",
      "dataset 49\n",
      "Test loss (NRMSE): 0.0423496\n",
      "===========================\n",
      "dataset 50\n",
      "Test loss (NRMSE): 0.0317341\n",
      "===========================\n",
      "dataset 51\n",
      "Test loss (NRMSE): 0.0297772\n",
      "===========================\n",
      "dataset 52\n",
      "Test loss (NRMSE): 0.0339954\n",
      "===========================\n",
      "dataset 53\n",
      "Test loss (NRMSE): 0.0369017\n",
      "===========================\n",
      "dataset 54\n",
      "Test loss (NRMSE): 0.0291130\n",
      "===========================\n",
      "dataset 55\n",
      "Test loss (NRMSE): 0.0313527\n",
      "===========================\n",
      "dataset 56\n",
      "Test loss (NRMSE): 0.0399362\n",
      "===========================\n",
      "dataset 57\n",
      "Test loss (NRMSE): 0.0310404\n",
      "===========================\n",
      "dataset 58\n",
      "Test loss (NRMSE): 0.0366069\n",
      "===========================\n",
      "dataset 59\n",
      "Test loss (NRMSE): 0.0312510\n",
      "===========================\n",
      "dataset 60\n",
      "Test loss (NRMSE): 0.0291163\n",
      "===========================\n",
      "dataset 61\n",
      "Test loss (NRMSE): 0.0315006\n",
      "===========================\n",
      "dataset 62\n",
      "Test loss (NRMSE): 0.0329004\n",
      "===========================\n",
      "dataset 63\n",
      "Test loss (NRMSE): 0.0412340\n",
      "===========================\n",
      "dataset 64\n",
      "Test loss (NRMSE): 0.0294884\n",
      "===========================\n",
      "dataset 65\n",
      "Test loss (NRMSE): 0.0257962\n",
      "===========================\n",
      "dataset 66\n",
      "Test loss (NRMSE): 0.0305600\n",
      "===========================\n",
      "dataset 67\n",
      "Test loss (NRMSE): 0.0291493\n",
      "===========================\n",
      "dataset 68\n",
      "Test loss (NRMSE): 0.0322921\n",
      "===========================\n",
      "dataset 69\n",
      "Test loss (NRMSE): 0.0348039\n",
      "===========================\n",
      "dataset 70\n",
      "Test loss (NRMSE): 0.0561432\n",
      "===========================\n",
      "dataset 71\n",
      "Test loss (NRMSE): 0.0308631\n",
      "===========================\n",
      "dataset 72\n",
      "Test loss (NRMSE): 0.0352247\n",
      "===========================\n",
      "dataset 73\n",
      "Test loss (NRMSE): 0.0626452\n",
      "===========================\n",
      "dataset 74\n",
      "Test loss (NRMSE): 0.0295727\n",
      "===========================\n",
      "dataset 75\n",
      "Test loss (NRMSE): 0.0409559\n",
      "===========================\n",
      "dataset 76\n",
      "Test loss (NRMSE): 0.0263423\n",
      "===========================\n",
      "dataset 77\n",
      "Test loss (NRMSE): 0.0302091\n",
      "===========================\n",
      "dataset 78\n",
      "Test loss (NRMSE): 0.0406679\n",
      "===========================\n",
      "dataset 79\n",
      "Test loss (NRMSE): 0.0262708\n",
      "===========================\n",
      "dataset 80\n",
      "Test loss (NRMSE): 0.0411890\n",
      "===========================\n",
      "dataset 81\n",
      "Test loss (NRMSE): 0.0298664\n",
      "===========================\n",
      "dataset 82\n",
      "Test loss (NRMSE): 0.0489895\n",
      "===========================\n",
      "dataset 83\n",
      "Test loss (NRMSE): 0.0304142\n",
      "===========================\n",
      "dataset 84\n",
      "Test loss (NRMSE): 0.0444167\n",
      "===========================\n",
      "dataset 85\n",
      "Test loss (NRMSE): 0.0284624\n",
      "===========================\n",
      "dataset 86\n",
      "Test loss (NRMSE): 0.0569501\n",
      "===========================\n",
      "dataset 87\n",
      "Test loss (NRMSE): 0.0376496\n",
      "===========================\n",
      "dataset 88\n",
      "Test loss (NRMSE): 0.0303921\n",
      "===========================\n",
      "dataset 89\n",
      "Test loss (NRMSE): 0.0294698\n",
      "===========================\n",
      "dataset 90\n",
      "Test loss (NRMSE): 0.0380682\n",
      "===========================\n",
      "dataset 91\n",
      "Test loss (NRMSE): 0.0287727\n",
      "===========================\n",
      "dataset 92\n",
      "Test loss (NRMSE): 0.0454142\n",
      "===========================\n",
      "dataset 93\n",
      "Test loss (NRMSE): 0.0812481\n",
      "===========================\n",
      "dataset 94\n",
      "Test loss (NRMSE): 0.0350376\n",
      "===========================\n",
      "dataset 95\n",
      "Test loss (NRMSE): 0.0273244\n",
      "===========================\n",
      "dataset 96\n",
      "Test loss (NRMSE): 0.0428654\n",
      "===========================\n",
      "dataset 97\n",
      "Test loss (NRMSE): 0.0217905\n",
      "===========================\n",
      "dataset 98\n",
      "Test loss (NRMSE): 0.0531996\n",
      "===========================\n",
      "dataset 99\n",
      "Test loss (NRMSE): 0.0321515\n",
      "===========================\n",
      "dataset 100\n",
      "Test loss (NRMSE): 0.0276727\n",
      "===========================\n",
      "dataset 101\n",
      "Test loss (NRMSE): 0.0237809\n",
      "===========================\n",
      "dataset 102\n",
      "Test loss (NRMSE): 0.0454936\n",
      "===========================\n",
      "\n",
      "test loss file saved!\n",
      "\n",
      "Wall time: 20min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model = torch.load(\"[PyG] [14 bus] Best_GNN_NN_model.pt\")\n",
    "best_model.eval()\n",
    "\n",
    "test_loss_list = []\n",
    "\n",
    "for i in range(102):\n",
    "    \n",
    "    dataset = pd.read_excel('dataset\\Grid_14 bus_%d.xlsx' % (i+1)).values\n",
    "    test_percentage = 100\n",
    "    test_dataset = slice_dataset(dataset, test_percentage)\n",
    "    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)\n",
    "    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)\n",
    "    \n",
    "    x_test, y_test = x_norm_test, y_norm_test\n",
    "    \n",
    "    data_test_list = []\n",
    "    for j,_ in enumerate(x_test):\n",
    "        data_test_list.append(Data(x=x_test[j], y=y_test[j], edge_index=edge_index))\n",
    "\n",
    "    test_loader = DataLoader(data_test_list, batch_size=1)\n",
    "    \n",
    "    print('dataset {:d}'.format(i+1))\n",
    "    \n",
    "    test_loss = 0\n",
    "    yhat = torch.empty(0, n_bus*2)\n",
    "    for batch in test_loader:\n",
    "        y_test_prediction = best_model(batch)\n",
    "        yhat = torch.cat((yhat, y_test_prediction.reshape(1, n_bus*2)))\n",
    "    \n",
    "    yhat = denormalize_output(yhat, y_val_mean, y_val_std)\n",
    "    y = y_raw_test\n",
    "    test_loss_NRMSE = NRMSE(yhat, y)\n",
    "    \n",
    "    if i == 0:\n",
    "        print('Train loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))\n",
    "    elif i == 1:\n",
    "        print('Val loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))\n",
    "    else:\n",
    "        print('Test loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))\n",
    "        test_loss_list.append(test_loss_NRMSE.detach().numpy())\n",
    "    \n",
    "    print(\"===========================\")\n",
    "\n",
    "column = []\n",
    "for i in range(100):\n",
    "    column.append('test loss %d' % (i+1))\n",
    "    \n",
    "test_loss_file = pd.DataFrame([test_loss_list], columns=column)\n",
    "test_loss_file.to_excel(\"[PyG] [14 bus] [NRMSE] GNN NN test loss.xlsx\")\n",
    "print(\"\\ntest loss file saved!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
