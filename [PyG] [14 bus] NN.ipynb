{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "torch.set_printoptions(precision=5, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dataset(dataset, percentage):\n",
    "    data_size = len(dataset)\n",
    "    return dataset[:int(data_size*percentage/100)]\n",
    "\n",
    "def make_dataset(dataset, n_bus):\n",
    "    x_raw_1, y_raw_1 = [], []\n",
    "    x_raw, y_raw = [], []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        for n in range(n_bus):\n",
    "            x_raw_1.extend(dataset[i, 4*n+1:4*n+3])\n",
    "            y_raw_1.extend(dataset[i, 4*n+3:4*n+5])\n",
    "        x_raw.append(x_raw_1)\n",
    "        y_raw.append(y_raw_1)\n",
    "        x_raw_1, y_raw_1 = [], []\n",
    "        \n",
    "    x_raw = torch.tensor(x_raw, dtype=torch.float)\n",
    "    y_raw = torch.tensor(y_raw, dtype=torch.float)\n",
    "    return x_raw, y_raw\n",
    "\n",
    "def normalize_dataset(x, y):\n",
    "    x_mean = torch.mean(x,0)\n",
    "    y_mean = torch.mean(y,0)\n",
    "    x_std = torch.std(x,0)\n",
    "    y_std = torch.std(y,0)\n",
    "    x_norm = (x-x_mean)/x_std\n",
    "    y_norm = (y-y_mean)/y_std\n",
    "    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)\n",
    "    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)\n",
    "    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)\n",
    "    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)\n",
    "    return x_norm, y_norm, x_mean, y_mean, x_std, y_std\n",
    "\n",
    "def denormalize_output(y_norm, y_mean, y_std):\n",
    "    y = y_norm*y_std+y_mean\n",
    "    return y\n",
    "\n",
    "def NRMSE(yhat,y):\n",
    "    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))\n",
    "\n",
    "def MSE(yhat,y):\n",
    "    return torch.mean((yhat-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_excel('dataset\\Grid_14 bus_1.xlsx').values\n",
    "dataset2 = pd.read_excel('dataset\\Grid_14 bus_2.xlsx').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 100\n",
    "val_percentage = 100\n",
    "\n",
    "train_dataset = slice_dataset(dataset1, train_percentage)\n",
    "val_dataset = slice_dataset(dataset2, val_percentage)\n",
    "\n",
    "n_bus = 14\n",
    "\n",
    "#actual data\n",
    "x_raw_train, y_raw_train = make_dataset(train_dataset, n_bus)\n",
    "x_raw_val, y_raw_val = make_dataset(val_dataset, n_bus)\n",
    "\n",
    "#normalized data\n",
    "x_norm_train, y_norm_train, _, _, _, _ = normalize_dataset(x_raw_train, y_raw_train)\n",
    "x_norm_val, y_norm_val, x_val_mean, y_val_mean, x_val_std, y_val_std = normalize_dataset(x_raw_val, y_raw_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_NN(torch.nn.Module):\n",
    "    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, output_size=None):\n",
    "        super(My_NN, self).__init__()\n",
    "        self.input_size = input_size if input_size is not None else 18 \n",
    "        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38\n",
    "        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38\n",
    "        self.output_size = output_size if output_size is not None else 18\n",
    "        \n",
    "        self.lin1 = Linear(self.input_size, self.hidden_size1)\n",
    "        self.lin2 = Linear(self.hidden_size1, self.hidden_size2)\n",
    "        self.lin3 = Linear(self.hidden_size2, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        x = self.lin2(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def save_weights(self, model, name):\n",
    "        torch.save(model, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0    train loss: 0.3572779    val loss: 0.3549372\n",
      "epoch: 10    train loss: 0.3271315    val loss: 0.3258092\n",
      "epoch: 20    train loss: 0.2890157    val loss: 0.2879375\n",
      "epoch: 30    train loss: 0.2325724    val loss: 0.2308816\n",
      "epoch: 40    train loss: 0.1559985    val loss: 0.1535463\n",
      "epoch: 50    train loss: 0.0816438    val loss: 0.0793228\n",
      "epoch: 60    train loss: 0.0394573    val loss: 0.0375504\n",
      "epoch: 70    train loss: 0.0226896    val loss: 0.0213710\n",
      "epoch: 80    train loss: 0.0166587    val loss: 0.0159275\n",
      "epoch: 90    train loss: 0.0147257    val loss: 0.0142873\n",
      "epoch: 100    train loss: 0.0133413    val loss: 0.0130554\n",
      "epoch: 110    train loss: 0.0122153    val loss: 0.0119590\n",
      "epoch: 120    train loss: 0.0111635    val loss: 0.0109526\n",
      "epoch: 130    train loss: 0.0102636    val loss: 0.0100893\n",
      "epoch: 140    train loss: 0.0094546    val loss: 0.0093021\n",
      "epoch: 150    train loss: 0.0087223    val loss: 0.0085963\n",
      "epoch: 160    train loss: 0.0080692    val loss: 0.0079720\n",
      "epoch: 170    train loss: 0.0074929    val loss: 0.0074187\n",
      "epoch: 180    train loss: 0.0069881    val loss: 0.0069389\n",
      "epoch: 190    train loss: 0.0065479    val loss: 0.0065313\n",
      "epoch: 200    train loss: 0.0061642    val loss: 0.0061826\n",
      "epoch: 210    train loss: 0.0058287    val loss: 0.0058820\n",
      "epoch: 220    train loss: 0.0055335    val loss: 0.0056234\n",
      "epoch: 230    train loss: 0.0052716    val loss: 0.0053996\n",
      "epoch: 240    train loss: 0.0050373    val loss: 0.0052034\n",
      "epoch: 250    train loss: 0.0048257    val loss: 0.0050295\n",
      "epoch: 260    train loss: 0.0046331    val loss: 0.0048740\n",
      "epoch: 270    train loss: 0.0044563    val loss: 0.0047333\n",
      "epoch: 280    train loss: 0.0042930    val loss: 0.0046051\n",
      "epoch: 290    train loss: 0.0041414    val loss: 0.0044874\n",
      "epoch: 300    train loss: 0.0040000    val loss: 0.0043784\n",
      "epoch: 310    train loss: 0.0038676    val loss: 0.0042771\n",
      "epoch: 320    train loss: 0.0037432    val loss: 0.0041825\n",
      "epoch: 330    train loss: 0.0036261    val loss: 0.0040937\n",
      "epoch: 340    train loss: 0.0035156    val loss: 0.0040101\n",
      "epoch: 350    train loss: 0.0034111    val loss: 0.0039312\n",
      "epoch: 360    train loss: 0.0033121    val loss: 0.0038564\n",
      "epoch: 370    train loss: 0.0032182    val loss: 0.0037853\n",
      "epoch: 380    train loss: 0.0031289    val loss: 0.0037177\n",
      "epoch: 390    train loss: 0.0030438    val loss: 0.0036531\n",
      "epoch: 400    train loss: 0.0029628    val loss: 0.0035914\n",
      "epoch: 410    train loss: 0.0028854    val loss: 0.0035323\n",
      "epoch: 420    train loss: 0.0028115    val loss: 0.0034756\n",
      "epoch: 430    train loss: 0.0027407    val loss: 0.0034211\n",
      "epoch: 440    train loss: 0.0026729    val loss: 0.0033686\n",
      "epoch: 450    train loss: 0.0026078    val loss: 0.0033180\n",
      "epoch: 460    train loss: 0.0025453    val loss: 0.0032691\n",
      "epoch: 470    train loss: 0.0024853    val loss: 0.0032219\n",
      "epoch: 480    train loss: 0.0024275    val loss: 0.0031763\n",
      "epoch: 490    train loss: 0.0023718    val loss: 0.0031320\n",
      "epoch: 500    train loss: 0.0023182    val loss: 0.0030892\n",
      "epoch: 510    train loss: 0.0022665    val loss: 0.0030477\n",
      "epoch: 520    train loss: 0.0022166    val loss: 0.0030073\n",
      "epoch: 530    train loss: 0.0021684    val loss: 0.0029682\n",
      "epoch: 540    train loss: 0.0021218    val loss: 0.0029301\n",
      "epoch: 550    train loss: 0.0020768    val loss: 0.0028932\n",
      "epoch: 560    train loss: 0.0020333    val loss: 0.0028572\n",
      "epoch: 570    train loss: 0.0019911    val loss: 0.0028222\n",
      "epoch: 580    train loss: 0.0019504    val loss: 0.0027881\n",
      "epoch: 590    train loss: 0.0019109    val loss: 0.0027549\n",
      "epoch: 600    train loss: 0.0018726    val loss: 0.0027226\n",
      "epoch: 610    train loss: 0.0018355    val loss: 0.0026911\n",
      "epoch: 620    train loss: 0.0017996    val loss: 0.0026604\n",
      "epoch: 630    train loss: 0.0017647    val loss: 0.0026304\n",
      "epoch: 640    train loss: 0.0017309    val loss: 0.0026012\n",
      "epoch: 650    train loss: 0.0016981    val loss: 0.0025727\n",
      "epoch: 660    train loss: 0.0016662    val loss: 0.0025448\n",
      "epoch: 670    train loss: 0.0016353    val loss: 0.0025176\n",
      "epoch: 680    train loss: 0.0016052    val loss: 0.0024911\n",
      "epoch: 690    train loss: 0.0015760    val loss: 0.0024651\n",
      "epoch: 700    train loss: 0.0015477    val loss: 0.0024398\n",
      "epoch: 710    train loss: 0.0015201    val loss: 0.0024150\n",
      "epoch: 720    train loss: 0.0014933    val loss: 0.0023908\n",
      "epoch: 730    train loss: 0.0014673    val loss: 0.0023670\n",
      "epoch: 740    train loss: 0.0014419    val loss: 0.0023438\n",
      "epoch: 750    train loss: 0.0014173    val loss: 0.0023211\n",
      "epoch: 760    train loss: 0.0013933    val loss: 0.0022988\n",
      "epoch: 770    train loss: 0.0013699    val loss: 0.0022770\n",
      "epoch: 780    train loss: 0.0013472    val loss: 0.0022557\n",
      "epoch: 790    train loss: 0.0013250    val loss: 0.0022347\n",
      "epoch: 800    train loss: 0.0013034    val loss: 0.0022142\n",
      "epoch: 810    train loss: 0.0012824    val loss: 0.0021940\n",
      "epoch: 820    train loss: 0.0012619    val loss: 0.0021743\n",
      "epoch: 830    train loss: 0.0012420    val loss: 0.0021548\n",
      "epoch: 840    train loss: 0.0012225    val loss: 0.0021358\n",
      "epoch: 850    train loss: 0.0012035    val loss: 0.0021170\n",
      "epoch: 860    train loss: 0.0011850    val loss: 0.0020986\n",
      "epoch: 870    train loss: 0.0011669    val loss: 0.0020806\n",
      "epoch: 880    train loss: 0.0011493    val loss: 0.0020628\n",
      "epoch: 890    train loss: 0.0011321    val loss: 0.0020453\n",
      "epoch: 900    train loss: 0.0011153    val loss: 0.0020281\n",
      "epoch: 910    train loss: 0.0010989    val loss: 0.0020112\n",
      "epoch: 920    train loss: 0.0010829    val loss: 0.0019945\n",
      "epoch: 930    train loss: 0.0010673    val loss: 0.0019781\n",
      "epoch: 940    train loss: 0.0010520    val loss: 0.0019619\n",
      "epoch: 950    train loss: 0.0010371    val loss: 0.0019460\n",
      "epoch: 960    train loss: 0.0010225    val loss: 0.0019303\n",
      "epoch: 970    train loss: 0.0010082    val loss: 0.0019149\n",
      "epoch: 980    train loss: 0.0009943    val loss: 0.0018997\n",
      "epoch: 990    train loss: 0.0009806    val loss: 0.0018846\n",
      "epoch: 1000    train loss: 0.0009673    val loss: 0.0018698\n",
      "epoch: 1010    train loss: 0.0009542    val loss: 0.0018552\n",
      "epoch: 1020    train loss: 0.0009414    val loss: 0.0018408\n",
      "epoch: 1030    train loss: 0.0009289    val loss: 0.0018266\n",
      "epoch: 1040    train loss: 0.0009166    val loss: 0.0018126\n",
      "epoch: 1050    train loss: 0.0009046    val loss: 0.0017988\n",
      "epoch: 1060    train loss: 0.0008929    val loss: 0.0017851\n",
      "epoch: 1070    train loss: 0.0008814    val loss: 0.0017717\n",
      "epoch: 1080    train loss: 0.0008701    val loss: 0.0017584\n",
      "epoch: 1090    train loss: 0.0008590    val loss: 0.0017452\n",
      "epoch: 1100    train loss: 0.0008482    val loss: 0.0017323\n",
      "epoch: 1110    train loss: 0.0008376    val loss: 0.0017195\n",
      "epoch: 1120    train loss: 0.0008272    val loss: 0.0017069\n",
      "epoch: 1130    train loss: 0.0008170    val loss: 0.0016944\n",
      "epoch: 1140    train loss: 0.0008070    val loss: 0.0016820\n",
      "epoch: 1150    train loss: 0.0007971    val loss: 0.0016699\n",
      "epoch: 1160    train loss: 0.0007875    val loss: 0.0016578\n",
      "epoch: 1170    train loss: 0.0007781    val loss: 0.0016460\n",
      "epoch: 1180    train loss: 0.0007688    val loss: 0.0016342\n",
      "epoch: 1190    train loss: 0.0007597    val loss: 0.0016226\n",
      "epoch: 1200    train loss: 0.0007508    val loss: 0.0016112\n",
      "epoch: 1210    train loss: 0.0007420    val loss: 0.0015999\n",
      "epoch: 1220    train loss: 0.0007334    val loss: 0.0015887\n",
      "epoch: 1230    train loss: 0.0007250    val loss: 0.0015776\n",
      "epoch: 1240    train loss: 0.0007167    val loss: 0.0015667\n",
      "epoch: 1250    train loss: 0.0007085    val loss: 0.0015559\n",
      "epoch: 1260    train loss: 0.0007005    val loss: 0.0015452\n",
      "epoch: 1270    train loss: 0.0006927    val loss: 0.0015346\n",
      "epoch: 1280    train loss: 0.0006849    val loss: 0.0015242\n",
      "epoch: 1290    train loss: 0.0006774    val loss: 0.0015139\n",
      "epoch: 1300    train loss: 0.0006699    val loss: 0.0015037\n",
      "epoch: 1310    train loss: 0.0006626    val loss: 0.0014936\n",
      "epoch: 1320    train loss: 0.0006554    val loss: 0.0014836\n",
      "epoch: 1330    train loss: 0.0006483    val loss: 0.0014737\n",
      "epoch: 1340    train loss: 0.0006414    val loss: 0.0014639\n",
      "epoch: 1350    train loss: 0.0006345    val loss: 0.0014543\n",
      "epoch: 1360    train loss: 0.0006278    val loss: 0.0014447\n",
      "epoch: 1370    train loss: 0.0006212    val loss: 0.0014354\n",
      "epoch: 1380    train loss: 0.0006155    val loss: 0.0014270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1390    train loss: 0.0006086    val loss: 0.0014171\n",
      "epoch: 1400    train loss: 0.0006022    val loss: 0.0014076\n",
      "epoch: 1410    train loss: 0.0005959    val loss: 0.0013985\n",
      "epoch: 1420    train loss: 0.0005898    val loss: 0.0013896\n",
      "epoch: 1430    train loss: 0.0005839    val loss: 0.0013807\n",
      "epoch: 1440    train loss: 0.0005780    val loss: 0.0013719\n",
      "epoch: 1450    train loss: 0.0005722    val loss: 0.0013632\n",
      "epoch: 1460    train loss: 0.0005665    val loss: 0.0013545\n",
      "epoch: 1470    train loss: 0.0005609    val loss: 0.0013460\n",
      "epoch: 1480    train loss: 0.0005554    val loss: 0.0013376\n",
      "epoch: 1490    train loss: 0.0005500    val loss: 0.0013292\n",
      "epoch: 1500    train loss: 0.0005446    val loss: 0.0013209\n",
      "epoch: 1510    train loss: 0.0005394    val loss: 0.0013128\n",
      "epoch: 1520    train loss: 0.0005353    val loss: 0.0013071\n",
      "epoch: 1530    train loss: 0.0005292    val loss: 0.0012976\n",
      "epoch: 1540    train loss: 0.0005242    val loss: 0.0012893\n",
      "epoch: 1550    train loss: 0.0005192    val loss: 0.0012813\n",
      "epoch: 1560    train loss: 0.0005143    val loss: 0.0012733\n",
      "epoch: 1570    train loss: 0.0005094    val loss: 0.0012655\n",
      "epoch: 1580    train loss: 0.0005047    val loss: 0.0012577\n",
      "epoch: 1590    train loss: 0.0005001    val loss: 0.0012502\n",
      "epoch: 1600    train loss: 0.0004956    val loss: 0.0012427\n",
      "epoch: 1610    train loss: 0.0004909    val loss: 0.0012348\n",
      "epoch: 1620    train loss: 0.0004865    val loss: 0.0012276\n",
      "epoch: 1630    train loss: 0.0004821    val loss: 0.0012204\n",
      "epoch: 1640    train loss: 0.0004777    val loss: 0.0012130\n",
      "epoch: 1650    train loss: 0.0004742    val loss: 0.0012065\n",
      "epoch: 1660    train loss: 0.0004698    val loss: 0.0011997\n",
      "epoch: 1670    train loss: 0.0004652    val loss: 0.0011920\n",
      "epoch: 1680    train loss: 0.0004611    val loss: 0.0011850\n",
      "epoch: 1690    train loss: 0.0004570    val loss: 0.0011780\n",
      "epoch: 1700    train loss: 0.0004541    val loss: 0.0011715\n",
      "epoch: 1710    train loss: 0.0004491    val loss: 0.0011644\n",
      "epoch: 1720    train loss: 0.0004452    val loss: 0.0011581\n",
      "epoch: 1730    train loss: 0.0004414    val loss: 0.0011511\n",
      "epoch: 1740    train loss: 0.0004376    val loss: 0.0011446\n",
      "epoch: 1750    train loss: 0.0004339    val loss: 0.0011383\n",
      "epoch: 1760    train loss: 0.0004316    val loss: 0.0011315\n",
      "epoch: 1770    train loss: 0.0004271    val loss: 0.0011252\n",
      "epoch: 1780    train loss: 0.0004231    val loss: 0.0011189\n",
      "epoch: 1790    train loss: 0.0004195    val loss: 0.0011124\n",
      "epoch: 1800    train loss: 0.0004160    val loss: 0.0011062\n",
      "epoch: 1810    train loss: 0.0004126    val loss: 0.0010999\n",
      "epoch: 1820    train loss: 0.0004105    val loss: 0.0010946\n",
      "epoch: 1830    train loss: 0.0004062    val loss: 0.0010885\n",
      "epoch: 1840    train loss: 0.0004026    val loss: 0.0010818\n",
      "epoch: 1850    train loss: 0.0003992    val loss: 0.0010764\n",
      "epoch: 1860    train loss: 0.0003959    val loss: 0.0010701\n",
      "epoch: 1870    train loss: 0.0003928    val loss: 0.0010642\n",
      "epoch: 1880    train loss: 0.0003900    val loss: 0.0010583\n",
      "epoch: 1890    train loss: 0.0003879    val loss: 0.0010533\n",
      "epoch: 1900    train loss: 0.0003836    val loss: 0.0010475\n",
      "epoch: 1910    train loss: 0.0003804    val loss: 0.0010417\n",
      "epoch: 1920    train loss: 0.0003774    val loss: 0.0010358\n",
      "epoch: 1930    train loss: 0.0003744    val loss: 0.0010303\n",
      "epoch: 1940    train loss: 0.0003723    val loss: 0.0010250\n",
      "epoch: 1950    train loss: 0.0003688    val loss: 0.0010200\n",
      "epoch: 1960    train loss: 0.0003659    val loss: 0.0010146\n",
      "epoch: 1970    train loss: 0.0003650    val loss: 0.0010097\n",
      "epoch: 1980    train loss: 0.0003604    val loss: 0.0010040\n",
      "epoch: 1990    train loss: 0.0003574    val loss: 0.0009993\n",
      "epoch: 2000    train loss: 0.0003547    val loss: 0.0009938\n",
      "epoch: 2010    train loss: 0.0003519    val loss: 0.0009886\n",
      "epoch: 2020    train loss: 0.0003495    val loss: 0.0009832\n",
      "epoch: 2030    train loss: 0.0003506    val loss: 0.0009814\n",
      "epoch: 2040    train loss: 0.0003445    val loss: 0.0009754\n",
      "epoch: 2050    train loss: 0.0003418    val loss: 0.0009694\n",
      "epoch: 2060    train loss: 0.0003389    val loss: 0.0009640\n",
      "epoch: 2070    train loss: 0.0003364    val loss: 0.0009588\n",
      "epoch: 2080    train loss: 0.0003338    val loss: 0.0009538\n",
      "epoch: 2090    train loss: 0.0003315    val loss: 0.0009491\n",
      "epoch: 2100    train loss: 0.0003327    val loss: 0.0009464\n",
      "epoch: 2110    train loss: 0.0003271    val loss: 0.0009411\n",
      "epoch: 2120    train loss: 0.0003245    val loss: 0.0009371\n",
      "epoch: 2130    train loss: 0.0003219    val loss: 0.0009302\n",
      "epoch: 2140    train loss: 0.0003195    val loss: 0.0009267\n",
      "epoch: 2150    train loss: 0.0003172    val loss: 0.0009224\n",
      "epoch: 2160    train loss: 0.0003154    val loss: 0.0009187\n",
      "epoch: 2170    train loss: 0.0003143    val loss: 0.0009160\n",
      "epoch: 2180    train loss: 0.0003106    val loss: 0.0009088\n",
      "epoch: 2190    train loss: 0.0003084    val loss: 0.0009041\n",
      "epoch: 2200    train loss: 0.0003060    val loss: 0.0009000\n",
      "epoch: 2210    train loss: 0.0003044    val loss: 0.0008967\n",
      "epoch: 2220    train loss: 0.0003018    val loss: 0.0008923\n",
      "epoch: 2230    train loss: 0.0003003    val loss: 0.0008872\n",
      "epoch: 2240    train loss: 0.0002985    val loss: 0.0008865\n",
      "epoch: 2250    train loss: 0.0002963    val loss: 0.0008795\n",
      "epoch: 2260    train loss: 0.0002937    val loss: 0.0008750\n",
      "epoch: 2270    train loss: 0.0002913    val loss: 0.0008718\n",
      "epoch: 2280    train loss: 0.0002894    val loss: 0.0008676\n",
      "epoch: 2290    train loss: 0.0002893    val loss: 0.0008646\n",
      "epoch: 2300    train loss: 0.0002861    val loss: 0.0008623\n",
      "epoch: 2310    train loss: 0.0002839    val loss: 0.0008558\n",
      "epoch: 2320    train loss: 0.0002816    val loss: 0.0008519\n",
      "epoch: 2330    train loss: 0.0002796    val loss: 0.0008479\n",
      "epoch: 2340    train loss: 0.0002778    val loss: 0.0008440\n",
      "epoch: 2350    train loss: 0.0002768    val loss: 0.0008410\n",
      "epoch: 2360    train loss: 0.0002751    val loss: 0.0008380\n",
      "epoch: 2370    train loss: 0.0002724    val loss: 0.0008350\n",
      "epoch: 2380    train loss: 0.0002705    val loss: 0.0008290\n",
      "epoch: 2390    train loss: 0.0002686    val loss: 0.0008270\n",
      "epoch: 2400    train loss: 0.0002669    val loss: 0.0008229\n",
      "epoch: 2410    train loss: 0.0002692    val loss: 0.0008234\n",
      "epoch: 2420    train loss: 0.0002642    val loss: 0.0008128\n",
      "epoch: 2430    train loss: 0.0002619    val loss: 0.0008113\n",
      "epoch: 2440    train loss: 0.0002598    val loss: 0.0008080\n",
      "epoch: 2450    train loss: 0.0002581    val loss: 0.0008056\n",
      "epoch: 2460    train loss: 0.0002567    val loss: 0.0008020\n",
      "epoch: 2470    train loss: 0.0002558    val loss: 0.0007987\n",
      "epoch: 2480    train loss: 0.0002532    val loss: 0.0007967\n",
      "epoch: 2490    train loss: 0.0002532    val loss: 0.0007929\n",
      "epoch: 2500    train loss: 0.0002508    val loss: 0.0007882\n",
      "epoch: 2510    train loss: 0.0002484    val loss: 0.0007832\n",
      "epoch: 2520    train loss: 0.0002467    val loss: 0.0007829\n",
      "epoch: 2530    train loss: 0.0002450    val loss: 0.0007785\n",
      "epoch: 2540    train loss: 0.0002438    val loss: 0.0007760\n",
      "epoch: 2550    train loss: 0.0002444    val loss: 0.0007754\n",
      "epoch: 2560    train loss: 0.0002417    val loss: 0.0007667\n",
      "epoch: 2570    train loss: 0.0002393    val loss: 0.0007664\n",
      "epoch: 2580    train loss: 0.0002375    val loss: 0.0007637\n",
      "epoch: 2590    train loss: 0.0002360    val loss: 0.0007601\n",
      "epoch: 2600    train loss: 0.0002352    val loss: 0.0007585\n",
      "epoch: 2610    train loss: 0.0002352    val loss: 0.0007554\n",
      "epoch: 2620    train loss: 0.0002322    val loss: 0.0007493\n",
      "epoch: 2630    train loss: 0.0002307    val loss: 0.0007490\n",
      "epoch: 2640    train loss: 0.0002287    val loss: 0.0007452\n",
      "epoch: 2650    train loss: 0.0002271    val loss: 0.0007416\n",
      "epoch: 2660    train loss: 0.0002257    val loss: 0.0007391\n",
      "epoch: 2670    train loss: 0.0002260    val loss: 0.0007388\n",
      "epoch: 2680    train loss: 0.0002250    val loss: 0.0007340\n",
      "epoch: 2690    train loss: 0.0002225    val loss: 0.0007308\n",
      "epoch: 2700    train loss: 0.0002206    val loss: 0.0007287\n",
      "epoch: 2710    train loss: 0.0002189    val loss: 0.0007245\n",
      "epoch: 2720    train loss: 0.0002175    val loss: 0.0007221\n",
      "epoch: 2730    train loss: 0.0002164    val loss: 0.0007202\n",
      "epoch: 2740    train loss: 0.0002211    val loss: 0.0007239\n",
      "epoch: 2750    train loss: 0.0002155    val loss: 0.0007143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2760    train loss: 0.0002129    val loss: 0.0007114\n",
      "epoch: 2770    train loss: 0.0002111    val loss: 0.0007093\n",
      "epoch: 2780    train loss: 0.0002097    val loss: 0.0007058\n",
      "epoch: 2790    train loss: 0.0002084    val loss: 0.0007035\n",
      "epoch: 2800    train loss: 0.0002076    val loss: 0.0007026\n",
      "epoch: 2810    train loss: 0.0002077    val loss: 0.0007002\n",
      "epoch: 2820    train loss: 0.0002052    val loss: 0.0006961\n",
      "epoch: 2830    train loss: 0.0002041    val loss: 0.0006942\n",
      "epoch: 2840    train loss: 0.0002024    val loss: 0.0006913\n",
      "epoch: 2850    train loss: 0.0002011    val loss: 0.0006885\n",
      "epoch: 2860    train loss: 0.0002008    val loss: 0.0006869\n",
      "epoch: 2870    train loss: 0.0002018    val loss: 0.0006856\n",
      "epoch: 2880    train loss: 0.0001989    val loss: 0.0006827\n",
      "epoch: 2890    train loss: 0.0001969    val loss: 0.0006777\n",
      "epoch: 2900    train loss: 0.0001953    val loss: 0.0006758\n",
      "epoch: 2910    train loss: 0.0001941    val loss: 0.0006735\n",
      "epoch: 2920    train loss: 0.0001930    val loss: 0.0006711\n",
      "epoch: 2930    train loss: 0.0001922    val loss: 0.0006697\n",
      "epoch: 2940    train loss: 0.0001934    val loss: 0.0006692\n",
      "epoch: 2950    train loss: 0.0001903    val loss: 0.0006662\n",
      "epoch: 2960    train loss: 0.0001899    val loss: 0.0006620\n",
      "epoch: 2970    train loss: 0.0001897    val loss: 0.0006612\n",
      "epoch: 2980    train loss: 0.0001869    val loss: 0.0006583\n",
      "epoch: 2990    train loss: 0.0001854    val loss: 0.0006544\n",
      "epoch: 3000    train loss: 0.0001845    val loss: 0.0006528\n",
      "epoch: 3010    train loss: 0.0001834    val loss: 0.0006499\n",
      "epoch: 3020    train loss: 0.0001840    val loss: 0.0006490\n",
      "epoch: 3030    train loss: 0.0001829    val loss: 0.0006477\n",
      "epoch: 3040    train loss: 0.0001804    val loss: 0.0006443\n",
      "epoch: 3050    train loss: 0.0001801    val loss: 0.0006424\n",
      "epoch: 3060    train loss: 0.0001798    val loss: 0.0006418\n",
      "epoch: 3070    train loss: 0.0001782    val loss: 0.0006380\n",
      "epoch: 3080    train loss: 0.0001764    val loss: 0.0006353\n",
      "epoch: 3090    train loss: 0.0001752    val loss: 0.0006334\n",
      "epoch: 3100    train loss: 0.0001741    val loss: 0.0006310\n",
      "epoch: 3110    train loss: 0.0001734    val loss: 0.0006299\n",
      "epoch: 3120    train loss: 0.0001764    val loss: 0.0006325\n",
      "epoch: 3130    train loss: 0.0001731    val loss: 0.0006247\n",
      "epoch: 3140    train loss: 0.0001707    val loss: 0.0006233\n",
      "epoch: 3150    train loss: 0.0001694    val loss: 0.0006214\n",
      "epoch: 3160    train loss: 0.0001687    val loss: 0.0006180\n",
      "epoch: 3170    train loss: 0.0001701    val loss: 0.0006177\n",
      "epoch: 3180    train loss: 0.0001674    val loss: 0.0006178\n",
      "epoch: 3190    train loss: 0.0001668    val loss: 0.0006130\n",
      "epoch: 3200    train loss: 0.0001649    val loss: 0.0006110\n",
      "epoch: 3210    train loss: 0.0001642    val loss: 0.0006095\n",
      "epoch: 3220    train loss: 0.0001650    val loss: 0.0006083\n",
      "epoch: 3230    train loss: 0.0001629    val loss: 0.0006047\n",
      "epoch: 3240    train loss: 0.0001613    val loss: 0.0006029\n",
      "epoch: 3250    train loss: 0.0001603    val loss: 0.0006021\n",
      "epoch: 3260    train loss: 0.0001600    val loss: 0.0006014\n",
      "epoch: 3270    train loss: 0.0001622    val loss: 0.0006030\n",
      "epoch: 3280    train loss: 0.0001585    val loss: 0.0005963\n",
      "epoch: 3290    train loss: 0.0001594    val loss: 0.0005999\n",
      "epoch: 3300    train loss: 0.0001570    val loss: 0.0005927\n",
      "epoch: 3310    train loss: 0.0001554    val loss: 0.0005902\n",
      "epoch: 3320    train loss: 0.0001543    val loss: 0.0005874\n",
      "epoch: 3330    train loss: 0.0001541    val loss: 0.0005847\n",
      "epoch: 3340    train loss: 0.0001581    val loss: 0.0005823\n",
      "epoch: 3350    train loss: 0.0001532    val loss: 0.0005882\n",
      "epoch: 3360    train loss: 0.0001526    val loss: 0.0005816\n",
      "epoch: 3370    train loss: 0.0001530    val loss: 0.0005831\n",
      "epoch: 3380    train loss: 0.0001500    val loss: 0.0005778\n",
      "epoch: 3390    train loss: 0.0001490    val loss: 0.0005772\n",
      "epoch: 3400    train loss: 0.0001484    val loss: 0.0005750\n",
      "epoch: 3410    train loss: 0.0001471    val loss: 0.0005727\n",
      "epoch: 3420    train loss: 0.0001462    val loss: 0.0005703\n",
      "epoch: 3430    train loss: 0.0001456    val loss: 0.0005686\n",
      "epoch: 3440    train loss: 0.0001476    val loss: 0.0005658\n",
      "epoch: 3450    train loss: 0.0001453    val loss: 0.0005662\n",
      "epoch: 3460    train loss: 0.0001441    val loss: 0.0005653\n",
      "epoch: 3470    train loss: 0.0001436    val loss: 0.0005628\n",
      "epoch: 3480    train loss: 0.0001419    val loss: 0.0005602\n",
      "epoch: 3490    train loss: 0.0001411    val loss: 0.0005590\n",
      "epoch: 3500    train loss: 0.0001415    val loss: 0.0005589\n",
      "epoch: 3510    train loss: 0.0001507    val loss: 0.0005683\n",
      "epoch: 3520    train loss: 0.0001431    val loss: 0.0005565\n",
      "epoch: 3530    train loss: 0.0001385    val loss: 0.0005519\n",
      "epoch: 3540    train loss: 0.0001374    val loss: 0.0005519\n",
      "epoch: 3550    train loss: 0.0001365    val loss: 0.0005496\n",
      "epoch: 3560    train loss: 0.0001358    val loss: 0.0005490\n",
      "epoch: 3570    train loss: 0.0001379    val loss: 0.0005485\n",
      "epoch: 3580    train loss: 0.0001377    val loss: 0.0005468\n",
      "epoch: 3590    train loss: 0.0001362    val loss: 0.0005495\n",
      "epoch: 3600    train loss: 0.0001334    val loss: 0.0005399\n",
      "epoch: 3610    train loss: 0.0001326    val loss: 0.0005426\n",
      "epoch: 3620    train loss: 0.0001316    val loss: 0.0005396\n",
      "epoch: 3630    train loss: 0.0001310    val loss: 0.0005373\n",
      "epoch: 3640    train loss: 0.0001376    val loss: 0.0005437\n",
      "epoch: 3650    train loss: 0.0001329    val loss: 0.0005421\n",
      "epoch: 3660    train loss: 0.0001307    val loss: 0.0005359\n",
      "epoch: 3670    train loss: 0.0001286    val loss: 0.0005304\n",
      "epoch: 3680    train loss: 0.0001277    val loss: 0.0005313\n",
      "epoch: 3690    train loss: 0.0001270    val loss: 0.0005289\n",
      "epoch: 3700    train loss: 0.0001267    val loss: 0.0005253\n",
      "epoch: 3710    train loss: 0.0001387    val loss: 0.0005273\n",
      "epoch: 3720    train loss: 0.0001283    val loss: 0.0005317\n",
      "epoch: 3730    train loss: 0.0001249    val loss: 0.0005237\n",
      "epoch: 3740    train loss: 0.0001243    val loss: 0.0005231\n",
      "epoch: 3750    train loss: 0.0001233    val loss: 0.0005215\n",
      "epoch: 3760    train loss: 0.0001226    val loss: 0.0005195\n",
      "epoch: 3770    train loss: 0.0001219    val loss: 0.0005195\n",
      "epoch: 3780    train loss: 0.0001215    val loss: 0.0005177\n",
      "epoch: 3790    train loss: 0.0001493    val loss: 0.0005497\n",
      "epoch: 3800    train loss: 0.0001252    val loss: 0.0005372\n",
      "epoch: 3810    train loss: 0.0001208    val loss: 0.0005195\n",
      "epoch: 3820    train loss: 0.0001191    val loss: 0.0005161\n",
      "epoch: 3830    train loss: 0.0001184    val loss: 0.0005115\n",
      "epoch: 3840    train loss: 0.0001177    val loss: 0.0005095\n",
      "epoch: 3850    train loss: 0.0001171    val loss: 0.0005077\n",
      "epoch: 3860    train loss: 0.0001165    val loss: 0.0005063\n",
      "epoch: 3870    train loss: 0.0001159    val loss: 0.0005055\n",
      "epoch: 3880    train loss: 0.0001154    val loss: 0.0005043\n",
      "epoch: 3890    train loss: 0.0001308    val loss: 0.0005113\n",
      "epoch: 3900    train loss: 0.0001212    val loss: 0.0005139\n",
      "epoch: 3910    train loss: 0.0001160    val loss: 0.0005033\n",
      "epoch: 3920    train loss: 0.0001135    val loss: 0.0004991\n",
      "epoch: 3930    train loss: 0.0001128    val loss: 0.0004973\n",
      "epoch: 3940    train loss: 0.0001127    val loss: 0.0004965\n",
      "epoch: 3950    train loss: 0.0001222    val loss: 0.0005029\n",
      "epoch: 3960    train loss: 0.0001112    val loss: 0.0004941\n",
      "epoch: 3970    train loss: 0.0001117    val loss: 0.0004938\n",
      "epoch: 3980    train loss: 0.0001099    val loss: 0.0004921\n",
      "epoch: 3990    train loss: 0.0001094    val loss: 0.0004917\n",
      "epoch: 4000    train loss: 0.0001088    val loss: 0.0004898\n",
      "epoch: 4010    train loss: 0.0001084    val loss: 0.0004887\n",
      "epoch: 4020    train loss: 0.0001132    val loss: 0.0004912\n",
      "epoch: 4030    train loss: 0.0001115    val loss: 0.0004922\n",
      "epoch: 4040    train loss: 0.0001088    val loss: 0.0004954\n",
      "epoch: 4050    train loss: 0.0001071    val loss: 0.0004868\n",
      "epoch: 4060    train loss: 0.0001062    val loss: 0.0004801\n",
      "epoch: 4070    train loss: 0.0001056    val loss: 0.0004824\n",
      "epoch: 4080    train loss: 0.0001072    val loss: 0.0004788\n",
      "epoch: 4090    train loss: 0.0001148    val loss: 0.0004789\n",
      "epoch: 4100    train loss: 0.0001062    val loss: 0.0004828\n",
      "epoch: 4110    train loss: 0.0001036    val loss: 0.0004777\n",
      "epoch: 4120    train loss: 0.0001027    val loss: 0.0004755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4130    train loss: 0.0001022    val loss: 0.0004741\n",
      "epoch: 4140    train loss: 0.0001022    val loss: 0.0004735\n",
      "epoch: 4150    train loss: 0.0001180    val loss: 0.0004840\n",
      "epoch: 4160    train loss: 0.0001084    val loss: 0.0004823\n",
      "epoch: 4170    train loss: 0.0001017    val loss: 0.0004686\n",
      "epoch: 4180    train loss: 0.0000999    val loss: 0.0004691\n",
      "epoch: 4190    train loss: 0.0000996    val loss: 0.0004693\n",
      "epoch: 4200    train loss: 0.0000989    val loss: 0.0004674\n",
      "epoch: 4210    train loss: 0.0000985    val loss: 0.0004678\n",
      "epoch: 4220    train loss: 0.0001086    val loss: 0.0004803\n",
      "epoch: 4230    train loss: 0.0001008    val loss: 0.0004672\n",
      "epoch: 4240    train loss: 0.0000975    val loss: 0.0004638\n",
      "epoch: 4250    train loss: 0.0000970    val loss: 0.0004649\n",
      "epoch: 4260    train loss: 0.0000967    val loss: 0.0004687\n",
      "epoch: 4270    train loss: 0.0001011    val loss: 0.0004756\n",
      "epoch: 4280    train loss: 0.0001006    val loss: 0.0004575\n",
      "epoch: 4290    train loss: 0.0000971    val loss: 0.0004636\n",
      "epoch: 4300    train loss: 0.0000948    val loss: 0.0004539\n",
      "epoch: 4310    train loss: 0.0000944    val loss: 0.0004583\n",
      "epoch: 4320    train loss: 0.0000996    val loss: 0.0004592\n",
      "epoch: 4330    train loss: 0.0000972    val loss: 0.0004613\n",
      "epoch: 4340    train loss: 0.0000946    val loss: 0.0004573\n",
      "epoch: 4350    train loss: 0.0000926    val loss: 0.0004539\n",
      "epoch: 4360    train loss: 0.0000917    val loss: 0.0004482\n",
      "epoch: 4370    train loss: 0.0000922    val loss: 0.0004463\n",
      "epoch: 4380    train loss: 0.0001027    val loss: 0.0004487\n",
      "epoch: 4390    train loss: 0.0000943    val loss: 0.0004605\n",
      "epoch: 4400    train loss: 0.0000919    val loss: 0.0004419\n",
      "epoch: 4410    train loss: 0.0000903    val loss: 0.0004507\n",
      "epoch: 4420    train loss: 0.0000906    val loss: 0.0004437\n",
      "epoch: 4430    train loss: 0.0000989    val loss: 0.0004532\n",
      "epoch: 4440    train loss: 0.0000916    val loss: 0.0004450\n",
      "epoch: 4450    train loss: 0.0000885    val loss: 0.0004420\n",
      "epoch: 4460    train loss: 0.0000873    val loss: 0.0004427\n",
      "epoch: 4470    train loss: 0.0000870    val loss: 0.0004418\n",
      "epoch: 4480    train loss: 0.0000882    val loss: 0.0004453\n",
      "epoch: 4490    train loss: 0.0000938    val loss: 0.0004464\n",
      "epoch: 4500    train loss: 0.0000934    val loss: 0.0004488\n",
      "epoch: 4510    train loss: 0.0000867    val loss: 0.0004313\n",
      "epoch: 4520    train loss: 0.0000852    val loss: 0.0004410\n",
      "epoch: 4530    train loss: 0.0000848    val loss: 0.0004354\n",
      "epoch: 4540    train loss: 0.0000842    val loss: 0.0004347\n",
      "epoch: 4550    train loss: 0.0000839    val loss: 0.0004343\n",
      "epoch: 4560    train loss: 0.0000929    val loss: 0.0004544\n",
      "epoch: 4570    train loss: 0.0000874    val loss: 0.0004316\n",
      "epoch: 4580    train loss: 0.0000865    val loss: 0.0004404\n",
      "epoch: 4590    train loss: 0.0000827    val loss: 0.0004287\n",
      "epoch: 4600    train loss: 0.0000820    val loss: 0.0004273\n",
      "epoch: 4610    train loss: 0.0000815    val loss: 0.0004283\n",
      "epoch: 4620    train loss: 0.0000824    val loss: 0.0004247\n",
      "epoch: 4630    train loss: 0.0000936    val loss: 0.0004349\n",
      "epoch: 4640    train loss: 0.0000819    val loss: 0.0004292\n",
      "epoch: 4650    train loss: 0.0000802    val loss: 0.0004239\n",
      "epoch: 4660    train loss: 0.0000800    val loss: 0.0004262\n",
      "epoch: 4670    train loss: 0.0000810    val loss: 0.0004302\n",
      "epoch: 4680    train loss: 0.0000864    val loss: 0.0004376\n",
      "epoch: 4690    train loss: 0.0000812    val loss: 0.0004184\n",
      "epoch: 4700    train loss: 0.0000788    val loss: 0.0004221\n",
      "epoch: 4710    train loss: 0.0000781    val loss: 0.0004218\n",
      "epoch: 4720    train loss: 0.0000775    val loss: 0.0004197\n",
      "epoch: 4730    train loss: 0.0000805    val loss: 0.0004247\n",
      "epoch: 4740    train loss: 0.0000858    val loss: 0.0004289\n",
      "epoch: 4750    train loss: 0.0000784    val loss: 0.0004166\n",
      "epoch: 4760    train loss: 0.0000762    val loss: 0.0004160\n",
      "epoch: 4770    train loss: 0.0000760    val loss: 0.0004143\n",
      "epoch: 4780    train loss: 0.0000758    val loss: 0.0004111\n",
      "epoch: 4790    train loss: 0.0000874    val loss: 0.0004046\n",
      "epoch: 4800    train loss: 0.0000797    val loss: 0.0004239\n",
      "epoch: 4810    train loss: 0.0000766    val loss: 0.0004045\n",
      "epoch: 4820    train loss: 0.0000747    val loss: 0.0004140\n",
      "epoch: 4830    train loss: 0.0000752    val loss: 0.0004061\n",
      "epoch: 4840    train loss: 0.0000838    val loss: 0.0004121\n",
      "epoch: 4850    train loss: 0.0000753    val loss: 0.0004148\n",
      "epoch: 4860    train loss: 0.0000729    val loss: 0.0004072\n",
      "epoch: 4870    train loss: 0.0000721    val loss: 0.0004043\n",
      "epoch: 4880    train loss: 0.0000724    val loss: 0.0004022\n",
      "epoch: 4890    train loss: 0.0000786    val loss: 0.0004008\n",
      "epoch: 4900    train loss: 0.0000744    val loss: 0.0004120\n",
      "epoch: 4910    train loss: 0.0000727    val loss: 0.0003981\n",
      "epoch: 4920    train loss: 0.0000712    val loss: 0.0004088\n",
      "epoch: 4930    train loss: 0.0000707    val loss: 0.0003993\n",
      "epoch: 4940    train loss: 0.0000824    val loss: 0.0004185\n",
      "epoch: 4950    train loss: 0.0000740    val loss: 0.0004075\n",
      "epoch: 4960    train loss: 0.0000721    val loss: 0.0004040\n",
      "epoch: 4970    train loss: 0.0000697    val loss: 0.0003974\n",
      "epoch: 4980    train loss: 0.0000686    val loss: 0.0003951\n",
      "epoch: 4990    train loss: 0.0000693    val loss: 0.0003883\n",
      "epoch: 5000    train loss: 0.0000759    val loss: 0.0003835\n",
      "epoch: 5010    train loss: 0.0000755    val loss: 0.0004130\n",
      "epoch: 5020    train loss: 0.0000702    val loss: 0.0003842\n",
      "epoch: 5030    train loss: 0.0000676    val loss: 0.0004029\n",
      "epoch: 5040    train loss: 0.0000669    val loss: 0.0003871\n",
      "epoch: 5050    train loss: 0.0000662    val loss: 0.0003950\n",
      "epoch: 5060    train loss: 0.0000663    val loss: 0.0003916\n",
      "epoch: 5070    train loss: 0.0000756    val loss: 0.0003966\n",
      "epoch: 5080    train loss: 0.0000682    val loss: 0.0003971\n",
      "epoch: 5090    train loss: 0.0000676    val loss: 0.0003878\n",
      "epoch: 5100    train loss: 0.0000652    val loss: 0.0003909\n",
      "epoch: 5110    train loss: 0.0000645    val loss: 0.0003847\n",
      "epoch: 5120    train loss: 0.0000641    val loss: 0.0003820\n",
      "epoch: 5130    train loss: 0.0000661    val loss: 0.0003760\n",
      "epoch: 5140    train loss: 0.0000703    val loss: 0.0003759\n",
      "epoch: 5150    train loss: 0.0000693    val loss: 0.0003993\n",
      "epoch: 5160    train loss: 0.0000642    val loss: 0.0003794\n",
      "epoch: 5170    train loss: 0.0000632    val loss: 0.0003874\n",
      "epoch: 5180    train loss: 0.0000627    val loss: 0.0003800\n",
      "epoch: 5190    train loss: 0.0000660    val loss: 0.0003793\n",
      "epoch: 5200    train loss: 0.0000663    val loss: 0.0003820\n",
      "epoch: 5210    train loss: 0.0000634    val loss: 0.0003833\n",
      "epoch: 5220    train loss: 0.0000620    val loss: 0.0003767\n",
      "epoch: 5230    train loss: 0.0000617    val loss: 0.0003788\n",
      "epoch: 5240    train loss: 0.0000736    val loss: 0.0003837\n",
      "epoch: 5250    train loss: 0.0000632    val loss: 0.0003851\n",
      "epoch: 5260    train loss: 0.0000621    val loss: 0.0003712\n",
      "epoch: 5270    train loss: 0.0000612    val loss: 0.0003707\n",
      "epoch: 5280    train loss: 0.0000638    val loss: 0.0003615\n",
      "epoch: 5290    train loss: 0.0000590    val loss: 0.0003792\n",
      "epoch: 5300    train loss: 0.0000593    val loss: 0.0003783\n",
      "epoch: 5310    train loss: 0.0000588    val loss: 0.0003696\n",
      "epoch: 5320    train loss: 0.0000715    val loss: 0.0003834\n",
      "epoch: 5330    train loss: 0.0000650    val loss: 0.0003839\n",
      "epoch: 5340    train loss: 0.0000608    val loss: 0.0003730\n",
      "epoch: 5350    train loss: 0.0000582    val loss: 0.0003680\n",
      "epoch: 5360    train loss: 0.0000569    val loss: 0.0003671\n",
      "epoch: 5370    train loss: 0.0000565    val loss: 0.0003673\n",
      "epoch: 5380    train loss: 0.0000569    val loss: 0.0003614\n",
      "epoch: 5390    train loss: 0.0000681    val loss: 0.0003487\n",
      "epoch: 5400    train loss: 0.0000662    val loss: 0.0003984\n",
      "epoch: 5410    train loss: 0.0000584    val loss: 0.0003571\n",
      "epoch: 5420    train loss: 0.0000563    val loss: 0.0003758\n",
      "epoch: 5430    train loss: 0.0000555    val loss: 0.0003598\n",
      "epoch: 5440    train loss: 0.0000548    val loss: 0.0003667\n",
      "epoch: 5450    train loss: 0.0000544    val loss: 0.0003614\n",
      "epoch: 5460    train loss: 0.0000544    val loss: 0.0003618\n",
      "epoch: 5470    train loss: 0.0000699    val loss: 0.0003820\n",
      "epoch: 5480    train loss: 0.0000608    val loss: 0.0003605\n",
      "epoch: 5490    train loss: 0.0000575    val loss: 0.0003629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5500    train loss: 0.0000540    val loss: 0.0003606\n",
      "epoch: 5510    train loss: 0.0000530    val loss: 0.0003608\n",
      "epoch: 5520    train loss: 0.0000527    val loss: 0.0003584\n",
      "epoch: 5530    train loss: 0.0000523    val loss: 0.0003571\n",
      "epoch: 5540    train loss: 0.0000536    val loss: 0.0003564\n",
      "epoch: 5550    train loss: 0.0000685    val loss: 0.0003610\n",
      "epoch: 5560    train loss: 0.0000531    val loss: 0.0003575\n",
      "epoch: 5570    train loss: 0.0000534    val loss: 0.0003579\n",
      "epoch: 5580    train loss: 0.0000513    val loss: 0.0003517\n",
      "epoch: 5590    train loss: 0.0000508    val loss: 0.0003556\n",
      "epoch: 5600    train loss: 0.0000505    val loss: 0.0003535\n",
      "epoch: 5610    train loss: 0.0000502    val loss: 0.0003535\n",
      "epoch: 5620    train loss: 0.0000503    val loss: 0.0003532\n",
      "epoch: 5630    train loss: 0.0000678    val loss: 0.0003787\n",
      "epoch: 5640    train loss: 0.0000605    val loss: 0.0003642\n",
      "epoch: 5650    train loss: 0.0000527    val loss: 0.0003450\n",
      "epoch: 5660    train loss: 0.0000501    val loss: 0.0003562\n",
      "epoch: 5670    train loss: 0.0000492    val loss: 0.0003460\n",
      "epoch: 5680    train loss: 0.0000486    val loss: 0.0003505\n",
      "epoch: 5690    train loss: 0.0000482    val loss: 0.0003469\n",
      "epoch: 5700    train loss: 0.0000496    val loss: 0.0003486\n",
      "epoch: 5710    train loss: 0.0000627    val loss: 0.0003511\n",
      "epoch: 5720    train loss: 0.0000504    val loss: 0.0003423\n",
      "epoch: 5730    train loss: 0.0000482    val loss: 0.0003462\n",
      "epoch: 5740    train loss: 0.0000483    val loss: 0.0003373\n",
      "epoch: 5750    train loss: 0.0000537    val loss: 0.0003361\n",
      "epoch: 5760    train loss: 0.0000466    val loss: 0.0003456\n",
      "epoch: 5770    train loss: 0.0000470    val loss: 0.0003428\n",
      "epoch: 5780    train loss: 0.0000466    val loss: 0.0003356\n",
      "epoch: 5790    train loss: 0.0000465    val loss: 0.0003385\n",
      "epoch: 5800    train loss: 0.0000601    val loss: 0.0003262\n",
      "epoch: 5810    train loss: 0.0000493    val loss: 0.0003580\n",
      "epoch: 5820    train loss: 0.0000466    val loss: 0.0003345\n",
      "epoch: 5830    train loss: 0.0000454    val loss: 0.0003433\n",
      "epoch: 5840    train loss: 0.0000450    val loss: 0.0003389\n",
      "epoch: 5850    train loss: 0.0000485    val loss: 0.0003531\n",
      "epoch: 5860    train loss: 0.0000558    val loss: 0.0003442\n",
      "epoch: 5870    train loss: 0.0000469    val loss: 0.0003357\n",
      "epoch: 5880    train loss: 0.0000445    val loss: 0.0003369\n",
      "epoch: 5890    train loss: 0.0000437    val loss: 0.0003377\n",
      "epoch: 5900    train loss: 0.0000435    val loss: 0.0003385\n",
      "epoch: 5910    train loss: 0.0000472    val loss: 0.0003574\n",
      "epoch: 5920    train loss: 0.0000475    val loss: 0.0003402\n",
      "epoch: 5930    train loss: 0.0000435    val loss: 0.0003412\n",
      "epoch: 5940    train loss: 0.0000427    val loss: 0.0003249\n",
      "epoch: 5950    train loss: 0.0000424    val loss: 0.0003380\n",
      "epoch: 5960    train loss: 0.0000444    val loss: 0.0003418\n",
      "epoch: 5970    train loss: 0.0000534    val loss: 0.0003424\n",
      "epoch: 5980    train loss: 0.0000435    val loss: 0.0003261\n",
      "epoch: 5990    train loss: 0.0000419    val loss: 0.0003301\n",
      "epoch: 6000    train loss: 0.0000423    val loss: 0.0003316\n",
      "epoch: 6010    train loss: 0.0000460    val loss: 0.0003330\n",
      "epoch: 6020    train loss: 0.0000415    val loss: 0.0003386\n",
      "epoch: 6030    train loss: 0.0000412    val loss: 0.0003311\n",
      "epoch: 6040    train loss: 0.0000425    val loss: 0.0003376\n",
      "epoch: 6050    train loss: 0.0000491    val loss: 0.0003592\n",
      "epoch: 6060    train loss: 0.0000432    val loss: 0.0003294\n",
      "epoch: 6070    train loss: 0.0000410    val loss: 0.0003204\n",
      "epoch: 6080    train loss: 0.0000395    val loss: 0.0003261\n",
      "epoch: 6090    train loss: 0.0000407    val loss: 0.0003354\n",
      "epoch: 6100    train loss: 0.0000547    val loss: 0.0003600\n",
      "epoch: 6110    train loss: 0.0000418    val loss: 0.0003112\n",
      "epoch: 6120    train loss: 0.0000391    val loss: 0.0003293\n",
      "epoch: 6130    train loss: 0.0000385    val loss: 0.0003183\n",
      "epoch: 6140    train loss: 0.0000383    val loss: 0.0003198\n",
      "epoch: 6150    train loss: 0.0000380    val loss: 0.0003202\n",
      "epoch: 6160    train loss: 0.0000378    val loss: 0.0003193\n",
      "epoch: 6170    train loss: 0.0000434    val loss: 0.0003234\n",
      "epoch: 6180    train loss: 0.0000464    val loss: 0.0003119\n",
      "epoch: 6190    train loss: 0.0000415    val loss: 0.0003233\n",
      "epoch: 6200    train loss: 0.0000382    val loss: 0.0003246\n",
      "epoch: 6210    train loss: 0.0000374    val loss: 0.0003157\n",
      "epoch: 6220    train loss: 0.0000367    val loss: 0.0003174\n",
      "epoch: 6230    train loss: 0.0000364    val loss: 0.0003140\n",
      "epoch: 6240    train loss: 0.0000368    val loss: 0.0003057\n",
      "epoch: 6250    train loss: 0.0000537    val loss: 0.0002837\n",
      "epoch: 6260    train loss: 0.0000437    val loss: 0.0003446\n",
      "epoch: 6270    train loss: 0.0000372    val loss: 0.0003119\n",
      "epoch: 6280    train loss: 0.0000359    val loss: 0.0003096\n",
      "epoch: 6290    train loss: 0.0000355    val loss: 0.0003137\n",
      "epoch: 6300    train loss: 0.0000353    val loss: 0.0003069\n",
      "epoch: 6310    train loss: 0.0000384    val loss: 0.0003077\n",
      "epoch: 6320    train loss: 0.0000375    val loss: 0.0003032\n",
      "epoch: 6330    train loss: 0.0000357    val loss: 0.0003181\n",
      "epoch: 6340    train loss: 0.0000348    val loss: 0.0003084\n",
      "epoch: 6350    train loss: 0.0000347    val loss: 0.0003066\n",
      "epoch: 6360    train loss: 0.0000384    val loss: 0.0003124\n",
      "epoch: 6370    train loss: 0.0000457    val loss: 0.0003027\n",
      "epoch: 6380    train loss: 0.0000378    val loss: 0.0003082\n",
      "epoch: 6390    train loss: 0.0000345    val loss: 0.0003075\n",
      "epoch: 6400    train loss: 0.0000336    val loss: 0.0003098\n",
      "epoch: 6410    train loss: 0.0000345    val loss: 0.0003252\n",
      "epoch: 6420    train loss: 0.0000437    val loss: 0.0003553\n",
      "epoch: 6430    train loss: 0.0000347    val loss: 0.0002853\n",
      "epoch: 6440    train loss: 0.0000332    val loss: 0.0003120\n",
      "epoch: 6450    train loss: 0.0000363    val loss: 0.0003081\n",
      "epoch: 6460    train loss: 0.0000353    val loss: 0.0002950\n",
      "epoch: 6470    train loss: 0.0000331    val loss: 0.0003069\n",
      "epoch: 6480    train loss: 0.0000330    val loss: 0.0003033\n",
      "epoch: 6490    train loss: 0.0000370    val loss: 0.0003104\n",
      "epoch: 6500    train loss: 0.0000338    val loss: 0.0003006\n",
      "epoch: 6510    train loss: 0.0000348    val loss: 0.0002980\n",
      "epoch: 6520    train loss: 0.0000344    val loss: 0.0002826\n",
      "epoch: 6530    train loss: 0.0000349    val loss: 0.0002784\n",
      "epoch: 6540    train loss: 0.0000321    val loss: 0.0003138\n",
      "epoch: 6550    train loss: 0.0000312    val loss: 0.0003036\n",
      "epoch: 6560    train loss: 0.0000332    val loss: 0.0002946\n",
      "epoch: 6570    train loss: 0.0000365    val loss: 0.0002998\n",
      "epoch: 6580    train loss: 0.0000369    val loss: 0.0003102\n",
      "epoch: 6590    train loss: 0.0000322    val loss: 0.0003077\n",
      "epoch: 6600    train loss: 0.0000345    val loss: 0.0003278\n",
      "epoch: 6610    train loss: 0.0000322    val loss: 0.0003147\n",
      "epoch: 6620    train loss: 0.0000311    val loss: 0.0002875\n",
      "epoch: 6630    train loss: 0.0000329    val loss: 0.0003129\n",
      "epoch: 6640    train loss: 0.0000371    val loss: 0.0003122\n",
      "epoch: 6650    train loss: 0.0000311    val loss: 0.0002925\n",
      "epoch: 6660    train loss: 0.0000299    val loss: 0.0002951\n",
      "epoch: 6670    train loss: 0.0000293    val loss: 0.0002909\n",
      "epoch: 6680    train loss: 0.0000292    val loss: 0.0002918\n",
      "epoch: 6690    train loss: 0.0000322    val loss: 0.0002917\n",
      "epoch: 6700    train loss: 0.0000446    val loss: 0.0002880\n",
      "epoch: 6710    train loss: 0.0000351    val loss: 0.0002844\n",
      "epoch: 6720    train loss: 0.0000301    val loss: 0.0002911\n",
      "epoch: 6730    train loss: 0.0000287    val loss: 0.0002951\n",
      "epoch: 6740    train loss: 0.0000285    val loss: 0.0003012\n",
      "epoch: 6750    train loss: 0.0000331    val loss: 0.0003277\n",
      "epoch: 6760    train loss: 0.0000304    val loss: 0.0002983\n",
      "epoch: 6770    train loss: 0.0000291    val loss: 0.0002754\n",
      "epoch: 6780    train loss: 0.0000281    val loss: 0.0003041\n",
      "epoch: 6790    train loss: 0.0000272    val loss: 0.0002867\n",
      "epoch: 6800    train loss: 0.0000277    val loss: 0.0002850\n",
      "epoch: 6810    train loss: 0.0000344    val loss: 0.0003038\n",
      "epoch: 6820    train loss: 0.0000280    val loss: 0.0002868\n",
      "epoch: 6830    train loss: 0.0000355    val loss: 0.0002788\n",
      "epoch: 6840    train loss: 0.0000297    val loss: 0.0002670\n",
      "epoch: 6850    train loss: 0.0000274    val loss: 0.0002783\n",
      "epoch: 6860    train loss: 0.0000266    val loss: 0.0002926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6870    train loss: 0.0000266    val loss: 0.0002971\n",
      "epoch: 6880    train loss: 0.0000272    val loss: 0.0003017\n",
      "epoch: 6890    train loss: 0.0000319    val loss: 0.0003265\n",
      "epoch: 6900    train loss: 0.0000281    val loss: 0.0002885\n",
      "epoch: 6910    train loss: 0.0000347    val loss: 0.0002715\n",
      "epoch: 6920    train loss: 0.0000272    val loss: 0.0003040\n",
      "epoch: 6930    train loss: 0.0000263    val loss: 0.0002691\n",
      "epoch: 6940    train loss: 0.0000253    val loss: 0.0002794\n",
      "epoch: 6950    train loss: 0.0000251    val loss: 0.0002827\n",
      "epoch: 6960    train loss: 0.0000248    val loss: 0.0002775\n",
      "epoch: 6970    train loss: 0.0000259    val loss: 0.0002704\n",
      "epoch: 6980    train loss: 0.0000541    val loss: 0.0002848\n",
      "epoch: 6990    train loss: 0.0000370    val loss: 0.0003207\n",
      "epoch: 7000    train loss: 0.0000272    val loss: 0.0002632\n",
      "epoch: 7010    train loss: 0.0000249    val loss: 0.0002778\n",
      "epoch: 7020    train loss: 0.0000247    val loss: 0.0002825\n",
      "epoch: 7030    train loss: 0.0000240    val loss: 0.0002762\n",
      "epoch: 7040    train loss: 0.0000239    val loss: 0.0002744\n",
      "epoch: 7050    train loss: 0.0000240    val loss: 0.0002714\n",
      "epoch: 7060    train loss: 0.0000292    val loss: 0.0002548\n",
      "epoch: 7070    train loss: 0.0000297    val loss: 0.0002658\n",
      "epoch: 7080    train loss: 0.0000246    val loss: 0.0002780\n",
      "epoch: 7090    train loss: 0.0000234    val loss: 0.0002805\n",
      "epoch: 7100    train loss: 0.0000237    val loss: 0.0002816\n",
      "epoch: 7110    train loss: 0.0000342    val loss: 0.0003175\n",
      "epoch: 7120    train loss: 0.0000252    val loss: 0.0002590\n",
      "epoch: 7130    train loss: 0.0000244    val loss: 0.0002907\n",
      "epoch: 7140    train loss: 0.0000233    val loss: 0.0002659\n",
      "epoch: 7150    train loss: 0.0000229    val loss: 0.0002760\n",
      "epoch: 7160    train loss: 0.0000226    val loss: 0.0002695\n",
      "epoch: 7170    train loss: 0.0000225    val loss: 0.0002692\n",
      "epoch: 7180    train loss: 0.0000240    val loss: 0.0002642\n",
      "epoch: 7190    train loss: 0.0000441    val loss: 0.0002612\n",
      "epoch: 7200    train loss: 0.0000293    val loss: 0.0002641\n",
      "epoch: 7210    train loss: 0.0000236    val loss: 0.0002625\n",
      "epoch: 7220    train loss: 0.0000224    val loss: 0.0002679\n",
      "epoch: 7230    train loss: 0.0000220    val loss: 0.0002747\n",
      "epoch: 7240    train loss: 0.0000219    val loss: 0.0002763\n",
      "epoch: 7250    train loss: 0.0000259    val loss: 0.0003117\n",
      "epoch: 7260    train loss: 0.0000348    val loss: 0.0003095\n",
      "epoch: 7270    train loss: 0.0000230    val loss: 0.0002779\n",
      "epoch: 7280    train loss: 0.0000219    val loss: 0.0002563\n",
      "epoch: 7290    train loss: 0.0000219    val loss: 0.0002811\n",
      "epoch: 7300    train loss: 0.0000214    val loss: 0.0002631\n",
      "epoch: 7310    train loss: 0.0000212    val loss: 0.0002737\n",
      "epoch: 7320    train loss: 0.0000218    val loss: 0.0002695\n",
      "epoch: 7330    train loss: 0.0000323    val loss: 0.0002904\n",
      "epoch: 7340    train loss: 0.0000265    val loss: 0.0002664\n",
      "epoch: 7350    train loss: 0.0000229    val loss: 0.0002758\n",
      "epoch: 7360    train loss: 0.0000221    val loss: 0.0002637\n",
      "epoch: 7370    train loss: 0.0000208    val loss: 0.0002701\n",
      "epoch: 7380    train loss: 0.0000205    val loss: 0.0002645\n",
      "epoch: 7390    train loss: 0.0000205    val loss: 0.0002595\n",
      "epoch: 7400    train loss: 0.0000221    val loss: 0.0002458\n",
      "epoch: 7410    train loss: 0.0000402    val loss: 0.0002269\n",
      "epoch: 7420    train loss: 0.0000279    val loss: 0.0003181\n",
      "epoch: 7430    train loss: 0.0000242    val loss: 0.0002364\n",
      "epoch: 7440    train loss: 0.0000213    val loss: 0.0002797\n",
      "epoch: 7450    train loss: 0.0000204    val loss: 0.0002591\n",
      "epoch: 7460    train loss: 0.0000200    val loss: 0.0002737\n",
      "epoch: 7470    train loss: 0.0000201    val loss: 0.0002649\n",
      "epoch: 7480    train loss: 0.0000266    val loss: 0.0002775\n",
      "epoch: 7490    train loss: 0.0000322    val loss: 0.0002423\n",
      "epoch: 7500    train loss: 0.0000233    val loss: 0.0002842\n",
      "epoch: 7510    train loss: 0.0000215    val loss: 0.0002573\n",
      "epoch: 7520    train loss: 0.0000200    val loss: 0.0002640\n",
      "epoch: 7530    train loss: 0.0000193    val loss: 0.0002640\n",
      "epoch: 7540    train loss: 0.0000191    val loss: 0.0002623\n",
      "epoch: 7550    train loss: 0.0000190    val loss: 0.0002616\n",
      "epoch: 7560    train loss: 0.0000189    val loss: 0.0002613\n",
      "epoch: 7570    train loss: 0.0000203    val loss: 0.0002605\n",
      "epoch: 7580    train loss: 0.0000382    val loss: 0.0002475\n",
      "epoch: 7590    train loss: 0.0000277    val loss: 0.0002379\n",
      "epoch: 7600    train loss: 0.0000225    val loss: 0.0002551\n",
      "epoch: 7610    train loss: 0.0000188    val loss: 0.0002632\n",
      "epoch: 7620    train loss: 0.0000189    val loss: 0.0002532\n",
      "epoch: 7630    train loss: 0.0000184    val loss: 0.0002619\n",
      "epoch: 7640    train loss: 0.0000183    val loss: 0.0002583\n",
      "epoch: 7650    train loss: 0.0000182    val loss: 0.0002564\n",
      "epoch: 7660    train loss: 0.0000192    val loss: 0.0002548\n",
      "epoch: 7670    train loss: 0.0000437    val loss: 0.0002695\n",
      "epoch: 7680    train loss: 0.0000244    val loss: 0.0002679\n",
      "epoch: 7690    train loss: 0.0000189    val loss: 0.0002602\n",
      "epoch: 7700    train loss: 0.0000187    val loss: 0.0002540\n",
      "epoch: 7710    train loss: 0.0000180    val loss: 0.0002569\n",
      "epoch: 7720    train loss: 0.0000178    val loss: 0.0002615\n",
      "epoch: 7730    train loss: 0.0000180    val loss: 0.0002646\n",
      "epoch: 7740    train loss: 0.0000314    val loss: 0.0003346\n",
      "epoch: 7750    train loss: 0.0000189    val loss: 0.0002229\n",
      "epoch: 7760    train loss: 0.0000199    val loss: 0.0002884\n",
      "epoch: 7770    train loss: 0.0000190    val loss: 0.0002396\n",
      "epoch: 7780    train loss: 0.0000179    val loss: 0.0002669\n",
      "epoch: 7790    train loss: 0.0000174    val loss: 0.0002494\n",
      "epoch: 7800    train loss: 0.0000172    val loss: 0.0002594\n",
      "epoch: 7810    train loss: 0.0000171    val loss: 0.0002522\n",
      "epoch: 7820    train loss: 0.0000170    val loss: 0.0002544\n",
      "epoch: 7830    train loss: 0.0000173    val loss: 0.0002599\n",
      "epoch: 7840    train loss: 0.0000592    val loss: 0.0003223\n",
      "epoch: 7850    train loss: 0.0000273    val loss: 0.0002789\n",
      "epoch: 7860    train loss: 0.0000186    val loss: 0.0002614\n",
      "epoch: 7870    train loss: 0.0000184    val loss: 0.0002548\n",
      "epoch: 7880    train loss: 0.0000172    val loss: 0.0002493\n",
      "epoch: 7890    train loss: 0.0000169    val loss: 0.0002486\n",
      "epoch: 7900    train loss: 0.0000166    val loss: 0.0002496\n",
      "epoch: 7910    train loss: 0.0000165    val loss: 0.0002523\n",
      "epoch: 7920    train loss: 0.0000164    val loss: 0.0002536\n",
      "epoch: 7930    train loss: 0.0000163    val loss: 0.0002534\n",
      "epoch: 7940    train loss: 0.0000164    val loss: 0.0002578\n",
      "epoch: 7950    train loss: 0.0000251    val loss: 0.0003159\n",
      "epoch: 7960    train loss: 0.0000187    val loss: 0.0002306\n",
      "epoch: 7970    train loss: 0.0000207    val loss: 0.0002978\n",
      "epoch: 7980    train loss: 0.0000179    val loss: 0.0002431\n",
      "epoch: 7990    train loss: 0.0000162    val loss: 0.0002443\n",
      "epoch: 8000    train loss: 0.0000162    val loss: 0.0002604\n",
      "epoch: 8010    train loss: 0.0000160    val loss: 0.0002478\n",
      "epoch: 8020    train loss: 0.0000166    val loss: 0.0002597\n",
      "epoch: 8030    train loss: 0.0000332    val loss: 0.0002915\n",
      "epoch: 8040    train loss: 0.0000204    val loss: 0.0002429\n",
      "epoch: 8050    train loss: 0.0000165    val loss: 0.0002556\n",
      "epoch: 8060    train loss: 0.0000157    val loss: 0.0002460\n",
      "epoch: 8070    train loss: 0.0000157    val loss: 0.0002492\n",
      "epoch: 8080    train loss: 0.0000156    val loss: 0.0002531\n",
      "epoch: 8090    train loss: 0.0000162    val loss: 0.0002517\n",
      "epoch: 8100    train loss: 0.0000262    val loss: 0.0002632\n",
      "epoch: 8110    train loss: 0.0000172    val loss: 0.0002325\n",
      "epoch: 8120    train loss: 0.0000156    val loss: 0.0002484\n",
      "epoch: 8130    train loss: 0.0000154    val loss: 0.0002518\n",
      "epoch: 8140    train loss: 0.0000157    val loss: 0.0002565\n",
      "epoch: 8150    train loss: 0.0000199    val loss: 0.0002855\n",
      "epoch: 8160    train loss: 0.0000225    val loss: 0.0002825\n",
      "epoch: 8170    train loss: 0.0000191    val loss: 0.0002443\n",
      "epoch: 8180    train loss: 0.0000172    val loss: 0.0002738\n",
      "epoch: 8190    train loss: 0.0000166    val loss: 0.0002553\n",
      "epoch: 8200    train loss: 0.0000181    val loss: 0.0002808\n",
      "epoch: 8210    train loss: 0.0000211    val loss: 0.0002868\n",
      "epoch: 8220    train loss: 0.0000164    val loss: 0.0002518\n",
      "epoch: 8230    train loss: 0.0000186    val loss: 0.0002433\n",
      "epoch: 8240    train loss: 0.0000153    val loss: 0.0002497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8250    train loss: 0.0000157    val loss: 0.0002633\n",
      "epoch: 8260    train loss: 0.0000182    val loss: 0.0002833\n",
      "epoch: 8270    train loss: 0.0000240    val loss: 0.0002938\n",
      "epoch: 8280    train loss: 0.0000192    val loss: 0.0002313\n",
      "epoch: 8290    train loss: 0.0000162    val loss: 0.0002537\n",
      "epoch: 8300    train loss: 0.0000162    val loss: 0.0002683\n",
      "epoch: 8310    train loss: 0.0000249    val loss: 0.0002869\n",
      "epoch: 8320    train loss: 0.0000161    val loss: 0.0002356\n",
      "epoch: 8330    train loss: 0.0000151    val loss: 0.0002410\n",
      "epoch: 8340    train loss: 0.0000146    val loss: 0.0002549\n",
      "epoch: 8350    train loss: 0.0000145    val loss: 0.0002504\n",
      "epoch: 8360    train loss: 0.0000145    val loss: 0.0002549\n",
      "epoch: 8370    train loss: 0.0000234    val loss: 0.0003009\n",
      "epoch: 8380    train loss: 0.0000189    val loss: 0.0002544\n",
      "epoch: 8390    train loss: 0.0000148    val loss: 0.0002588\n",
      "epoch: 8400    train loss: 0.0000148    val loss: 0.0002354\n",
      "epoch: 8410    train loss: 0.0000144    val loss: 0.0002571\n",
      "epoch: 8420    train loss: 0.0000146    val loss: 0.0002491\n",
      "epoch: 8430    train loss: 0.0000243    val loss: 0.0003073\n",
      "epoch: 8440    train loss: 0.0000168    val loss: 0.0002383\n",
      "epoch: 8450    train loss: 0.0000146    val loss: 0.0002559\n",
      "epoch: 8460    train loss: 0.0000144    val loss: 0.0002309\n",
      "epoch: 8470    train loss: 0.0000139    val loss: 0.0002477\n",
      "epoch: 8480    train loss: 0.0000136    val loss: 0.0002382\n",
      "epoch: 8490    train loss: 0.0000138    val loss: 0.0002368\n",
      "epoch: 8500    train loss: 0.0000241    val loss: 0.0002194\n",
      "epoch: 8510    train loss: 0.0000153    val loss: 0.0002400\n",
      "epoch: 8520    train loss: 0.0000153    val loss: 0.0002374\n",
      "epoch: 8530    train loss: 0.0000149    val loss: 0.0002479\n",
      "epoch: 8540    train loss: 0.0000139    val loss: 0.0002425\n",
      "epoch: 8550    train loss: 0.0000137    val loss: 0.0002502\n",
      "epoch: 8560    train loss: 0.0000192    val loss: 0.0002804\n",
      "epoch: 8570    train loss: 0.0000153    val loss: 0.0002459\n",
      "epoch: 8580    train loss: 0.0000139    val loss: 0.0002381\n",
      "epoch: 8590    train loss: 0.0000133    val loss: 0.0002424\n",
      "epoch: 8600    train loss: 0.0000132    val loss: 0.0002381\n",
      "epoch: 8610    train loss: 0.0000132    val loss: 0.0002437\n",
      "epoch: 8620    train loss: 0.0000137    val loss: 0.0002287\n",
      "epoch: 8630    train loss: 0.0000478    val loss: 0.0002017\n",
      "epoch: 8640    train loss: 0.0000250    val loss: 0.0002752\n",
      "epoch: 8650    train loss: 0.0000142    val loss: 0.0002458\n",
      "epoch: 8660    train loss: 0.0000147    val loss: 0.0002379\n",
      "epoch: 8670    train loss: 0.0000130    val loss: 0.0002392\n",
      "epoch: 8680    train loss: 0.0000130    val loss: 0.0002453\n",
      "epoch: 8690    train loss: 0.0000128    val loss: 0.0002380\n",
      "epoch: 8700    train loss: 0.0000127    val loss: 0.0002393\n",
      "epoch: 8710    train loss: 0.0000127    val loss: 0.0002382\n",
      "epoch: 8720    train loss: 0.0000130    val loss: 0.0002287\n",
      "epoch: 8730    train loss: 0.0000325    val loss: 0.0001892\n",
      "epoch: 8740    train loss: 0.0000236    val loss: 0.0002679\n",
      "epoch: 8750    train loss: 0.0000158    val loss: 0.0002556\n",
      "epoch: 8760    train loss: 0.0000133    val loss: 0.0002232\n",
      "epoch: 8770    train loss: 0.0000129    val loss: 0.0002382\n",
      "epoch: 8780    train loss: 0.0000126    val loss: 0.0002440\n",
      "epoch: 8790    train loss: 0.0000125    val loss: 0.0002413\n",
      "epoch: 8800    train loss: 0.0000127    val loss: 0.0002482\n",
      "epoch: 8810    train loss: 0.0000185    val loss: 0.0002859\n",
      "epoch: 8820    train loss: 0.0000209    val loss: 0.0002713\n",
      "epoch: 8830    train loss: 0.0000142    val loss: 0.0002258\n",
      "epoch: 8840    train loss: 0.0000128    val loss: 0.0002454\n",
      "epoch: 8850    train loss: 0.0000125    val loss: 0.0002358\n",
      "epoch: 8860    train loss: 0.0000123    val loss: 0.0002380\n",
      "epoch: 8870    train loss: 0.0000121    val loss: 0.0002374\n",
      "epoch: 8880    train loss: 0.0000121    val loss: 0.0002392\n",
      "epoch: 8890    train loss: 0.0000122    val loss: 0.0002442\n",
      "epoch: 8900    train loss: 0.0000154    val loss: 0.0002726\n",
      "epoch: 8910    train loss: 0.0000384    val loss: 0.0003208\n",
      "epoch: 8920    train loss: 0.0000169    val loss: 0.0002220\n",
      "epoch: 8930    train loss: 0.0000144    val loss: 0.0002481\n",
      "epoch: 8940    train loss: 0.0000134    val loss: 0.0002419\n",
      "epoch: 8950    train loss: 0.0000121    val loss: 0.0002374\n",
      "epoch: 8960    train loss: 0.0000120    val loss: 0.0002317\n",
      "epoch: 8970    train loss: 0.0000147    val loss: 0.0002271\n",
      "epoch: 8980    train loss: 0.0000272    val loss: 0.0002160\n",
      "epoch: 8990    train loss: 0.0000166    val loss: 0.0002554\n",
      "epoch: 9000    train loss: 0.0000133    val loss: 0.0002328\n",
      "epoch: 9010    train loss: 0.0000122    val loss: 0.0002379\n",
      "epoch: 9020    train loss: 0.0000117    val loss: 0.0002349\n",
      "epoch: 9030    train loss: 0.0000116    val loss: 0.0002339\n",
      "epoch: 9040    train loss: 0.0000117    val loss: 0.0002316\n",
      "epoch: 9050    train loss: 0.0000155    val loss: 0.0002007\n",
      "epoch: 9060    train loss: 0.0000392    val loss: 0.0001832\n",
      "epoch: 9070    train loss: 0.0000152    val loss: 0.0002455\n",
      "epoch: 9080    train loss: 0.0000153    val loss: 0.0002371\n",
      "epoch: 9090    train loss: 0.0000117    val loss: 0.0002493\n",
      "epoch: 9100    train loss: 0.0000118    val loss: 0.0002265\n",
      "epoch: 9110    train loss: 0.0000114    val loss: 0.0002376\n",
      "epoch: 9120    train loss: 0.0000113    val loss: 0.0002366\n",
      "epoch: 9130    train loss: 0.0000113    val loss: 0.0002340\n",
      "epoch: 9140    train loss: 0.0000112    val loss: 0.0002354\n",
      "epoch: 9150    train loss: 0.0000112    val loss: 0.0002323\n",
      "epoch: 9160    train loss: 0.0000129    val loss: 0.0002224\n",
      "epoch: 9170    train loss: 0.0000440    val loss: 0.0001819\n",
      "epoch: 9180    train loss: 0.0000165    val loss: 0.0002768\n",
      "epoch: 9190    train loss: 0.0000150    val loss: 0.0002210\n",
      "epoch: 9200    train loss: 0.0000116    val loss: 0.0002326\n",
      "epoch: 9210    train loss: 0.0000116    val loss: 0.0002405\n",
      "epoch: 9220    train loss: 0.0000112    val loss: 0.0002308\n",
      "epoch: 9230    train loss: 0.0000110    val loss: 0.0002352\n",
      "epoch: 9240    train loss: 0.0000117    val loss: 0.0002268\n",
      "epoch: 9250    train loss: 0.0000293    val loss: 0.0002155\n",
      "epoch: 9260    train loss: 0.0000147    val loss: 0.0002389\n",
      "epoch: 9270    train loss: 0.0000125    val loss: 0.0002390\n",
      "epoch: 9280    train loss: 0.0000111    val loss: 0.0002316\n",
      "epoch: 9290    train loss: 0.0000110    val loss: 0.0002371\n",
      "epoch: 9300    train loss: 0.0000109    val loss: 0.0002317\n",
      "epoch: 9310    train loss: 0.0000108    val loss: 0.0002333\n",
      "epoch: 9320    train loss: 0.0000108    val loss: 0.0002322\n",
      "epoch: 9330    train loss: 0.0000112    val loss: 0.0002247\n",
      "epoch: 9340    train loss: 0.0000261    val loss: 0.0001896\n",
      "epoch: 9350    train loss: 0.0000225    val loss: 0.0002401\n",
      "epoch: 9360    train loss: 0.0000162    val loss: 0.0002188\n",
      "epoch: 9370    train loss: 0.0000120    val loss: 0.0002354\n",
      "epoch: 9380    train loss: 0.0000113    val loss: 0.0002353\n",
      "epoch: 9390    train loss: 0.0000107    val loss: 0.0002286\n",
      "epoch: 9400    train loss: 0.0000106    val loss: 0.0002335\n",
      "epoch: 9410    train loss: 0.0000105    val loss: 0.0002365\n",
      "epoch: 9420    train loss: 0.0000111    val loss: 0.0002465\n",
      "epoch: 9430    train loss: 0.0000315    val loss: 0.0003398\n",
      "epoch: 9440    train loss: 0.0000180    val loss: 0.0001897\n",
      "epoch: 9450    train loss: 0.0000149    val loss: 0.0002640\n",
      "epoch: 9460    train loss: 0.0000109    val loss: 0.0002302\n",
      "epoch: 9470    train loss: 0.0000105    val loss: 0.0002267\n",
      "epoch: 9480    train loss: 0.0000105    val loss: 0.0002378\n",
      "epoch: 9490    train loss: 0.0000105    val loss: 0.0002250\n",
      "epoch: 9500    train loss: 0.0000120    val loss: 0.0002264\n",
      "epoch: 9510    train loss: 0.0000348    val loss: 0.0002025\n",
      "epoch: 9520    train loss: 0.0000163    val loss: 0.0002570\n",
      "epoch: 9530    train loss: 0.0000111    val loss: 0.0002239\n",
      "epoch: 9540    train loss: 0.0000108    val loss: 0.0002315\n",
      "epoch: 9550    train loss: 0.0000105    val loss: 0.0002351\n",
      "epoch: 9560    train loss: 0.0000102    val loss: 0.0002325\n",
      "epoch: 9570    train loss: 0.0000101    val loss: 0.0002283\n",
      "epoch: 9580    train loss: 0.0000101    val loss: 0.0002280\n",
      "epoch: 9590    train loss: 0.0000113    val loss: 0.0002123\n",
      "epoch: 9600    train loss: 0.0000429    val loss: 0.0001602\n",
      "epoch: 9610    train loss: 0.0000214    val loss: 0.0002914\n",
      "epoch: 9620    train loss: 0.0000131    val loss: 0.0002181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9630    train loss: 0.0000101    val loss: 0.0002217\n",
      "epoch: 9640    train loss: 0.0000104    val loss: 0.0002431\n",
      "epoch: 9650    train loss: 0.0000101    val loss: 0.0002268\n",
      "epoch: 9660    train loss: 0.0000100    val loss: 0.0002332\n",
      "epoch: 9670    train loss: 0.0000111    val loss: 0.0002431\n",
      "epoch: 9680    train loss: 0.0000304    val loss: 0.0002877\n",
      "epoch: 9690    train loss: 0.0000135    val loss: 0.0002208\n",
      "epoch: 9700    train loss: 0.0000106    val loss: 0.0002120\n",
      "epoch: 9710    train loss: 0.0000108    val loss: 0.0002364\n",
      "epoch: 9720    train loss: 0.0000123    val loss: 0.0002078\n",
      "epoch: 9730    train loss: 0.0000189    val loss: 0.0002052\n",
      "epoch: 9740    train loss: 0.0000120    val loss: 0.0002434\n",
      "epoch: 9750    train loss: 0.0000101    val loss: 0.0002268\n",
      "epoch: 9760    train loss: 0.0000100    val loss: 0.0002341\n",
      "epoch: 9770    train loss: 0.0000115    val loss: 0.0002455\n",
      "epoch: 9780    train loss: 0.0000189    val loss: 0.0002877\n",
      "epoch: 9790    train loss: 0.0000117    val loss: 0.0002311\n",
      "epoch: 9800    train loss: 0.0000110    val loss: 0.0002312\n",
      "epoch: 9810    train loss: 0.0000104    val loss: 0.0002350\n",
      "epoch: 9820    train loss: 0.0000096    val loss: 0.0002333\n",
      "epoch: 9830    train loss: 0.0000100    val loss: 0.0002321\n",
      "epoch: 9840    train loss: 0.0000217    val loss: 0.0002770\n",
      "epoch: 9850    train loss: 0.0000153    val loss: 0.0002246\n",
      "epoch: 9860    train loss: 0.0000158    val loss: 0.0002673\n",
      "epoch: 9870    train loss: 0.0000110    val loss: 0.0002397\n",
      "epoch: 9880    train loss: 0.0000098    val loss: 0.0002250\n",
      "epoch: 9890    train loss: 0.0000097    val loss: 0.0002274\n",
      "epoch: 9900    train loss: 0.0000098    val loss: 0.0002194\n",
      "epoch: 9910    train loss: 0.0000186    val loss: 0.0001895\n",
      "epoch: 9920    train loss: 0.0000134    val loss: 0.0002204\n",
      "epoch: 9930    train loss: 0.0000099    val loss: 0.0002141\n",
      "epoch: 9940    train loss: 0.0000104    val loss: 0.0002485\n",
      "epoch: 9950    train loss: 0.0000097    val loss: 0.0002199\n",
      "epoch: 9960    train loss: 0.0000094    val loss: 0.0002325\n",
      "epoch: 9970    train loss: 0.0000096    val loss: 0.0002324\n",
      "epoch: 9980    train loss: 0.0000174    val loss: 0.0002549\n",
      "epoch: 9990    train loss: 0.0000165    val loss: 0.0002512\n",
      "epoch: 10000    train loss: 0.0000112    val loss: 0.0002226\n",
      "Wall time: 1h 22min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_size = n_bus*2\n",
    "hidden_size1 = 30\n",
    "hidden_size2 = 30\n",
    "output_size = n_bus*2\n",
    "lr = 0.001\n",
    "\n",
    "model = My_NN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train_loss_list, val_loss_list = [], []\n",
    "\n",
    "count=0\n",
    "patience=10000\n",
    "lossMin = 1e10\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_train_prediction = model(x_norm_train)\n",
    "    train_loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_train, y_val_mean, y_val_std))\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss_list.append(train_loss.detach())\n",
    "\n",
    "    model.eval()\n",
    "    y_val_prediction = model(x_norm_val)\n",
    "    val_loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))\n",
    "    val_loss_list.append(val_loss.detach())\n",
    "\n",
    "    #early stopping\n",
    "    if (val_loss < lossMin):\n",
    "        lossMin = val_loss\n",
    "        count = 0\n",
    "        best_epoch = epoch\n",
    "        best_train_loss = train_loss\n",
    "        best_val_loss = val_loss\n",
    "        model.save_weights(model, \"[PyG] [14 bus] Best_NN_model.pt\")\n",
    "    else:\n",
    "        count+=1\n",
    "        if(count>patience):\n",
    "            print(\"early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(epoch, train_loss, val_loss))\n",
    "            print(\"best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(best_epoch, best_train_loss, best_val_loss))\n",
    "            break\n",
    "    \n",
    "    #if (train_loss <= 0):\n",
    "    #    print(\"min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(epoch, train_loss, val_loss))\n",
    "    #    break\n",
    "\n",
    "    if (epoch % 10) == 0:\n",
    "        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last epoch: 10000, train loss: 0.0000112, val loss: 0.0002226\n",
      "best epoch: 9602, train loss: 0.0000455, val loss: 0.0001579\n"
     ]
    }
   ],
   "source": [
    "plt.title('NN on power flow dataset')\n",
    "plt.plot(train_loss_list, label=\"train loss\")\n",
    "plt.plot(val_loss_list, label=\"val loss\")\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"# Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print('last epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(epoch, train_loss, val_loss))\n",
    "print('best epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(best_epoch, best_train_loss, best_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin1.weight\n",
      "torch.Size([30, 28])\n",
      "lin1.bias\n",
      "torch.Size([30])\n",
      "lin2.weight\n",
      "torch.Size([30, 30])\n",
      "lin2.bias\n",
      "torch.Size([30])\n",
      "lin3.weight\n",
      "torch.Size([28, 30])\n",
      "lin3.bias\n",
      "torch.Size([28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2668"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  print(name)\n",
    "  print(param.size())\n",
    "\n",
    "param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train output ground-truth: \n",
      "[  10.324 -154.216    9.882 -156.117    9.753 -156.712    9.724 -156.839\n",
      "    9.659 -157.161    9.607 -157.399    9.617 -157.37     9.627 -157.311\n",
      "    9.581 -157.512    9.56  -157.633    9.773 -156.613    9.682 -157.039\n",
      "    9.658 -157.152    9.664 -157.123]\n",
      "Train output prediction: \n",
      "[  10.32337 -154.21791    9.88046 -156.12112    9.75189 -156.71469\n",
      "    9.72361 -156.84132    9.65743 -157.16484    9.60688 -157.39487\n",
      "    9.6149  -157.37616    9.62569 -157.31027    9.58075 -157.51251\n",
      "    9.56007 -157.62479    9.77127 -156.61847    9.68154 -157.04373\n",
      "    9.65747 -157.15675    9.66304 -157.12936]\n",
      "Train loss (MSE): 0.0000113\n",
      "===========================================================================\n",
      "Train output ground-truth: \n",
      "[  10.302 -154.788    9.844 -156.745    9.686 -157.465    9.685 -157.484\n",
      "    9.574 -157.989    9.568 -158.053    9.543 -158.141    9.532 -158.187\n",
      "    9.518 -158.294    9.535 -158.209    9.727 -157.275    9.629 -157.731\n",
      "    9.599 -157.869    9.594 -157.896]\n",
      "Train output prediction: \n",
      "[  10.30176 -154.786      9.84274 -156.7442     9.6863  -157.45932\n",
      "    9.684   -157.48647    9.57496 -157.98235    9.56532 -158.05833\n",
      "    9.54306 -158.13431    9.53333 -158.17989    9.51518 -158.30139\n",
      "    9.5325  -158.21848    9.72635 -157.27475    9.62966 -157.7315\n",
      "    9.59921 -157.8712     9.59554 -157.89432]\n",
      "Train loss (MSE): 0.0002226\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_train_prediction = model(x_norm_train)\n",
    "train_loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_train, y_val_mean, y_val_std))\n",
    "print(\"Train output ground-truth: \\n\" + str(y_raw_train.detach().numpy()[0]))\n",
    "print(\"Train output prediction: \\n\" + str(denormalize_output(y_train_prediction, y_val_mean, y_val_std).detach().numpy()[0]))\n",
    "print('Train loss (MSE): {:.7f}'.format(train_loss))\n",
    "\n",
    "print(\"===========================================================================\")\n",
    "\n",
    "y_val_prediction = model(x_norm_val)\n",
    "val_loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))\n",
    "print(\"Train output ground-truth: \\n\" + str(y_raw_val.detach().numpy()[0]))\n",
    "print(\"Train output prediction: \\n\" + str(denormalize_output(y_val_prediction, y_val_mean, y_val_std).detach().numpy()[0]))\n",
    "print('Train loss (MSE): {:.7f}'.format(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train output ground-truth: \n",
      "[  10.324 -154.216    9.882 -156.117    9.753 -156.712    9.724 -156.839\n",
      "    9.659 -157.161    9.607 -157.399    9.617 -157.37     9.627 -157.311\n",
      "    9.581 -157.512    9.56  -157.633    9.773 -156.613    9.682 -157.039\n",
      "    9.658 -157.152    9.664 -157.123]\n",
      "Train output prediction: \n",
      "[  10.32377 -154.2075     9.88146 -156.10791    9.75312 -156.70035\n",
      "    9.72478 -156.82715    9.65885 -157.14973    9.60828 -157.37967\n",
      "    9.61632 -157.36072    9.62708 -157.29482    9.58217 -157.49727\n",
      "    9.56154 -157.60919    9.77247 -156.60439    9.68292 -157.029\n",
      "    9.65883 -157.14175    9.66444 -157.11444]\n",
      "Train loss (MSE): 0.0000398\n",
      "===========================================================================\n",
      "Train output ground-truth: \n",
      "[  10.302 -154.788    9.844 -156.745    9.686 -157.465    9.685 -157.484\n",
      "    9.574 -157.989    9.568 -158.053    9.543 -158.141    9.532 -158.187\n",
      "    9.518 -158.294    9.535 -158.209    9.727 -157.275    9.629 -157.731\n",
      "    9.599 -157.869    9.594 -157.896]\n",
      "Train output prediction: \n",
      "[  10.30203 -154.78003    9.84329 -156.73668    9.68707 -157.45119\n",
      "    9.68472 -157.47833    9.57582 -157.97365    9.56615 -158.04968\n",
      "    9.54388 -158.1254     9.53412 -158.17119    9.51595 -158.29263\n",
      "    9.53329 -158.20984    9.72704 -157.26665    9.63057 -157.72304\n",
      "    9.6     -157.86253    9.59649 -157.88567]\n",
      "Train loss (MSE): 0.0001579\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load(\"[PyG] [14 bus] Best_NN_model.pt\")\n",
    "best_model.eval()\n",
    "\n",
    "y_train_prediction = best_model(x_norm_train)\n",
    "train_loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_train, y_val_mean, y_val_std))\n",
    "print(\"Train output ground-truth: \\n\" + str(y_raw_train.detach().numpy()[0]))\n",
    "print(\"Train output prediction: \\n\" + str(denormalize_output(y_train_prediction, y_val_mean, y_val_std).detach().numpy()[0]))\n",
    "print('Train loss (MSE): {:.7f}'.format(train_loss))\n",
    "\n",
    "print(\"===========================================================================\")\n",
    "\n",
    "y_val_prediction = best_model(x_norm_val)\n",
    "val_loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))\n",
    "print(\"Train output ground-truth: \\n\" + str(y_raw_val.detach().numpy()[0]))\n",
    "print(\"Train output prediction: \\n\" + str(denormalize_output(y_val_prediction, y_val_mean, y_val_std).detach().numpy()[0]))\n",
    "print('Train loss (MSE): {:.7f}'.format(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 1\n",
      "Train loss (MSE): 0.0000398\n",
      "===========================\n",
      "dataset 2\n",
      "Val loss (MSE): 0.0001579\n",
      "===========================\n",
      "dataset 3\n",
      "Test loss (MSE): 0.0000688\n",
      "===========================\n",
      "dataset 4\n",
      "Test loss (MSE): 0.0001142\n",
      "===========================\n",
      "dataset 5\n",
      "Test loss (MSE): 0.0002316\n",
      "===========================\n",
      "dataset 6\n",
      "Test loss (MSE): 0.0000720\n",
      "===========================\n",
      "dataset 7\n",
      "Test loss (MSE): 0.0001097\n",
      "===========================\n",
      "dataset 8\n",
      "Test loss (MSE): 0.0002451\n",
      "===========================\n",
      "dataset 9\n",
      "Test loss (MSE): 0.0001211\n",
      "===========================\n",
      "dataset 10\n",
      "Test loss (MSE): 0.0001841\n",
      "===========================\n",
      "dataset 11\n",
      "Test loss (MSE): 0.0001146\n",
      "===========================\n",
      "dataset 12\n",
      "Test loss (MSE): 0.0003957\n",
      "===========================\n",
      "dataset 13\n",
      "Test loss (MSE): 0.0000778\n",
      "===========================\n",
      "dataset 14\n",
      "Test loss (MSE): 0.0000807\n",
      "===========================\n",
      "dataset 15\n",
      "Test loss (MSE): 0.0001075\n",
      "===========================\n",
      "dataset 16\n",
      "Test loss (MSE): 0.0000741\n",
      "===========================\n",
      "dataset 17\n",
      "Test loss (MSE): 0.0000708\n",
      "===========================\n",
      "dataset 18\n",
      "Test loss (MSE): 0.0001177\n",
      "===========================\n",
      "dataset 19\n",
      "Test loss (MSE): 0.0001014\n",
      "===========================\n",
      "dataset 20\n",
      "Test loss (MSE): 0.0003082\n",
      "===========================\n",
      "dataset 21\n",
      "Test loss (MSE): 0.0002131\n",
      "===========================\n",
      "dataset 22\n",
      "Test loss (MSE): 0.0001551\n",
      "===========================\n",
      "dataset 23\n",
      "Test loss (MSE): 0.0000882\n",
      "===========================\n",
      "dataset 24\n",
      "Test loss (MSE): 0.0000836\n",
      "===========================\n",
      "dataset 25\n",
      "Test loss (MSE): 0.0001135\n",
      "===========================\n",
      "dataset 26\n",
      "Test loss (MSE): 0.0001030\n",
      "===========================\n",
      "dataset 27\n",
      "Test loss (MSE): 0.0002208\n",
      "===========================\n",
      "dataset 28\n",
      "Test loss (MSE): 0.0002991\n",
      "===========================\n",
      "dataset 29\n",
      "Test loss (MSE): 0.0000912\n",
      "===========================\n",
      "dataset 30\n",
      "Test loss (MSE): 0.0001366\n",
      "===========================\n",
      "dataset 31\n",
      "Test loss (MSE): 0.0001061\n",
      "===========================\n",
      "dataset 32\n",
      "Test loss (MSE): 0.0000771\n",
      "===========================\n",
      "dataset 33\n",
      "Test loss (MSE): 0.0001122\n",
      "===========================\n",
      "dataset 34\n",
      "Test loss (MSE): 0.0000748\n",
      "===========================\n",
      "dataset 35\n",
      "Test loss (MSE): 0.0000912\n",
      "===========================\n",
      "dataset 36\n",
      "Test loss (MSE): 0.0001132\n",
      "===========================\n",
      "dataset 37\n",
      "Test loss (MSE): 0.0000828\n",
      "===========================\n",
      "dataset 38\n",
      "Test loss (MSE): 0.0001088\n",
      "===========================\n",
      "dataset 39\n",
      "Test loss (MSE): 0.0001705\n",
      "===========================\n",
      "dataset 40\n",
      "Test loss (MSE): 0.0002441\n",
      "===========================\n",
      "dataset 41\n",
      "Test loss (MSE): 0.0001212\n",
      "===========================\n",
      "dataset 42\n",
      "Test loss (MSE): 0.0002514\n",
      "===========================\n",
      "dataset 43\n",
      "Test loss (MSE): 0.0000730\n",
      "===========================\n",
      "dataset 44\n",
      "Test loss (MSE): 0.0001093\n",
      "===========================\n",
      "dataset 45\n",
      "Test loss (MSE): 0.0000907\n",
      "===========================\n",
      "dataset 46\n",
      "Test loss (MSE): 0.0000810\n",
      "===========================\n",
      "dataset 47\n",
      "Test loss (MSE): 0.0001234\n",
      "===========================\n",
      "dataset 48\n",
      "Test loss (MSE): 0.0000988\n",
      "===========================\n",
      "dataset 49\n",
      "Test loss (MSE): 0.0001332\n",
      "===========================\n",
      "dataset 50\n",
      "Test loss (MSE): 0.0001530\n",
      "===========================\n",
      "dataset 51\n",
      "Test loss (MSE): 0.0001175\n",
      "===========================\n",
      "dataset 52\n",
      "Test loss (MSE): 0.0001583\n",
      "===========================\n",
      "dataset 53\n",
      "Test loss (MSE): 0.0000912\n",
      "===========================\n",
      "dataset 54\n",
      "Test loss (MSE): 0.0000915\n",
      "===========================\n",
      "dataset 55\n",
      "Test loss (MSE): 0.0002475\n",
      "===========================\n",
      "dataset 56\n",
      "Test loss (MSE): 0.0002510\n",
      "===========================\n",
      "dataset 57\n",
      "Test loss (MSE): 0.0001204\n",
      "===========================\n",
      "dataset 58\n",
      "Test loss (MSE): 0.0000867\n",
      "===========================\n",
      "dataset 59\n",
      "Test loss (MSE): 0.0001369\n",
      "===========================\n",
      "dataset 60\n",
      "Test loss (MSE): 0.0004693\n",
      "===========================\n",
      "dataset 61\n",
      "Test loss (MSE): 0.0001237\n",
      "===========================\n",
      "dataset 62\n",
      "Test loss (MSE): 0.0001159\n",
      "===========================\n",
      "dataset 63\n",
      "Test loss (MSE): 0.0000612\n",
      "===========================\n",
      "dataset 64\n",
      "Test loss (MSE): 0.0001302\n",
      "===========================\n",
      "dataset 65\n",
      "Test loss (MSE): 0.0001516\n",
      "===========================\n",
      "dataset 66\n",
      "Test loss (MSE): 0.0001728\n",
      "===========================\n",
      "dataset 67\n",
      "Test loss (MSE): 0.0000823\n",
      "===========================\n",
      "dataset 68\n",
      "Test loss (MSE): 0.0001305\n",
      "===========================\n",
      "dataset 69\n",
      "Test loss (MSE): 0.0000969\n",
      "===========================\n",
      "dataset 70\n",
      "Test loss (MSE): 0.0001284\n",
      "===========================\n",
      "dataset 71\n",
      "Test loss (MSE): 0.0001090\n",
      "===========================\n",
      "dataset 72\n",
      "Test loss (MSE): 0.0001290\n",
      "===========================\n",
      "dataset 73\n",
      "Test loss (MSE): 0.0004100\n",
      "===========================\n",
      "dataset 74\n",
      "Test loss (MSE): 0.0001745\n",
      "===========================\n",
      "dataset 75\n",
      "Test loss (MSE): 0.0002086\n",
      "===========================\n",
      "dataset 76\n",
      "Test loss (MSE): 0.0001694\n",
      "===========================\n",
      "dataset 77\n",
      "Test loss (MSE): 0.0000900\n",
      "===========================\n",
      "dataset 78\n",
      "Test loss (MSE): 0.0002596\n",
      "===========================\n",
      "dataset 79\n",
      "Test loss (MSE): 0.0001405\n",
      "===========================\n",
      "dataset 80\n",
      "Test loss (MSE): 0.0000838\n",
      "===========================\n",
      "dataset 81\n",
      "Test loss (MSE): 0.0001416\n",
      "===========================\n",
      "dataset 82\n",
      "Test loss (MSE): 0.0002507\n",
      "===========================\n",
      "dataset 83\n",
      "Test loss (MSE): 0.0000972\n",
      "===========================\n",
      "dataset 84\n",
      "Test loss (MSE): 0.0000915\n",
      "===========================\n",
      "dataset 85\n",
      "Test loss (MSE): 0.0001009\n",
      "===========================\n",
      "dataset 86\n",
      "Test loss (MSE): 0.0001007\n",
      "===========================\n",
      "dataset 87\n",
      "Test loss (MSE): 0.0001176\n",
      "===========================\n",
      "dataset 88\n",
      "Test loss (MSE): 0.0000986\n",
      "===========================\n",
      "dataset 89\n",
      "Test loss (MSE): 0.0002892\n",
      "===========================\n",
      "dataset 90\n",
      "Test loss (MSE): 0.0001439\n",
      "===========================\n",
      "dataset 91\n",
      "Test loss (MSE): 0.0004422\n",
      "===========================\n",
      "dataset 92\n",
      "Test loss (MSE): 0.0001364\n",
      "===========================\n",
      "dataset 93\n",
      "Test loss (MSE): 0.0001466\n",
      "===========================\n",
      "dataset 94\n",
      "Test loss (MSE): 0.0000839\n",
      "===========================\n",
      "dataset 95\n",
      "Test loss (MSE): 0.0001391\n",
      "===========================\n",
      "dataset 96\n",
      "Test loss (MSE): 0.0000879\n",
      "===========================\n",
      "dataset 97\n",
      "Test loss (MSE): 0.0002292\n",
      "===========================\n",
      "dataset 98\n",
      "Test loss (MSE): 0.0000769\n",
      "===========================\n",
      "dataset 99\n",
      "Test loss (MSE): 0.0001423\n",
      "===========================\n",
      "dataset 100\n",
      "Test loss (MSE): 0.0001171\n",
      "===========================\n",
      "dataset 101\n",
      "Test loss (MSE): 0.0002404\n",
      "===========================\n",
      "dataset 102\n",
      "Test loss (MSE): 0.0001535\n",
      "===========================\n",
      "\n",
      "test loss file saved!\n",
      "\n",
      "Wall time: 4min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model = torch.load(\"[PyG] [14 bus] Best_NN_model.pt\")\n",
    "best_model.eval()\n",
    "\n",
    "test_loss_list = []\n",
    "\n",
    "for i in range(102):\n",
    "    \n",
    "    dataset = pd.read_excel('dataset\\Grid_14 bus_%d.xlsx' % (i+1)).values\n",
    "    test_percentage = 100\n",
    "    test_dataset = slice_dataset(dataset, test_percentage)\n",
    "    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)\n",
    "    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)\n",
    "    \n",
    "    print('dataset {:d}'.format(i+1))\n",
    "    \n",
    "    y_test_prediction = best_model(x_norm_test)\n",
    "    test_loss = MSE(denormalize_output(y_test_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_test, y_val_mean, y_val_std))\n",
    "    \n",
    "    if i == 0:\n",
    "        print('Train loss (MSE): {:.7f}'.format(test_loss.detach().numpy()))\n",
    "    elif i == 1:\n",
    "        print('Val loss (MSE): {:.7f}'.format(test_loss.detach().numpy()))\n",
    "    else:\n",
    "        print('Test loss (MSE): {:.7f}'.format(test_loss))\n",
    "        test_loss_list.append(test_loss.detach().numpy())\n",
    "    \n",
    "    print(\"===========================\")\n",
    "\n",
    "column = []\n",
    "for i in range(100):\n",
    "    column.append('test loss %d' % (i+1))\n",
    "    \n",
    "test_loss_file = pd.DataFrame([test_loss_list], columns=column)\n",
    "test_loss_file.to_excel(\"[PyG] [14 bus] [MSE] NN test loss.xlsx\")\n",
    "print(\"\\ntest loss file saved!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 1\n",
      "Train loss (NRMSE): 0.0224649\n",
      "===========================\n",
      "dataset 2\n",
      "Val loss (NRMSE): 0.0236703\n",
      "===========================\n",
      "dataset 3\n",
      "Test loss (NRMSE): 0.0248321\n",
      "===========================\n",
      "dataset 4\n",
      "Test loss (NRMSE): 0.0262397\n",
      "===========================\n",
      "dataset 5\n",
      "Test loss (NRMSE): 0.0306412\n",
      "===========================\n",
      "dataset 6\n",
      "Test loss (NRMSE): 0.0221979\n",
      "===========================\n",
      "dataset 7\n",
      "Test loss (NRMSE): 0.0474466\n",
      "===========================\n",
      "dataset 8\n",
      "Test loss (NRMSE): 0.0652829\n",
      "===========================\n",
      "dataset 9\n",
      "Test loss (NRMSE): 0.0266288\n",
      "===========================\n",
      "dataset 10\n",
      "Test loss (NRMSE): 0.0534130\n",
      "===========================\n",
      "dataset 11\n",
      "Test loss (NRMSE): 0.0409014\n",
      "===========================\n",
      "dataset 12\n",
      "Test loss (NRMSE): 0.0340073\n",
      "===========================\n",
      "dataset 13\n",
      "Test loss (NRMSE): 0.0249103\n",
      "===========================\n",
      "dataset 14\n",
      "Test loss (NRMSE): 0.0243192\n",
      "===========================\n",
      "dataset 15\n",
      "Test loss (NRMSE): 0.0259423\n",
      "===========================\n",
      "dataset 16\n",
      "Test loss (NRMSE): 0.0286170\n",
      "===========================\n",
      "dataset 17\n",
      "Test loss (NRMSE): 0.0287907\n",
      "===========================\n",
      "dataset 18\n",
      "Test loss (NRMSE): 0.0354325\n",
      "===========================\n",
      "dataset 19\n",
      "Test loss (NRMSE): 0.0265823\n",
      "===========================\n",
      "dataset 20\n",
      "Test loss (NRMSE): 0.0299844\n",
      "===========================\n",
      "dataset 21\n",
      "Test loss (NRMSE): 0.0365966\n",
      "===========================\n",
      "dataset 22\n",
      "Test loss (NRMSE): 0.0315232\n",
      "===========================\n",
      "dataset 23\n",
      "Test loss (NRMSE): 0.0306503\n",
      "===========================\n",
      "dataset 24\n",
      "Test loss (NRMSE): 0.0541991\n",
      "===========================\n",
      "dataset 25\n",
      "Test loss (NRMSE): 0.0422271\n",
      "===========================\n",
      "dataset 26\n",
      "Test loss (NRMSE): 0.0258286\n",
      "===========================\n",
      "dataset 27\n",
      "Test loss (NRMSE): 0.0397166\n",
      "===========================\n",
      "dataset 28\n",
      "Test loss (NRMSE): 0.0269307\n",
      "===========================\n",
      "dataset 29\n",
      "Test loss (NRMSE): 0.0240427\n",
      "===========================\n",
      "dataset 30\n",
      "Test loss (NRMSE): 0.0512886\n",
      "===========================\n",
      "dataset 31\n",
      "Test loss (NRMSE): 0.0493428\n",
      "===========================\n",
      "dataset 32\n",
      "Test loss (NRMSE): 0.0224386\n",
      "===========================\n",
      "dataset 33\n",
      "Test loss (NRMSE): 0.0396846\n",
      "===========================\n",
      "dataset 34\n",
      "Test loss (NRMSE): 0.0272284\n",
      "===========================\n",
      "dataset 35\n",
      "Test loss (NRMSE): 0.0581042\n",
      "===========================\n",
      "dataset 36\n",
      "Test loss (NRMSE): 0.0301112\n",
      "===========================\n",
      "dataset 37\n",
      "Test loss (NRMSE): 0.0237314\n",
      "===========================\n",
      "dataset 38\n",
      "Test loss (NRMSE): 0.0419455\n",
      "===========================\n",
      "dataset 39\n",
      "Test loss (NRMSE): 0.0370890\n",
      "===========================\n",
      "dataset 40\n",
      "Test loss (NRMSE): 0.0489609\n",
      "===========================\n",
      "dataset 41\n",
      "Test loss (NRMSE): 0.0256547\n",
      "===========================\n",
      "dataset 42\n",
      "Test loss (NRMSE): 0.0310231\n",
      "===========================\n",
      "dataset 43\n",
      "Test loss (NRMSE): 0.0297612\n",
      "===========================\n",
      "dataset 44\n",
      "Test loss (NRMSE): 0.0317308\n",
      "===========================\n",
      "dataset 45\n",
      "Test loss (NRMSE): 0.0259088\n",
      "===========================\n",
      "dataset 46\n",
      "Test loss (NRMSE): 0.0272202\n",
      "===========================\n",
      "dataset 47\n",
      "Test loss (NRMSE): 0.0290702\n",
      "===========================\n",
      "dataset 48\n",
      "Test loss (NRMSE): 0.0331988\n",
      "===========================\n",
      "dataset 49\n",
      "Test loss (NRMSE): 0.0390297\n",
      "===========================\n",
      "dataset 50\n",
      "Test loss (NRMSE): 0.0285324\n",
      "===========================\n",
      "dataset 51\n",
      "Test loss (NRMSE): 0.0311163\n",
      "===========================\n",
      "dataset 52\n",
      "Test loss (NRMSE): 0.0300224\n",
      "===========================\n",
      "dataset 53\n",
      "Test loss (NRMSE): 0.0376184\n",
      "===========================\n",
      "dataset 54\n",
      "Test loss (NRMSE): 0.0254863\n",
      "===========================\n",
      "dataset 55\n",
      "Test loss (NRMSE): 0.0275177\n",
      "===========================\n",
      "dataset 56\n",
      "Test loss (NRMSE): 0.0395991\n",
      "===========================\n",
      "dataset 57\n",
      "Test loss (NRMSE): 0.0293015\n",
      "===========================\n",
      "dataset 58\n",
      "Test loss (NRMSE): 0.0360091\n",
      "===========================\n",
      "dataset 59\n",
      "Test loss (NRMSE): 0.0285180\n",
      "===========================\n",
      "dataset 60\n",
      "Test loss (NRMSE): 0.0271815\n",
      "===========================\n",
      "dataset 61\n",
      "Test loss (NRMSE): 0.0321179\n",
      "===========================\n",
      "dataset 62\n",
      "Test loss (NRMSE): 0.0287197\n",
      "===========================\n",
      "dataset 63\n",
      "Test loss (NRMSE): 0.0376081\n",
      "===========================\n",
      "dataset 64\n",
      "Test loss (NRMSE): 0.0290444\n",
      "===========================\n",
      "dataset 65\n",
      "Test loss (NRMSE): 0.0270845\n",
      "===========================\n",
      "dataset 66\n",
      "Test loss (NRMSE): 0.0280758\n",
      "===========================\n",
      "dataset 67\n",
      "Test loss (NRMSE): 0.0258141\n",
      "===========================\n",
      "dataset 68\n",
      "Test loss (NRMSE): 0.0289292\n",
      "===========================\n",
      "dataset 69\n",
      "Test loss (NRMSE): 0.0301686\n",
      "===========================\n",
      "dataset 70\n",
      "Test loss (NRMSE): 0.0519737\n",
      "===========================\n",
      "dataset 71\n",
      "Test loss (NRMSE): 0.0272251\n",
      "===========================\n",
      "dataset 72\n",
      "Test loss (NRMSE): 0.0309792\n",
      "===========================\n",
      "dataset 73\n",
      "Test loss (NRMSE): 0.0587689\n",
      "===========================\n",
      "dataset 74\n",
      "Test loss (NRMSE): 0.0271860\n",
      "===========================\n",
      "dataset 75\n",
      "Test loss (NRMSE): 0.0357772\n",
      "===========================\n",
      "dataset 76\n",
      "Test loss (NRMSE): 0.0253062\n",
      "===========================\n",
      "dataset 77\n",
      "Test loss (NRMSE): 0.0311475\n",
      "===========================\n",
      "dataset 78\n",
      "Test loss (NRMSE): 0.0425853\n",
      "===========================\n",
      "dataset 79\n",
      "Test loss (NRMSE): 0.0253282\n",
      "===========================\n",
      "dataset 80\n",
      "Test loss (NRMSE): 0.0370705\n",
      "===========================\n",
      "dataset 81\n",
      "Test loss (NRMSE): 0.0267659\n",
      "===========================\n",
      "dataset 82\n",
      "Test loss (NRMSE): 0.0450549\n",
      "===========================\n",
      "dataset 83\n",
      "Test loss (NRMSE): 0.0259077\n",
      "===========================\n",
      "dataset 84\n",
      "Test loss (NRMSE): 0.0451159\n",
      "===========================\n",
      "dataset 85\n",
      "Test loss (NRMSE): 0.0265593\n",
      "===========================\n",
      "dataset 86\n",
      "Test loss (NRMSE): 0.0573284\n",
      "===========================\n",
      "dataset 87\n",
      "Test loss (NRMSE): 0.0330591\n",
      "===========================\n",
      "dataset 88\n",
      "Test loss (NRMSE): 0.0280914\n",
      "===========================\n",
      "dataset 89\n",
      "Test loss (NRMSE): 0.0267365\n",
      "===========================\n",
      "dataset 90\n",
      "Test loss (NRMSE): 0.0376383\n",
      "===========================\n",
      "dataset 91\n",
      "Test loss (NRMSE): 0.0258171\n",
      "===========================\n",
      "dataset 92\n",
      "Test loss (NRMSE): 0.0410293\n",
      "===========================\n",
      "dataset 93\n",
      "Test loss (NRMSE): 0.0771886\n",
      "===========================\n",
      "dataset 94\n",
      "Test loss (NRMSE): 0.0305091\n",
      "===========================\n",
      "dataset 95\n",
      "Test loss (NRMSE): 0.0250647\n",
      "===========================\n",
      "dataset 96\n",
      "Test loss (NRMSE): 0.0434113\n",
      "===========================\n",
      "dataset 97\n",
      "Test loss (NRMSE): 0.0199504\n",
      "===========================\n",
      "dataset 98\n",
      "Test loss (NRMSE): 0.0483359\n",
      "===========================\n",
      "dataset 99\n",
      "Test loss (NRMSE): 0.0281229\n",
      "===========================\n",
      "dataset 100\n",
      "Test loss (NRMSE): 0.0263574\n",
      "===========================\n",
      "dataset 101\n",
      "Test loss (NRMSE): 0.0221809\n",
      "===========================\n",
      "dataset 102\n",
      "Test loss (NRMSE): 0.0416146\n",
      "===========================\n",
      "\n",
      "test loss file saved!\n",
      "\n",
      "Wall time: 4min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model = torch.load(\"[PyG] [14 bus] Best_NN_model.pt\")\n",
    "best_model.eval()\n",
    "\n",
    "test_loss_list = []\n",
    "\n",
    "for i in range(102):\n",
    "    \n",
    "    dataset = pd.read_excel('dataset\\Grid_14 bus_%d.xlsx' % (i+1)).values\n",
    "    test_percentage = 100\n",
    "    test_dataset = slice_dataset(dataset, test_percentage)\n",
    "    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)\n",
    "    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)\n",
    "    \n",
    "    print('dataset {:d}'.format(i+1))\n",
    "    \n",
    "    yhat = denormalize_output(best_model(x_norm_test), y_val_mean, y_val_std)\n",
    "    y = y_raw_test\n",
    "    test_loss_NRMSE = NRMSE(yhat, y)\n",
    "    \n",
    "    if i == 0:\n",
    "        print('Train loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))\n",
    "    elif i == 1:\n",
    "        print('Val loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))\n",
    "    else:\n",
    "        print('Test loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))\n",
    "        test_loss_list.append(test_loss_NRMSE.detach().numpy())\n",
    "    \n",
    "    print(\"===========================\")\n",
    "\n",
    "column = []\n",
    "for i in range(100):\n",
    "    column.append('test loss %d' % (i+1))\n",
    "    \n",
    "test_loss_file = pd.DataFrame([test_loss_list], columns=column)\n",
    "test_loss_file.to_excel(\"[PyG] [14 bus] [NRMSE] NN test loss.xlsx\")\n",
    "print(\"\\ntest loss file saved!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
